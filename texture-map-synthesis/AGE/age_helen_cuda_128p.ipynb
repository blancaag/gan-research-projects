{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0,1\n",
    "import sys\n",
    "sys.path.append('/media/dataserver/workspace/blanca/utils/')\n",
    "\n",
    "from utils import libraries\n",
    "# from utils import utils_global\n",
    "from utils_global import *\n",
    "\n",
    "%matplotlib inline\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#########\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', required=True,\n",
    "                    help='cifar10 | lsun | imagenet | folder | lfw ')\n",
    "parser.add_argument('--dataroot', type=str, help='path to dataset')\n",
    "parser.add_argument('--workers', type=int,\n",
    "                    help='number of data loading workers', default=8)\n",
    "parser.add_argument('--batch_size', type=int,\n",
    "                    default=64, help='batch size')\n",
    "parser.add_argument('--image_size', type=int, default=32,\n",
    "                    help='the resolution of the input image to network')\n",
    "parser.add_argument('--nz', type=int, default=100,\n",
    "                    help='size of the latent z vector')\n",
    "parser.add_argument('--ngf', type=int, default=64)\n",
    "parser.add_argument('--ndf', type=int, default=64)\n",
    "parser.add_argument('--nc', type=int)\n",
    "\n",
    "parser.add_argument('--nepoch', type=int, default=25,\n",
    "                    help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.0002,\n",
    "                    help='learning rate, default=0.0002')\n",
    "parser.add_argument('--beta1', type=float, default=0.5,\n",
    "                    help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--cpu', action='store_true',\n",
    "                    help='use CPU instead of GPU')\n",
    "parser.add_argument('--ngpu', type=int, default=1,\n",
    "                    help='number of GPUs to use')\n",
    "\n",
    "parser.add_argument('--netG', default='',\n",
    "                    help=\"path to netG config\")\n",
    "parser.add_argument('--netE', default='',\n",
    "                    help=\"path to netE config\")\n",
    "parser.add_argument('--netG_chp', default='',\n",
    "                    help=\"path to netG (to continue training)\")\n",
    "parser.add_argument('--netE_chp', default='',\n",
    "                    help=\"path to netE (to continue training)\")\n",
    "\n",
    "parser.add_argument('--save_dir', default='.',\n",
    "                    help='folder to output images and model checkpoints')\n",
    "parser.add_argument('--criterion', default='param',\n",
    "                    help='param|nonparam, How to estimate KL')\n",
    "parser.add_argument('--KL', default='qp', help='pq|qp')\n",
    "parser.add_argument('--noise', default='sphere', help='normal|sphere')\n",
    "parser.add_argument('--match_z', default='cos', help='none|L1|L2|cos')\n",
    "parser.add_argument('--match_x', default='L1', help='none|L1|L2|cos')\n",
    "\n",
    "parser.add_argument('--drop_lr', default=5, type=int, help='')\n",
    "parser.add_argument('--save_every', default=50, type=int, help='')\n",
    "\n",
    "parser.add_argument('--manual_seed', type=int, default=123, help='manual seed')\n",
    "parser.add_argument('--start_epoch', type=int, default=0, help='epoch number to start with')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--e_updates', default=\"1;KL_fake:1,KL_real:1,match_z:0,match_x:0\",\n",
    "    help='Update plan for encoder <number of updates>;[<term:weight>]'\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--g_updates', default=\"2;KL_fake:1,match_z:1,match_x:0\",\n",
    "    help='Update plan for generator <number of updates>;[<term:weight>]'\n",
    ")\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "import importlib\n",
    "# from .dataset import FolderWithImages\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory was not created.\n",
      "64_128_1000_nz64_lrdlr_0_100\n"
     ]
    }
   ],
   "source": [
    "# setup function @utils.py\n",
    "\n",
    "cuda = torch.cuda.is_available() # not opt.cpu\n",
    "torch.set_num_threads(4)\n",
    "dataset = 'latest_v2.4'\n",
    "\n",
    "save_dir = 'output/' + dataset\n",
    "try:\n",
    "    os.makedirs(save_dir)\n",
    "except OSError:\n",
    "    print('Directory was not created.')\n",
    "\n",
    "manual_seed = random.randint(1, 10000)\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if torch.cuda.is_available() and not cuda:\n",
    "    print(\"WARNING: You have a CUDA device,\"\n",
    "            \"so you should probably run with --cuda\")\n",
    "\n",
    "e_updates = '1;KL_fake:1,KL_real:1,match_z:0,match_x:10' \n",
    "g_updates = '3;KL_fake:1,match_z:1000,match_x:0'\n",
    "    \n",
    "updates = {'e': {}, 'g': {}}\n",
    "updates['e']['num_updates'] = int(e_updates.split(';')[0])\n",
    "updates['e'].update({x.split(':')[0]: float(x.split(':')[1]) \n",
    "                     for x in e_updates.split(';')[1].split(',')})\n",
    "\n",
    "updates['g']['num_updates'] = int(g_updates.split(';')[0])\n",
    "updates['g'].update({x.split(':')[0]: float(x.split(':')[1]) \n",
    "                     for x in g_updates.split(';')[1].split(',')})\n",
    "\n",
    "\n",
    "###################\n",
    "\n",
    "image_size = 64 # 512\n",
    "batch_size = 128 # 64\n",
    "workers = 16\n",
    "shuffle = True\n",
    "drop_last = True\n",
    "train = True\n",
    "\n",
    "nc = 3\n",
    "nz = 64 #100 #'size of the latent z vector')\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "ngpu = 2 # 'number of GPUs to use')\n",
    "pin_memory = True # True | If ``True``, the data loader will copy tensors into CUDA pinned memory before returning them.\n",
    "\n",
    "noise = 'sphere' #help='normal|sphere')\n",
    "\n",
    "# netG = 'dcgan128px' \n",
    "# netE = 'dcgan128px' \n",
    "\n",
    "netG_chp = '' # \"path to netG (to continue training)\"\n",
    "netE_chp = '' # \"path to netE (to continue training)\"\n",
    "\n",
    "lr = 0.0002\n",
    "drop_lr = 100\n",
    "beta1 = 0.5 # help='beta1 for adam. default=0.5'\n",
    "criterion = 'param'# help='param|nonparam, How to estimate KL'\n",
    "KL ='qp' # help='pq|qp'\n",
    "match_z = 'cos' # help='none|L1|L2|cos'\n",
    "match_x = 'L1' # help='none|L1|L2|cos'\n",
    "\n",
    "start_epoch = 0\n",
    "nepoch = 1000\n",
    "\n",
    "save_every_e = 1\n",
    "save_every_b = None\n",
    "dataroot = '/media/dataserver/workspace/blanca/checkpoints/AGE-128/AGE/data/training/'\n",
    "\n",
    "run = '%d_%d_%d_nz%d_lrdlr_%d_%d' %(image_size, batch_size, nepoch, nz, lr, drop_lr); print(run)\n",
    "\n",
    "# --lr 0.0002 \n",
    "# --nz 64 \n",
    "# --batch_size 64 \n",
    "# --netG dcgan64px \n",
    "# --netE dcgan64px \n",
    "# --nepoch 5 \n",
    "# --drop_lr 5 \n",
    "# --e_updates '1;KL_fake:1,KL_real:1,match_z:0,match_x:10' \n",
    "# --g_updates '3;KL_fake:1,match_z:1000,match_x:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "##\n",
    "import torch.utils.data as data\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "def load_img(filepath):\n",
    "#     img = cv2.imread(filepath) #, cv2.IMREAD_UNCHANGED)\n",
    "    img = Image.open(filepath).convert('RGB')\n",
    "    return img\n",
    "\n",
    "class FolderWithImages(data.Dataset):\n",
    "    def __init__(self, root, input_transform=None, target_transform=None):\n",
    "        super(FolderWithImages, self).__init__()\n",
    "        self.image_filenames = [join(root, x)\n",
    "                                for x in listdir(root) if is_image_file(x.lower())]\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = load_img(self.image_filenames[index])\n",
    "        target = input.copy()\n",
    "        if self.input_transform:\n",
    "            input = self.input_transform(input)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return input, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    \n",
    "def setup_dataset(dataset, dataroot, train=True, shuffle=True, drop_last=True):\n",
    "    '''\n",
    "    Setups dataset.\n",
    "    '''\n",
    "    # Usual transform\n",
    "    t = transforms.Compose([\n",
    "        transforms.Scale([image_size, image_size]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    if dataset in ['imagenet', 'folder', 'lfw']:\n",
    "        imdir = 'train' if train else 'val'\n",
    "        dataroot = os.path.join(dataroot, imdir)\n",
    "\n",
    "        dataset = dset.ImageFolder(root=dataroot, transform=t)\n",
    "    elif dataset == 'lsun':\n",
    "        dataset = dset.LSUN(db_path=dataroot,\n",
    "                            classes=['bedroom_train'],\n",
    "                            train=train,\n",
    "                            transform=t)\n",
    "    elif dataset == 'cifar10':\n",
    "        dataset = dset.CIFAR10(root='data/raw/cifar10',\n",
    "                               download=True,\n",
    "                               train=train,\n",
    "                               transform=t\n",
    "                               )\n",
    "    elif dataset == 'mnist':\n",
    "        dataset = dset.MNIST(root='data/raw/mnist',\n",
    "                             download=True,\n",
    "                             train=train,\n",
    "                             transform=t\n",
    "                             )\n",
    "    elif dataset == 'svhn':\n",
    "        dataset = dset.SVHN(root='data/raw/svhn',\n",
    "                            download=True,\n",
    "                            train=train,\n",
    "                            transform=t)\n",
    "    elif dataset == 'celeba':\n",
    "        imdir = 'train' if train else 'val'\n",
    "        dataroot = os.path.join(dataroot, imdir)\n",
    "\n",
    "        dataset = FolderWithImages(root=dataroot,\n",
    "                                   input_transform=transforms.Compose([\n",
    "                                       ALICropAndScale(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                   ]),\n",
    "                                   target_transform=transforms.ToTensor()\n",
    "                                   )\n",
    "    elif dataset == 'helen':\n",
    "        imdir = 'train' if train else 'val'\n",
    "        dataroot = os.path.join(dataroot, imdir)\n",
    "\n",
    "        dataset = FolderWithImages(root=dataroot,\n",
    "                                   input_transform=transforms.Compose([\n",
    "                                       Scale(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                   ]),\n",
    "                                   target_transform=transforms.ToTensor()\n",
    "                                   )\n",
    "        \n",
    "    else:\n",
    "        imdir = 'train' if train else 'val'\n",
    "        dataroot = os.path.join(dataroot, imdir)\n",
    "\n",
    "        dataset = FolderWithImages(root=dataroot,\n",
    "                                   input_transform=transforms.Compose([\n",
    "                                       Scale(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                   ]),\n",
    "                                   target_transform=transforms.ToTensor()\n",
    "                                   )\n",
    "        \n",
    "#     else:\n",
    "#         assert False, 'Wrong dataset name.'\n",
    "\n",
    "    assert len(dataset) > 0, 'No images found, check your paths.'\n",
    "\n",
    "    # Shuffle and drop last when training\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=shuffle,\n",
    "                                             num_workers=int(workers),\n",
    "                                             pin_memory=pin_memory,\n",
    "                                             drop_last=drop_last)\n",
    "#     print(len(dataloader))\n",
    "#     for i in dataloader: print(i[0].shape, i[1].shape)\n",
    "        \n",
    "    return InfiniteDataLoader(dataloader)\n",
    "#     return dataloader\n",
    "\n",
    "class InfiniteDataLoader(object):\n",
    "    \"\"\"docstring for InfiniteDataLoader\"\"\"\n",
    "\n",
    "    def __init__(self, dataloader):\n",
    "        super(InfiniteDataLoader, self).__init__()\n",
    "        self.dataloader = dataloader\n",
    "        self.data_iter = None\n",
    "\n",
    "    def next(self):\n",
    "        try:\n",
    "            data = self.data_iter.next()\n",
    "        except Exception:\n",
    "            # Reached end of the dataset\n",
    "            self.data_iter = iter(self.dataloader)\n",
    "            data = self.data_iter.next()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "class ALICropAndScale(object):\n",
    "    def __call__(self, img):\n",
    "        return img.resize((64, 78), Image.ANTIALIAS).crop((0, 7, 64, 64 + 7))\n",
    "    \n",
    "class Scale(object):\n",
    "    def __call__(self, img):\n",
    "#         img = cv2.resize(img, (64, 64), cv2.INTER_AREA)\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         img = Image.fromarray(img)\n",
    "        img = img.resize((image_size, image_size), Image.ANTIALIAS)\n",
    "        return img \n",
    "\n",
    "# cv::INTER_AREA interpolation, whereas to\n",
    "# .   enlarge an image, it will generally look best with cv::INTER_CUBIC (slow) or cv::INTER_LINEAR\n",
    "# .   (faster but still looks OK).\n",
    "    \n",
    "# Setup dataset\n",
    "dataloader = dict(train=setup_dataset(dataset, dataroot, train=True),\n",
    "                  val=setup_dataset(dataset, dataroot, train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, dim=0):\n",
    "    '''\n",
    "    Projects points to a sphere.\n",
    "    '''\n",
    "    return x.div(x.norm(2, dim=dim).expand_as(x))\n",
    "\n",
    "def normalize_(x, dim=0):\n",
    "    '''\n",
    "    Projects points to a sphere inplace.\n",
    "    '''\n",
    "    x.div_(x.norm(2, dim=dim).expand_as(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator\n",
      " _netG_Base(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(64, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Encoder\n",
      " _netE_Base(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU(0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (7): LeakyReLU(0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (10): LeakyReLU(0.2, inplace)\n",
      "    (11): Conv2d(512, 64, kernel_size=(4, 4), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    '''\n",
    "    Custom weights initialization called on netG and netE\n",
    "    '''\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class _netE_Base(nn.Module):\n",
    "    def __init__(self, main):\n",
    "        super(_netE_Base, self).__init__()\n",
    "        self.noise = noise\n",
    "        self.ngpu = ngpu\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        gpu_ids = None\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 0:\n",
    "            gpu_ids = range(self.ngpu)\n",
    "            output = nn.parallel.data_parallel(self.main, input, gpu_ids)\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        if self.noise == 'sphere':\n",
    "            output = normalize(output)\n",
    "        return output\n",
    "    \n",
    "class _netG_Base(nn.Module):\n",
    "    def __init__(self, main):\n",
    "        super(_netG_Base, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # Check input is either (B,C,1,1) or (B,C)\n",
    "        assert input.nelement() == input.size(0) * input.size(1), 'wtf'\n",
    "        input = input.view(input.size(0), input.size(1), 1, 1)\n",
    "\n",
    "        gpu_ids = None\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 0:\n",
    "            gpu_ids = range(self.ngpu)\n",
    "            return nn.parallel.data_parallel(self.main, input, gpu_ids)\n",
    "        else:\n",
    "            return self.main(input)\n",
    "\n",
    "if image_size == 128:\n",
    "    def _netE():\n",
    "        main = nn.Sequential(\n",
    "            # input is (nc) x 128 x 128\n",
    "            # state size. (ndf) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 1 x 1\n",
    "            nn.Conv2d(ndf * 16, nz, 4, 1, 0, bias=True),\n",
    "        )\n",
    "\n",
    "        return _netE_Base(main)\n",
    "\n",
    "    def _netG():\n",
    "        main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(nz, ngf * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 64 x 64\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 128 x 128\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        return _netG_Base(main)\n",
    "\n",
    "elif image_size == 64:\n",
    "    def _netE():\n",
    "        main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 16 x 16\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 8 x 8\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 4 x 4\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, nz, 4, 1, 0, bias=True),\n",
    "        )\n",
    "        return _netE_Base(main)\n",
    "\n",
    "    def _netG():\n",
    "        main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "        return _netG_Base(main)\n",
    "    \n",
    "    \n",
    "def load_G():\n",
    "    '''\n",
    "    Loads generator model.\n",
    "    '''\n",
    "    netG = _netG()\n",
    "    netG.apply(weights_init)\n",
    "    netG.train()\n",
    "    if netG_chp != '':\n",
    "        netG.load_state_dict(torch.load(netG_chp).state_dict())\n",
    "\n",
    "    print('Generator\\n', netG)\n",
    "    return netG\n",
    "\n",
    "def load_E():\n",
    "    '''\n",
    "    Loads encoder model.\n",
    "    '''\n",
    "    netE = _netE()\n",
    "    netE.apply(weights_init)\n",
    "    netE.train()\n",
    "    if netE_chp != '':\n",
    "        netE.load_state_dict(torch.load(netE_chp).state_dict())\n",
    "\n",
    "    print('Encoder\\n', netE)\n",
    "\n",
    "    return netE\n",
    "\n",
    "# Load generator\n",
    "netG = load_G()\n",
    "\n",
    "# Load encoder\n",
    "netE = load_E()\n",
    "\n",
    "# RuntimeError: Given transposed=1, weight[64, 512, 4, 4], so expected input[64, 256, 1, 1] to have 64 channels,\n",
    "# but got 256 channels instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var(x, dim=0):\n",
    "    '''\n",
    "    Calculates variance.\n",
    "    '''\n",
    "    x_zero_meaned = x - x.mean(dim).expand_as(x)\n",
    "    return x_zero_meaned.pow(2).mean(dim)\n",
    "\n",
    "\n",
    "class KLN01Loss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, direction, minimize):\n",
    "        super(KLN01Loss, self).__init__()\n",
    "        self.minimize = minimize\n",
    "        assert direction in ['pq', 'qp'], 'direction?'\n",
    "\n",
    "        self.direction = direction\n",
    "\n",
    "    def forward(self, samples):\n",
    "\n",
    "        assert samples.nelement() == samples.size(1) * samples.size(0), 'wtf?'\n",
    "\n",
    "        samples = samples.view(samples.size(0), -1)\n",
    "\n",
    "        self.samples_var = var(samples)\n",
    "        self.samples_mean = samples.mean(0)\n",
    "\n",
    "        samples_mean = self.samples_mean\n",
    "        samples_var = self.samples_var\n",
    "\n",
    "        if self.direction == 'pq':\n",
    "            # mu_1 = 0; sigma_1 = 1\n",
    "\n",
    "            t1 = (1 + samples_mean.pow(2)) / (2 * samples_var.pow(2))\n",
    "            t2 = samples_var.log()\n",
    "\n",
    "            KL = (t1 + t2 - 0.5).mean()\n",
    "        else:\n",
    "            # mu_2 = 0; sigma_2 = 1\n",
    "\n",
    "            t1 = (samples_var.pow(2) + samples_mean.pow(2)) / 2\n",
    "            t2 = -samples_var.log()\n",
    "\n",
    "            KL = (t1 + t2 - 0.5).mean()\n",
    "\n",
    "        if not self.minimize:\n",
    "            KL *= -1\n",
    "\n",
    "        return KL\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "def pairwise_euclidean(samples):\n",
    "\n",
    "    B = samples.size(0)\n",
    "\n",
    "    samples_norm = samples.mul(samples).sum(1)\n",
    "    samples_norm = samples_norm.expand(B, B)\n",
    "\n",
    "    dist_mat = samples.mm(samples.t()).mul(-2) + \\\n",
    "        samples_norm.add(samples_norm.t())\n",
    "    return dist_mat\n",
    "\n",
    "def sample_entropy(samples):\n",
    "\n",
    "        # Assume B x C input\n",
    "\n",
    "    dist_mat = pairwise_euclidean(samples)\n",
    "\n",
    "    # Get max and add it to diag\n",
    "    m = dist_mat.max().detach()\n",
    "    dist_mat_d = dist_mat + \\\n",
    "        Variable(torch.eye(dist_mat.size(0)) * (m.data[0] + 1)).cuda()\n",
    "\n",
    "    entropy = (dist_mat_d.min(1)[0] + 1e-4).log().sum()\n",
    "\n",
    "    entropy *= (samples.size(1) + 0.) / samples.size(0)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "class SampleKLN01Loss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, direction, minimize):\n",
    "        super(SampleKLN01Loss, self).__init__()\n",
    "        self.minimize = minimize\n",
    "        assert direction in ['pq', 'qp'], 'direction?'\n",
    "\n",
    "        self.direction = direction\n",
    "\n",
    "    def forward(self, samples):\n",
    "\n",
    "        assert samples.ndimension == 2, 'wft'\n",
    "        samples = samples.view(samples.size(0), -1)\n",
    "\n",
    "        self.samples_var = var(samples)\n",
    "        self.samples_mean = samples.mean(0)\n",
    "\n",
    "        if self.direction == 'pq':\n",
    "            assert False, 'not possible'\n",
    "        else:\n",
    "            entropy = sample_entropy(samples)\n",
    "\n",
    "            cross_entropy = - samples.pow(2).mean() / 2.\n",
    "\n",
    "            KL = - cross_entropy - entropy\n",
    "\n",
    "        if not self.minimize:\n",
    "            KL *= -1\n",
    "\n",
    "        return KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.FloatTensor(batch_size, nc, image_size, image_size)\n",
    "# z = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "# fixed_z = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "# z = fixed_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parametric criterion KL_qp\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(batch_size, nc, image_size, image_size)\n",
    "z = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "fixed_z = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "\n",
    "def match(x, y, dist):\n",
    "    '''\n",
    "    Computes distance between corresponding points in `x` and `y`\n",
    "    using distance `dist`.\n",
    "    '''\n",
    "    if dist == 'L2':\n",
    "        return (x - y).pow(2).mean()\n",
    "    elif dist == 'L1':\n",
    "        return (x - y).abs().mean()\n",
    "    elif dist == 'cos':\n",
    "        x_n = normalize(x)\n",
    "        y_n = normalize(y)\n",
    "\n",
    "        return 2 - (x_n).mul(y_n).mean()\n",
    "    else:\n",
    "        assert dist == 'none', 'wtf ?'\n",
    "\n",
    "def normalize(x, dim=0):\n",
    "    '''\n",
    "    Projects points to a sphere.\n",
    "    '''\n",
    "    return x.div(x.norm(2, dim=dim).expand_as(x))\n",
    "\n",
    "def normalize_(x, dim=0):\n",
    "    '''\n",
    "    Projects points to a sphere inplace.\n",
    "    '''\n",
    "    x.div_(x.norm(2, dim=dim).expand_as(x))\n",
    "\n",
    "if noise == 'sphere':\n",
    "    normalize_(fixed_z)\n",
    "\n",
    "if cuda:\n",
    "    netE.cuda()\n",
    "    netG.cuda()\n",
    "    x = x.cuda()\n",
    "    z, fixed_z = z.cuda(), fixed_z.cuda()\n",
    "\n",
    "x = Variable(x)\n",
    "z = Variable(z)\n",
    "fixed_z = Variable(fixed_z)\n",
    "\n",
    "# Setup optimizers\n",
    "optimizerD = optim.Adam(netE.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Setup criterions\n",
    "if criterion == 'param':\n",
    "    print('Using parametric criterion KL_%s' % KL)\n",
    "    KL_minimizer = KLN01Loss(direction=KL, minimize=True)\n",
    "    KL_maximizer = KLN01Loss(direction=KL, minimize=False)\n",
    "elif criterion == 'nonparam':\n",
    "    print('Using NON-parametric criterion KL_%s' % KL)\n",
    "    KL_minimizer = SampleKLN01Loss(direction=KL, minimize=True)\n",
    "    KL_maximizer = SampleKLN01Loss(direction=KL, minimize=False)\n",
    "else:\n",
    "    assert False, 'criterion?'\n",
    "\n",
    "real_cpu = torch.FloatTensor()\n",
    "\n",
    "\n",
    "def save_image(tensor, filename, nrow=8, padding=2,\n",
    "               normalize=False, range=None, scale_each=False, pad_value=0):\n",
    "    \"\"\"Save a given Tensor into an image file.\n",
    "    Args:\n",
    "        tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,\n",
    "            saves the tensor as a grid of images by calling ``make_grid``.\n",
    "        **kwargs: Other arguments are documented in ``make_grid``.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    grid = vutils.make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value,\n",
    "                     normalize=normalize, range=range, scale_each=scale_each)\n",
    "    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy() # \n",
    "    im = Image.fromarray(ndarr)\n",
    "#     im = Image.fromarray(ndarr[:,:,::-1])    \n",
    "    im.save(filename)\n",
    "\n",
    "\n",
    "def save_images(epoch):\n",
    "\n",
    "    real_cpu.resize_(x.data.size()).copy_(x.data)\n",
    "\n",
    "    # Real samples\n",
    "    save_path = '%s/real_samples.png' % save_dir\n",
    "    save_image(real_cpu[:64] / 2 + 0.5, save_path)\n",
    "\n",
    "    netG.eval()\n",
    "    fake = netG(fixed_z)\n",
    "\n",
    "    # Fake samples\n",
    "    save_path = '%s/fake_samples_epoch_%03d.png' % (save_dir, epoch)\n",
    "    save_image(fake.data[:64] / 2 + 0.5, save_path)\n",
    "\n",
    "    # Save reconstructions\n",
    "    populate_x(x, dataloader['val'])\n",
    "    gex = netG(netE(x)) # here - at G entry\n",
    "\n",
    "    t = torch.FloatTensor(x.size(0) * 2, x.size(1),\n",
    "                          x.size(2), x.size(3))\n",
    "\n",
    "    t[0::2] = x.data[:]\n",
    "    t[1::2] = gex.data[:]\n",
    "\n",
    "    save_path = '%s/reconstructions_epoch_%03d.png' % (save_dir, epoch)\n",
    "    grid = save_image(t[:64] / 2 + 0.5, save_path)\n",
    "    \n",
    "    netG.train()\n",
    "\n",
    "def adjust_lr(epoch, drop_coef=2):\n",
    "    if (epoch + 1) % (drop_lr + 1) == 0:\n",
    "        ###\n",
    "        assert optimizerD.param_groups[0]['lr'] == optimizerG.param_groups[0]['lr']\n",
    "        lr = optimizerD.param_groups[0]['lr']\n",
    "        print('Adjusting learning rate from %f to %f on E and G' % (lr, lr / drop_coef))\n",
    "        optimizerD.param_groups[0]['lr'] /= drop_coef\n",
    "        optimizerG.param_groups[0]['lr'] /= drop_coef\n",
    "        ###\n",
    "#         lr /= 2\n",
    "#         for param_group in optimizerD.param_groups:\n",
    "#             param_group['lr'] = lr\n",
    "\n",
    "#         for param_group in optimizerG.param_groups:\n",
    "#             param_group['lr'] = lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_x(x, dataloader):\n",
    "    '''\n",
    "    Fills input variable `x` with data generated with dataloader\n",
    "    '''\n",
    "    real_cpu, _ = dataloader.next()\n",
    "    x.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "\n",
    "def populate_z(z):\n",
    "    '''\n",
    "    Fills noise variable `z` with noise U(S^M)\n",
    "    '''\n",
    "    z.data.resize_(batch_size, nz, 1, 1)\n",
    "    z.data.normal_(0, 1)\n",
    "    if noise == 'sphere':\n",
    "        normalize_(z.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(dataloader['train'].next()))\n",
    "# print(len(dataloader['val'].next()))\n",
    "# print(dataloader['train'].next()[0].shape, dataloader['train'].next()[1].shape)\n",
    "# print(dataloader['train'].next()[0].shape, dataloader['train'].next()[1].shape)\n",
    "\n",
    "# populate_x(x, dataloader['val'])\n",
    "# nepoch = 10\n",
    "# lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1000][15/16] KL_real/fake: 4.378/4.386 mean_real/fake: -0.002/-0.005 var_real/fake: 0.008/0.008 \n",
      "[1/1000][15/16] KL_real/fake: 4.464/4.938 mean_real/fake: -0.008/-0.011 var_real/fake: 0.007/0.005 \n",
      "[2/1000][15/16] KL_real/fake: 4.837/5.249 mean_real/fake: -0.008/-0.007 var_real/fake: 0.005/0.004 \n",
      "[3/1000][15/16] KL_real/fake: 5.010/4.934 mean_real/fake: -0.014/-0.012 var_real/fake: 0.005/0.005 \n",
      "[4/1000][15/16] KL_real/fake: 4.934/4.912 mean_real/fake: -0.015/-0.012 var_real/fake: 0.005/0.005 \n",
      "[5/1000][15/16] KL_real/fake: 4.984/4.958 mean_real/fake: -0.014/-0.013 var_real/fake: 0.005/0.005 \n",
      "[6/1000][15/16] KL_real/fake: 4.930/4.882 mean_real/fake: -0.012/-0.011 var_real/fake: 0.005/0.005 \n",
      "[7/1000][15/16] KL_real/fake: 4.948/4.908 mean_real/fake: -0.012/-0.010 var_real/fake: 0.005/0.005 \n",
      "[8/1000][15/16] KL_real/fake: 4.896/4.846 mean_real/fake: -0.010/-0.009 var_real/fake: 0.006/0.006 \n",
      "[9/1000][15/16] KL_real/fake: 4.916/4.883 mean_real/fake: -0.012/-0.011 var_real/fake: 0.005/0.006 \n",
      "[10/1000][15/16] KL_real/fake: 4.876/4.865 mean_real/fake: -0.012/-0.012 var_real/fake: 0.006/0.006 \n",
      "[11/1000][15/16] KL_real/fake: 4.862/4.829 mean_real/fake: -0.009/-0.011 var_real/fake: 0.006/0.006 \n",
      "[12/1000][15/16] KL_real/fake: 4.897/4.874 mean_real/fake: -0.012/-0.010 var_real/fake: 0.005/0.006 \n",
      "[13/1000][15/16] KL_real/fake: 4.856/4.833 mean_real/fake: -0.014/-0.013 var_real/fake: 0.006/0.006 \n",
      "[14/1000][15/16] KL_real/fake: 4.818/4.814 mean_real/fake: -0.012/-0.012 var_real/fake: 0.006/0.006 \n",
      "[15/1000][15/16] KL_real/fake: 4.866/4.855 mean_real/fake: -0.010/-0.010 var_real/fake: 0.006/0.006 \n",
      "[16/1000][15/16] KL_real/fake: 4.865/4.848 mean_real/fake: -0.012/-0.011 var_real/fake: 0.005/0.006 \n",
      "[17/1000][15/16] KL_real/fake: 4.851/4.825 mean_real/fake: -0.009/-0.010 var_real/fake: 0.006/0.006 \n",
      "[18/1000][15/16] KL_real/fake: 4.833/4.808 mean_real/fake: -0.011/-0.010 var_real/fake: 0.006/0.006 \n",
      "[19/1000][15/16] KL_real/fake: 4.841/4.813 mean_real/fake: -0.009/-0.011 var_real/fake: 0.006/0.006 \n",
      "[20/1000][15/16] KL_real/fake: 4.837/4.805 mean_real/fake: -0.013/-0.010 var_real/fake: 0.006/0.006 \n",
      "[21/1000][15/16] KL_real/fake: 4.816/4.809 mean_real/fake: -0.008/-0.010 var_real/fake: 0.006/0.006 \n",
      "[22/1000][15/16] KL_real/fake: 4.752/4.732 mean_real/fake: -0.010/-0.009 var_real/fake: 0.006/0.006 \n",
      "[23/1000][15/16] KL_real/fake: 4.727/4.715 mean_real/fake: -0.008/-0.008 var_real/fake: 0.006/0.006 \n",
      "[24/1000][15/16] KL_real/fake: 4.768/4.740 mean_real/fake: -0.006/-0.008 var_real/fake: 0.006/0.006 \n",
      "[25/1000][15/16] KL_real/fake: 4.776/4.758 mean_real/fake: -0.014/-0.011 var_real/fake: 0.006/0.006 \n",
      "[26/1000][15/16] KL_real/fake: 4.738/4.722 mean_real/fake: -0.009/-0.009 var_real/fake: 0.006/0.006 \n",
      "[27/1000][15/16] KL_real/fake: 4.761/4.739 mean_real/fake: -0.008/-0.010 var_real/fake: 0.006/0.006 \n",
      "[28/1000][15/16] KL_real/fake: 4.774/4.749 mean_real/fake: -0.008/-0.008 var_real/fake: 0.006/0.006 \n",
      "[29/1000][15/16] KL_real/fake: 4.734/4.704 mean_real/fake: -0.010/-0.009 var_real/fake: 0.006/0.006 \n",
      "[30/1000][15/16] KL_real/fake: 4.738/4.718 mean_real/fake: -0.007/-0.009 var_real/fake: 0.006/0.006 \n",
      "[31/1000][15/16] KL_real/fake: 4.766/4.733 mean_real/fake: -0.011/-0.009 var_real/fake: 0.006/0.006 \n",
      "[32/1000][15/16] KL_real/fake: 4.764/4.739 mean_real/fake: -0.013/-0.010 var_real/fake: 0.006/0.006 \n",
      "[33/1000][15/16] KL_real/fake: 4.722/4.706 mean_real/fake: -0.010/-0.009 var_real/fake: 0.006/0.006 \n",
      "[34/1000][15/16] KL_real/fake: 4.729/4.718 mean_real/fake: -0.009/-0.009 var_real/fake: 0.006/0.006 \n",
      "[35/1000][15/16] KL_real/fake: 4.728/4.711 mean_real/fake: -0.007/-0.008 var_real/fake: 0.006/0.006 \n",
      "[36/1000][15/16] KL_real/fake: 4.682/4.659 mean_real/fake: -0.007/-0.008 var_real/fake: 0.006/0.006 \n",
      "[37/1000][15/16] KL_real/fake: 4.717/4.698 mean_real/fake: -0.010/-0.008 var_real/fake: 0.006/0.006 \n",
      "[38/1000][15/16] KL_real/fake: 4.695/4.685 mean_real/fake: -0.010/-0.008 var_real/fake: 0.006/0.006 \n",
      "[39/1000][15/16] KL_real/fake: 4.678/4.661 mean_real/fake: -0.009/-0.009 var_real/fake: 0.006/0.006 \n",
      "[40/1000][15/16] KL_real/fake: 4.703/4.697 mean_real/fake: -0.007/-0.009 var_real/fake: 0.006/0.006 \n",
      "[41/1000][15/16] KL_real/fake: 4.670/4.651 mean_real/fake: -0.007/-0.006 var_real/fake: 0.006/0.006 \n",
      "[42/1000][15/16] KL_real/fake: 4.662/4.651 mean_real/fake: -0.006/-0.007 var_real/fake: 0.006/0.006 \n",
      "[43/1000][15/16] KL_real/fake: 4.670/4.659 mean_real/fake: -0.004/-0.007 var_real/fake: 0.006/0.006 \n",
      "[44/1000][15/16] KL_real/fake: 4.690/4.660 mean_real/fake: -0.006/-0.005 var_real/fake: 0.006/0.006 \n",
      "[45/1000][15/16] KL_real/fake: 4.671/4.656 mean_real/fake: -0.007/-0.007 var_real/fake: 0.006/0.006 \n",
      "[46/1000][15/16] KL_real/fake: 4.668/4.645 mean_real/fake: -0.002/-0.003 var_real/fake: 0.006/0.006 \n",
      "[47/1000][15/16] KL_real/fake: 4.638/4.619 mean_real/fake: -0.002/-0.002 var_real/fake: 0.006/0.006 \n",
      "[48/1000][15/16] KL_real/fake: 4.647/4.624 mean_real/fake: -0.002/-0.002 var_real/fake: 0.006/0.006 \n",
      "[49/1000][15/16] KL_real/fake: 4.658/4.653 mean_real/fake: -0.002/-0.004 var_real/fake: 0.006/0.006 \n",
      "[50/1000][15/16] KL_real/fake: 4.671/4.652 mean_real/fake: -0.003/-0.002 var_real/fake: 0.006/0.006 \n",
      "[51/1000][15/16] KL_real/fake: 4.643/4.635 mean_real/fake: -0.003/-0.003 var_real/fake: 0.006/0.006 \n",
      "[52/1000][15/16] KL_real/fake: 4.629/4.609 mean_real/fake: -0.004/-0.004 var_real/fake: 0.006/0.006 \n",
      "[53/1000][15/16] KL_real/fake: 4.631/4.625 mean_real/fake: -0.002/-0.001 var_real/fake: 0.006/0.006 \n",
      "[54/1000][15/16] KL_real/fake: 4.665/4.640 mean_real/fake: -0.004/-0.003 var_real/fake: 0.006/0.006 \n",
      "[55/1000][15/16] KL_real/fake: 4.656/4.645 mean_real/fake: -0.004/-0.001 var_real/fake: 0.006/0.006 \n",
      "[56/1000][15/16] KL_real/fake: 4.662/4.639 mean_real/fake: -0.003/-0.002 var_real/fake: 0.006/0.006 \n",
      "[57/1000][15/16] KL_real/fake: 4.620/4.596 mean_real/fake: -0.000/-0.004 var_real/fake: 0.006/0.006 \n",
      "[58/1000][15/16] KL_real/fake: 4.669/4.653 mean_real/fake: -0.001/-0.003 var_real/fake: 0.006/0.006 \n",
      "[59/1000][15/16] KL_real/fake: 4.626/4.613 mean_real/fake: -0.002/-0.000 var_real/fake: 0.006/0.006 \n",
      "[60/1000][15/16] KL_real/fake: 4.635/4.618 mean_real/fake: -0.001/-0.001 var_real/fake: 0.006/0.006 \n",
      "[61/1000][15/16] KL_real/fake: 4.693/4.659 mean_real/fake: -0.003/-0.002 var_real/fake: 0.006/0.006 \n",
      "[62/1000][15/16] KL_real/fake: 4.645/4.619 mean_real/fake: -0.002/-0.003 var_real/fake: 0.006/0.006 \n",
      "[63/1000][15/16] KL_real/fake: 4.647/4.623 mean_real/fake: 0.000/-0.003 var_real/fake: 0.006/0.006 \n",
      "[64/1000][15/16] KL_real/fake: 4.645/4.633 mean_real/fake: -0.005/-0.003 var_real/fake: 0.006/0.006 \n",
      "[65/1000][15/16] KL_real/fake: 4.635/4.623 mean_real/fake: -0.002/-0.002 var_real/fake: 0.006/0.006 \n",
      "[66/1000][15/16] KL_real/fake: 4.665/4.643 mean_real/fake: -0.004/-0.003 var_real/fake: 0.006/0.006 \n",
      "[67/1000][15/16] KL_real/fake: 4.656/4.641 mean_real/fake: -0.002/-0.002 var_real/fake: 0.006/0.006 \n",
      "[68/1000][15/16] KL_real/fake: 4.638/4.630 mean_real/fake: -0.003/-0.002 var_real/fake: 0.006/0.006 \n",
      "[69/1000][15/16] KL_real/fake: 4.632/4.618 mean_real/fake: -0.000/-0.003 var_real/fake: 0.006/0.006 \n",
      "[70/1000][15/16] KL_real/fake: 4.659/4.637 mean_real/fake: -0.000/-0.001 var_real/fake: 0.006/0.006 \n",
      "[71/1000][15/16] KL_real/fake: 4.654/4.645 mean_real/fake: -0.002/-0.001 var_real/fake: 0.006/0.006 \n",
      "[72/1000][15/16] KL_real/fake: 4.639/4.624 mean_real/fake: -0.002/-0.003 var_real/fake: 0.006/0.006 \n",
      "[73/1000][15/16] KL_real/fake: 4.626/4.609 mean_real/fake: 0.001/-0.002 var_real/fake: 0.006/0.006 \n",
      "[74/1000][15/16] KL_real/fake: 4.645/4.623 mean_real/fake: -0.003/-0.004 var_real/fake: 0.006/0.006 \n",
      "[75/1000][15/16] KL_real/fake: 4.626/4.607 mean_real/fake: 0.001/-0.001 var_real/fake: 0.006/0.006 \n",
      "[76/1000][15/16] KL_real/fake: 4.625/4.604 mean_real/fake: 0.001/0.000 var_real/fake: 0.006/0.006 \n",
      "[77/1000][15/16] KL_real/fake: 4.604/4.591 mean_real/fake: -0.001/-0.001 var_real/fake: 0.006/0.006 \n",
      "[78/1000][15/16] KL_real/fake: 4.612/4.602 mean_real/fake: 0.002/-0.001 var_real/fake: 0.006/0.006 \n",
      "[79/1000][15/16] KL_real/fake: 4.610/4.594 mean_real/fake: 0.000/0.001 var_real/fake: 0.006/0.006 \n",
      "[80/1000][15/16] KL_real/fake: 4.589/4.570 mean_real/fake: -0.001/-0.001 var_real/fake: 0.006/0.007 \n",
      "[81/1000][15/16] KL_real/fake: 4.583/4.572 mean_real/fake: -0.003/-0.001 var_real/fake: 0.006/0.006 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82/1000][15/16] KL_real/fake: 4.581/4.572 mean_real/fake: 0.001/-0.001 var_real/fake: 0.006/0.006 \n",
      "[83/1000][15/16] KL_real/fake: 4.576/4.564 mean_real/fake: -0.001/-0.002 var_real/fake: 0.007/0.007 \n",
      "[84/1000][15/16] KL_real/fake: 4.572/4.561 mean_real/fake: -0.005/-0.002 var_real/fake: 0.006/0.007 \n",
      "[85/1000][15/16] KL_real/fake: 4.588/4.573 mean_real/fake: 0.002/0.001 var_real/fake: 0.006/0.007 \n",
      "[86/1000][15/16] KL_real/fake: 4.594/4.577 mean_real/fake: -0.000/0.000 var_real/fake: 0.006/0.006 \n",
      "[87/1000][15/16] KL_real/fake: 4.591/4.577 mean_real/fake: -0.002/-0.002 var_real/fake: 0.006/0.006 \n",
      "[88/1000][15/16] KL_real/fake: 4.597/4.581 mean_real/fake: -0.002/-0.001 var_real/fake: 0.006/0.006 \n",
      "[89/1000][15/16] KL_real/fake: 4.571/4.561 mean_real/fake: -0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[90/1000][15/16] KL_real/fake: 4.588/4.573 mean_real/fake: 0.002/0.001 var_real/fake: 0.006/0.006 \n",
      "[91/1000][15/16] KL_real/fake: 4.585/4.570 mean_real/fake: -0.002/-0.003 var_real/fake: 0.006/0.006 \n",
      "[92/1000][15/16] KL_real/fake: 4.576/4.567 mean_real/fake: -0.000/0.001 var_real/fake: 0.006/0.007 \n",
      "[93/1000][15/16] KL_real/fake: 4.585/4.564 mean_real/fake: -0.004/0.001 var_real/fake: 0.006/0.007 \n",
      "[94/1000][15/16] KL_real/fake: 4.572/4.558 mean_real/fake: -0.002/0.000 var_real/fake: 0.006/0.007 \n",
      "[95/1000][15/16] KL_real/fake: 4.551/4.542 mean_real/fake: -0.001/-0.002 var_real/fake: 0.007/0.007 \n",
      "[96/1000][15/16] KL_real/fake: 4.562/4.555 mean_real/fake: -0.001/-0.001 var_real/fake: 0.007/0.007 \n",
      "[97/1000][15/16] KL_real/fake: 4.564/4.557 mean_real/fake: -0.001/0.000 var_real/fake: 0.007/0.007 \n",
      "[98/1000][15/16] KL_real/fake: 4.568/4.556 mean_real/fake: -0.001/-0.001 var_real/fake: 0.007/0.007 \n",
      "[99/1000][15/16] KL_real/fake: 4.560/4.544 mean_real/fake: 0.003/0.002 var_real/fake: 0.007/0.007 \n",
      "Adjusting learning rate from 0.000200 to 0.000100 on E and G\n",
      "[100/1000][15/16] KL_real/fake: 4.552/4.547 mean_real/fake: 0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[101/1000][15/16] KL_real/fake: 4.554/4.552 mean_real/fake: -0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[102/1000][15/16] KL_real/fake: 4.555/4.549 mean_real/fake: -0.001/-0.001 var_real/fake: 0.007/0.007 \n",
      "[103/1000][15/16] KL_real/fake: 4.546/4.539 mean_real/fake: -0.000/-0.000 var_real/fake: 0.007/0.007 \n",
      "[104/1000][15/16] KL_real/fake: 4.549/4.541 mean_real/fake: -0.001/0.000 var_real/fake: 0.007/0.007 \n",
      "[105/1000][15/16] KL_real/fake: 4.539/4.537 mean_real/fake: -0.001/-0.001 var_real/fake: 0.007/0.007 \n",
      "[106/1000][15/16] KL_real/fake: 4.545/4.541 mean_real/fake: -0.001/0.002 var_real/fake: 0.007/0.007 \n",
      "[107/1000][15/16] KL_real/fake: 4.533/4.527 mean_real/fake: 0.002/-0.003 var_real/fake: 0.007/0.007 \n",
      "[108/1000][15/16] KL_real/fake: 4.536/4.531 mean_real/fake: 0.000/0.002 var_real/fake: 0.007/0.007 \n",
      "[109/1000][15/16] KL_real/fake: 4.530/4.523 mean_real/fake: 0.001/-0.003 var_real/fake: 0.007/0.007 \n",
      "[110/1000][15/16] KL_real/fake: 4.539/4.526 mean_real/fake: -0.000/-0.001 var_real/fake: 0.007/0.007 \n",
      "[111/1000][15/16] KL_real/fake: 4.530/4.521 mean_real/fake: 0.001/0.000 var_real/fake: 0.007/0.007 \n",
      "[112/1000][15/16] KL_real/fake: 4.526/4.521 mean_real/fake: -0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[113/1000][15/16] KL_real/fake: 4.532/4.524 mean_real/fake: -0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[114/1000][15/16] KL_real/fake: 4.523/4.519 mean_real/fake: -0.002/0.000 var_real/fake: 0.007/0.007 \n",
      "[115/1000][15/16] KL_real/fake: 4.524/4.521 mean_real/fake: 0.000/0.000 var_real/fake: 0.007/0.007 \n",
      "[116/1000][15/16] KL_real/fake: 4.528/4.523 mean_real/fake: 0.002/-0.003 var_real/fake: 0.007/0.007 \n",
      "[117/1000][15/16] KL_real/fake: 4.532/4.523 mean_real/fake: -0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[118/1000][15/16] KL_real/fake: 4.520/4.512 mean_real/fake: 0.000/-0.001 var_real/fake: 0.007/0.007 \n",
      "[119/1000][15/16] KL_real/fake: 4.525/4.519 mean_real/fake: -0.000/-0.001 var_real/fake: 0.007/0.007 \n",
      "[120/1000][15/16] KL_real/fake: 4.515/4.507 mean_real/fake: 0.000/0.000 var_real/fake: 0.007/0.007 \n",
      "[121/1000][15/16] KL_real/fake: 4.516/4.511 mean_real/fake: 0.001/-0.002 var_real/fake: 0.007/0.007 \n",
      "[122/1000][15/16] KL_real/fake: 4.508/4.502 mean_real/fake: 0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[123/1000][15/16] KL_real/fake: 4.509/4.496 mean_real/fake: 0.001/0.000 var_real/fake: 0.007/0.007 \n",
      "[124/1000][15/16] KL_real/fake: 4.517/4.507 mean_real/fake: -0.001/0.000 var_real/fake: 0.007/0.007 \n",
      "[125/1000][15/16] KL_real/fake: 4.505/4.496 mean_real/fake: 0.000/-0.001 var_real/fake: 0.007/0.007 \n",
      "[126/1000][15/16] KL_real/fake: 4.508/4.499 mean_real/fake: 0.000/0.001 var_real/fake: 0.007/0.007 \n",
      "[127/1000][15/16] KL_real/fake: 4.506/4.500 mean_real/fake: -0.003/-0.001 var_real/fake: 0.007/0.007 \n",
      "[128/1000][15/16] KL_real/fake: 4.509/4.502 mean_real/fake: 0.001/-0.000 var_real/fake: 0.007/0.007 \n",
      "[129/1000][15/16] KL_real/fake: 4.498/4.494 mean_real/fake: 0.000/0.002 var_real/fake: 0.007/0.007 \n",
      "[130/1000][15/16] KL_real/fake: 4.511/4.502 mean_real/fake: -0.002/-0.000 var_real/fake: 0.007/0.007 \n",
      "[131/1000][15/16] KL_real/fake: 4.513/4.505 mean_real/fake: -0.000/0.000 var_real/fake: 0.007/0.007 \n",
      "[132/1000][15/16] KL_real/fake: 4.495/4.490 mean_real/fake: -0.000/-0.000 var_real/fake: 0.007/0.007 \n",
      "[133/1000][15/16] KL_real/fake: 4.500/4.495 mean_real/fake: -0.000/-0.001 var_real/fake: 0.007/0.007 \n",
      "[134/1000][15/16] KL_real/fake: 4.514/4.489 mean_real/fake: 0.001/-0.001 var_real/fake: 0.007/0.007 \n",
      "[135/1000][15/16] KL_real/fake: 4.500/4.485 mean_real/fake: 0.000/-0.000 var_real/fake: 0.007/0.007 \n",
      "[136/1000][15/16] KL_real/fake: 4.495/4.484 mean_real/fake: 0.001/-0.002 var_real/fake: 0.007/0.007 \n",
      "[137/1000][15/16] KL_real/fake: 4.513/4.503 mean_real/fake: 0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[138/1000][15/16] KL_real/fake: 4.496/4.475 mean_real/fake: 0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[139/1000][15/16] KL_real/fake: 4.518/4.499 mean_real/fake: 0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[140/1000][15/16] KL_real/fake: 4.479/4.462 mean_real/fake: 0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[141/1000][15/16] KL_real/fake: 4.494/4.481 mean_real/fake: -0.001/-0.001 var_real/fake: 0.007/0.007 \n",
      "[142/1000][15/16] KL_real/fake: 4.467/4.460 mean_real/fake: -0.000/0.000 var_real/fake: 0.007/0.007 \n",
      "[143/1000][15/16] KL_real/fake: 4.500/4.480 mean_real/fake: 0.003/-0.001 var_real/fake: 0.007/0.007 \n",
      "[144/1000][15/16] KL_real/fake: 4.470/4.467 mean_real/fake: 0.002/0.003 var_real/fake: 0.007/0.007 \n",
      "[145/1000][15/16] KL_real/fake: 4.478/4.472 mean_real/fake: 0.002/0.000 var_real/fake: 0.007/0.007 \n",
      "[146/1000][15/16] KL_real/fake: 4.473/4.470 mean_real/fake: -0.000/0.001 var_real/fake: 0.007/0.007 \n",
      "[147/1000][15/16] KL_real/fake: 4.474/4.465 mean_real/fake: 0.000/0.000 var_real/fake: 0.007/0.007 \n",
      "[148/1000][15/16] KL_real/fake: 4.459/4.449 mean_real/fake: -0.000/0.000 var_real/fake: 0.007/0.007 \n",
      "[149/1000][15/16] KL_real/fake: 4.466/4.462 mean_real/fake: 0.002/0.003 var_real/fake: 0.007/0.007 \n",
      "[150/1000][15/16] KL_real/fake: 4.470/4.465 mean_real/fake: -0.002/-0.001 var_real/fake: 0.007/0.007 \n",
      "[151/1000][15/16] KL_real/fake: 4.472/4.459 mean_real/fake: 0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[152/1000][15/16] KL_real/fake: 4.458/4.451 mean_real/fake: -0.000/-0.001 var_real/fake: 0.007/0.007 \n",
      "[153/1000][15/16] KL_real/fake: 4.474/4.462 mean_real/fake: 0.002/0.000 var_real/fake: 0.007/0.007 \n",
      "[154/1000][15/16] KL_real/fake: 4.453/4.445 mean_real/fake: 0.002/0.001 var_real/fake: 0.007/0.007 \n",
      "[155/1000][15/16] KL_real/fake: 4.458/4.448 mean_real/fake: 0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[156/1000][15/16] KL_real/fake: 4.457/4.452 mean_real/fake: 0.000/-0.000 var_real/fake: 0.007/0.007 \n",
      "[157/1000][15/16] KL_real/fake: 4.454/4.450 mean_real/fake: 0.001/-0.000 var_real/fake: 0.007/0.007 \n",
      "[158/1000][15/16] KL_real/fake: 4.465/4.444 mean_real/fake: 0.002/0.000 var_real/fake: 0.007/0.007 \n",
      "[159/1000][15/16] KL_real/fake: 4.467/4.455 mean_real/fake: 0.004/-0.000 var_real/fake: 0.007/0.007 \n",
      "[160/1000][15/16] KL_real/fake: 4.458/4.438 mean_real/fake: -0.000/-0.000 var_real/fake: 0.007/0.007 \n",
      "[161/1000][15/16] KL_real/fake: 4.449/4.446 mean_real/fake: -0.001/-0.001 var_real/fake: 0.007/0.007 \n",
      "[162/1000][15/16] KL_real/fake: 4.449/4.443 mean_real/fake: -0.001/-0.000 var_real/fake: 0.007/0.007 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[163/1000][15/16] KL_real/fake: 4.463/4.453 mean_real/fake: -0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[164/1000][15/16] KL_real/fake: 4.455/4.444 mean_real/fake: 0.002/0.001 var_real/fake: 0.007/0.007 \n",
      "[165/1000][15/16] KL_real/fake: 4.444/4.441 mean_real/fake: -0.000/0.001 var_real/fake: 0.007/0.007 \n",
      "[166/1000][15/16] KL_real/fake: 4.452/4.441 mean_real/fake: -0.000/0.002 var_real/fake: 0.007/0.007 \n",
      "[167/1000][15/16] KL_real/fake: 4.445/4.439 mean_real/fake: 0.002/0.001 var_real/fake: 0.007/0.007 \n",
      "[168/1000][15/16] KL_real/fake: 4.446/4.440 mean_real/fake: -0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[169/1000][15/16] KL_real/fake: 4.454/4.441 mean_real/fake: 0.000/-0.000 var_real/fake: 0.007/0.007 \n",
      "[170/1000][15/16] KL_real/fake: 4.450/4.438 mean_real/fake: 0.002/0.001 var_real/fake: 0.007/0.007 \n",
      "[171/1000][15/16] KL_real/fake: 4.447/4.441 mean_real/fake: 0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[172/1000][15/16] KL_real/fake: 4.444/4.437 mean_real/fake: -0.001/0.000 var_real/fake: 0.007/0.007 \n",
      "[173/1000][15/16] KL_real/fake: 4.451/4.442 mean_real/fake: 0.001/-0.002 var_real/fake: 0.007/0.007 \n",
      "[174/1000][15/16] KL_real/fake: 4.442/4.436 mean_real/fake: 0.002/-0.001 var_real/fake: 0.007/0.007 \n",
      "[175/1000][15/16] KL_real/fake: 4.448/4.442 mean_real/fake: -0.003/0.002 var_real/fake: 0.007/0.007 \n",
      "[176/1000][15/16] KL_real/fake: 4.437/4.430 mean_real/fake: -0.001/-0.000 var_real/fake: 0.007/0.007 \n",
      "[177/1000][15/16] KL_real/fake: 4.434/4.429 mean_real/fake: -0.003/0.001 var_real/fake: 0.007/0.007 \n",
      "[178/1000][15/16] KL_real/fake: 4.434/4.427 mean_real/fake: -0.000/-0.001 var_real/fake: 0.007/0.007 \n",
      "[179/1000][15/16] KL_real/fake: 4.427/4.422 mean_real/fake: -0.001/-0.000 var_real/fake: 0.007/0.007 \n",
      "[180/1000][15/16] KL_real/fake: 4.431/4.427 mean_real/fake: -0.000/-0.003 var_real/fake: 0.007/0.007 \n",
      "[181/1000][15/16] KL_real/fake: 4.424/4.419 mean_real/fake: 0.002/0.002 var_real/fake: 0.007/0.007 \n",
      "[182/1000][15/16] KL_real/fake: 4.430/4.421 mean_real/fake: -0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[183/1000][15/16] KL_real/fake: 4.430/4.423 mean_real/fake: 0.000/0.001 var_real/fake: 0.007/0.007 \n",
      "[184/1000][15/16] KL_real/fake: 4.420/4.415 mean_real/fake: 0.000/-0.000 var_real/fake: 0.007/0.007 \n",
      "[185/1000][15/16] KL_real/fake: 4.419/4.415 mean_real/fake: -0.000/0.000 var_real/fake: 0.007/0.007 \n",
      "[186/1000][15/16] KL_real/fake: 4.418/4.418 mean_real/fake: -0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[187/1000][15/16] KL_real/fake: 4.412/4.410 mean_real/fake: 0.001/-0.000 var_real/fake: 0.007/0.007 \n",
      "[188/1000][15/16] KL_real/fake: 4.422/4.417 mean_real/fake: 0.000/-0.001 var_real/fake: 0.007/0.007 \n",
      "[189/1000][15/16] KL_real/fake: 4.411/4.408 mean_real/fake: 0.001/-0.001 var_real/fake: 0.007/0.007 \n",
      "[190/1000][15/16] KL_real/fake: 4.407/4.405 mean_real/fake: 0.000/0.001 var_real/fake: 0.007/0.007 \n",
      "[191/1000][15/16] KL_real/fake: 4.407/4.404 mean_real/fake: 0.000/0.000 var_real/fake: 0.007/0.007 \n",
      "[192/1000][15/16] KL_real/fake: 4.402/4.402 mean_real/fake: -0.001/0.003 var_real/fake: 0.007/0.007 \n",
      "[193/1000][15/16] KL_real/fake: 4.406/4.400 mean_real/fake: 0.001/-0.001 var_real/fake: 0.007/0.007 \n",
      "[194/1000][15/16] KL_real/fake: 4.412/4.402 mean_real/fake: -0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[195/1000][15/16] KL_real/fake: 4.404/4.399 mean_real/fake: 0.000/0.001 var_real/fake: 0.007/0.007 \n",
      "[196/1000][15/16] KL_real/fake: 4.407/4.402 mean_real/fake: -0.002/0.002 var_real/fake: 0.007/0.007 \n",
      "[197/1000][15/16] KL_real/fake: 4.410/4.404 mean_real/fake: 0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[198/1000][15/16] KL_real/fake: 4.410/4.402 mean_real/fake: 0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[199/1000][15/16] KL_real/fake: 4.403/4.396 mean_real/fake: 0.001/0.002 var_real/fake: 0.007/0.007 \n",
      "[200/1000][15/16] KL_real/fake: 4.399/4.394 mean_real/fake: 0.000/0.001 var_real/fake: 0.007/0.008 \n",
      "Adjusting learning rate from 0.000100 to 0.000050 on E and G\n",
      "[201/1000][15/16] KL_real/fake: 4.401/4.397 mean_real/fake: -0.001/-0.002 var_real/fake: 0.007/0.007 \n",
      "[202/1000][15/16] KL_real/fake: 4.404/4.398 mean_real/fake: 0.001/0.001 var_real/fake: 0.007/0.007 \n",
      "[203/1000][15/16] KL_real/fake: 4.399/4.397 mean_real/fake: 0.002/0.001 var_real/fake: 0.007/0.007 \n",
      "[204/1000][15/16] KL_real/fake: 4.400/4.394 mean_real/fake: 0.002/0.002 var_real/fake: 0.007/0.008 \n",
      "[205/1000][15/16] KL_real/fake: 4.402/4.395 mean_real/fake: 0.001/0.000 var_real/fake: 0.007/0.007 \n",
      "[206/1000][15/16] KL_real/fake: 4.397/4.393 mean_real/fake: 0.000/0.001 var_real/fake: 0.007/0.008 \n",
      "[207/1000][15/16] KL_real/fake: 4.398/4.391 mean_real/fake: 0.001/0.001 var_real/fake: 0.007/0.008 \n",
      "[208/1000][15/16] KL_real/fake: 4.393/4.389 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[209/1000][15/16] KL_real/fake: 4.394/4.387 mean_real/fake: 0.001/-0.001 var_real/fake: 0.007/0.008 \n",
      "[210/1000][15/16] KL_real/fake: 4.394/4.388 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[211/1000][15/16] KL_real/fake: 4.390/4.387 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[212/1000][15/16] KL_real/fake: 4.387/4.382 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[213/1000][15/16] KL_real/fake: 4.384/4.382 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[214/1000][15/16] KL_real/fake: 4.389/4.385 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[215/1000][15/16] KL_real/fake: 4.390/4.387 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[216/1000][15/16] KL_real/fake: 4.390/4.385 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[217/1000][15/16] KL_real/fake: 4.395/4.385 mean_real/fake: -0.001/-0.000 var_real/fake: 0.007/0.008 \n",
      "[218/1000][15/16] KL_real/fake: 4.388/4.384 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[219/1000][15/16] KL_real/fake: 4.389/4.382 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[220/1000][15/16] KL_real/fake: 4.391/4.383 mean_real/fake: 0.002/0.000 var_real/fake: 0.008/0.008 \n",
      "[221/1000][15/16] KL_real/fake: 4.388/4.384 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[222/1000][15/16] KL_real/fake: 4.384/4.380 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[223/1000][15/16] KL_real/fake: 4.386/4.378 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[224/1000][15/16] KL_real/fake: 4.382/4.381 mean_real/fake: 0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[225/1000][15/16] KL_real/fake: 4.383/4.380 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[226/1000][15/16] KL_real/fake: 4.383/4.379 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[227/1000][15/16] KL_real/fake: 4.384/4.378 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[228/1000][15/16] KL_real/fake: 4.383/4.379 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[229/1000][15/16] KL_real/fake: 4.385/4.381 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[230/1000][15/16] KL_real/fake: 4.384/4.381 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[231/1000][15/16] KL_real/fake: 4.386/4.384 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[232/1000][15/16] KL_real/fake: 4.382/4.379 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[233/1000][15/16] KL_real/fake: 4.379/4.377 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[234/1000][15/16] KL_real/fake: 4.379/4.375 mean_real/fake: 0.002/0.001 var_real/fake: 0.008/0.008 \n",
      "[235/1000][15/16] KL_real/fake: 4.383/4.375 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[236/1000][15/16] KL_real/fake: 4.379/4.374 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[237/1000][15/16] KL_real/fake: 4.378/4.373 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[238/1000][15/16] KL_real/fake: 4.383/4.379 mean_real/fake: -0.001/-0.002 var_real/fake: 0.008/0.008 \n",
      "[239/1000][15/16] KL_real/fake: 4.378/4.376 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[240/1000][15/16] KL_real/fake: 4.383/4.376 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[241/1000][15/16] KL_real/fake: 4.384/4.379 mean_real/fake: 0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[242/1000][15/16] KL_real/fake: 4.383/4.378 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[243/1000][15/16] KL_real/fake: 4.382/4.375 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[244/1000][15/16] KL_real/fake: 4.380/4.377 mean_real/fake: 0.002/0.002 var_real/fake: 0.008/0.008 \n",
      "[245/1000][15/16] KL_real/fake: 4.376/4.371 mean_real/fake: 0.002/0.002 var_real/fake: 0.008/0.008 \n",
      "[246/1000][15/16] KL_real/fake: 4.382/4.375 mean_real/fake: 0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[247/1000][15/16] KL_real/fake: 4.378/4.376 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[248/1000][15/16] KL_real/fake: 4.381/4.376 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[249/1000][15/16] KL_real/fake: 4.382/4.375 mean_real/fake: 0.002/0.001 var_real/fake: 0.008/0.008 \n",
      "[250/1000][15/16] KL_real/fake: 4.379/4.372 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[251/1000][15/16] KL_real/fake: 4.381/4.374 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[252/1000][15/16] KL_real/fake: 4.374/4.373 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[253/1000][15/16] KL_real/fake: 4.379/4.374 mean_real/fake: 0.002/0.002 var_real/fake: 0.008/0.008 \n",
      "[254/1000][15/16] KL_real/fake: 4.382/4.374 mean_real/fake: 0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[255/1000][15/16] KL_real/fake: 4.378/4.372 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[256/1000][15/16] KL_real/fake: 4.376/4.372 mean_real/fake: 0.003/0.002 var_real/fake: 0.008/0.008 \n",
      "[257/1000][15/16] KL_real/fake: 4.377/4.373 mean_real/fake: 0.002/0.000 var_real/fake: 0.008/0.008 \n",
      "[258/1000][15/16] KL_real/fake: 4.375/4.371 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[259/1000][15/16] KL_real/fake: 4.375/4.370 mean_real/fake: 0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[260/1000][15/16] KL_real/fake: 4.373/4.370 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[261/1000][15/16] KL_real/fake: 4.377/4.371 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[262/1000][15/16] KL_real/fake: 4.376/4.371 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[263/1000][15/16] KL_real/fake: 4.378/4.372 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[264/1000][15/16] KL_real/fake: 4.377/4.374 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[265/1000][15/16] KL_real/fake: 4.373/4.371 mean_real/fake: 0.002/-0.001 var_real/fake: 0.008/0.008 \n",
      "[266/1000][15/16] KL_real/fake: 4.372/4.369 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[267/1000][15/16] KL_real/fake: 4.370/4.369 mean_real/fake: 0.002/0.001 var_real/fake: 0.008/0.008 \n",
      "[268/1000][15/16] KL_real/fake: 4.371/4.369 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[269/1000][15/16] KL_real/fake: 4.375/4.368 mean_real/fake: 0.002/-0.000 var_real/fake: 0.008/0.008 \n",
      "[270/1000][15/16] KL_real/fake: 4.372/4.370 mean_real/fake: 0.001/-0.002 var_real/fake: 0.008/0.008 \n",
      "[271/1000][15/16] KL_real/fake: 4.374/4.368 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[272/1000][15/16] KL_real/fake: 4.377/4.371 mean_real/fake: 0.001/0.003 var_real/fake: 0.008/0.008 \n",
      "[273/1000][15/16] KL_real/fake: 4.375/4.370 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[274/1000][15/16] KL_real/fake: 4.374/4.372 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[275/1000][15/16] KL_real/fake: 4.372/4.368 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[276/1000][15/16] KL_real/fake: 4.372/4.368 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[277/1000][15/16] KL_real/fake: 4.372/4.369 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[278/1000][15/16] KL_real/fake: 4.377/4.369 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[279/1000][15/16] KL_real/fake: 4.373/4.370 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[280/1000][15/16] KL_real/fake: 4.374/4.368 mean_real/fake: 0.002/0.001 var_real/fake: 0.008/0.008 \n",
      "[281/1000][15/16] KL_real/fake: 4.372/4.370 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[282/1000][15/16] KL_real/fake: 4.374/4.367 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[283/1000][15/16] KL_real/fake: 4.375/4.369 mean_real/fake: 0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[284/1000][15/16] KL_real/fake: 4.372/4.369 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[285/1000][15/16] KL_real/fake: 4.373/4.368 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[286/1000][15/16] KL_real/fake: 4.375/4.370 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[287/1000][15/16] KL_real/fake: 4.375/4.368 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[288/1000][15/16] KL_real/fake: 4.376/4.369 mean_real/fake: -0.002/0.001 var_real/fake: 0.008/0.008 \n",
      "[289/1000][15/16] KL_real/fake: 4.375/4.367 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[290/1000][15/16] KL_real/fake: 4.371/4.368 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[291/1000][15/16] KL_real/fake: 4.378/4.368 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[292/1000][15/16] KL_real/fake: 4.373/4.368 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[293/1000][15/16] KL_real/fake: 4.371/4.367 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[294/1000][15/16] KL_real/fake: 4.370/4.367 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[295/1000][15/16] KL_real/fake: 4.374/4.367 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[296/1000][15/16] KL_real/fake: 4.372/4.366 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[297/1000][15/16] KL_real/fake: 4.374/4.369 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[298/1000][15/16] KL_real/fake: 4.374/4.366 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[299/1000][15/16] KL_real/fake: 4.371/4.366 mean_real/fake: 0.002/0.001 var_real/fake: 0.008/0.008 \n",
      "[300/1000][15/16] KL_real/fake: 4.373/4.368 mean_real/fake: -0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[301/1000][15/16] KL_real/fake: 4.371/4.365 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "Adjusting learning rate from 0.000050 to 0.000025 on E and G\n",
      "[302/1000][15/16] KL_real/fake: 4.368/4.366 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[303/1000][15/16] KL_real/fake: 4.368/4.366 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[304/1000][15/16] KL_real/fake: 4.369/4.366 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[305/1000][15/16] KL_real/fake: 4.377/4.365 mean_real/fake: 0.002/0.002 var_real/fake: 0.008/0.008 \n",
      "[306/1000][15/16] KL_real/fake: 4.368/4.365 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[307/1000][15/16] KL_real/fake: 4.369/4.365 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[308/1000][15/16] KL_real/fake: 4.367/4.364 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[309/1000][15/16] KL_real/fake: 4.369/4.364 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[310/1000][15/16] KL_real/fake: 4.369/4.365 mean_real/fake: -0.002/-0.000 var_real/fake: 0.008/0.008 \n",
      "[311/1000][15/16] KL_real/fake: 4.368/4.365 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[312/1000][15/16] KL_real/fake: 4.369/4.365 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[313/1000][15/16] KL_real/fake: 4.369/4.364 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[314/1000][15/16] KL_real/fake: 4.370/4.365 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[315/1000][15/16] KL_real/fake: 4.368/4.365 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[316/1000][15/16] KL_real/fake: 4.365/4.364 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[317/1000][15/16] KL_real/fake: 4.368/4.364 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[318/1000][15/16] KL_real/fake: 4.367/4.363 mean_real/fake: 0.002/0.001 var_real/fake: 0.008/0.008 \n",
      "[319/1000][15/16] KL_real/fake: 4.366/4.362 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[320/1000][15/16] KL_real/fake: 4.368/4.363 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[321/1000][15/16] KL_real/fake: 4.369/4.362 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[322/1000][15/16] KL_real/fake: 4.367/4.362 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[323/1000][15/16] KL_real/fake: 4.367/4.363 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[324/1000][15/16] KL_real/fake: 4.366/4.363 mean_real/fake: 0.000/-0.002 var_real/fake: 0.008/0.008 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[325/1000][15/16] KL_real/fake: 4.370/4.366 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[326/1000][15/16] KL_real/fake: 4.367/4.366 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[327/1000][15/16] KL_real/fake: 4.366/4.363 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[328/1000][15/16] KL_real/fake: 4.369/4.366 mean_real/fake: 0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[329/1000][15/16] KL_real/fake: 4.370/4.363 mean_real/fake: -0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[330/1000][15/16] KL_real/fake: 4.367/4.361 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[331/1000][15/16] KL_real/fake: 4.365/4.363 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[332/1000][15/16] KL_real/fake: 4.363/4.362 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[333/1000][15/16] KL_real/fake: 4.370/4.363 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[334/1000][15/16] KL_real/fake: 4.368/4.363 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[335/1000][15/16] KL_real/fake: 4.367/4.362 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[336/1000][15/16] KL_real/fake: 4.368/4.363 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[337/1000][15/16] KL_real/fake: 4.365/4.363 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[338/1000][15/16] KL_real/fake: 4.366/4.362 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[339/1000][15/16] KL_real/fake: 4.366/4.364 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[340/1000][15/16] KL_real/fake: 4.368/4.364 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[341/1000][15/16] KL_real/fake: 4.367/4.363 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[342/1000][15/16] KL_real/fake: 4.368/4.364 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[343/1000][15/16] KL_real/fake: 4.367/4.365 mean_real/fake: 0.002/-0.001 var_real/fake: 0.008/0.008 \n",
      "[344/1000][15/16] KL_real/fake: 4.369/4.362 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[345/1000][15/16] KL_real/fake: 4.364/4.363 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[346/1000][15/16] KL_real/fake: 4.366/4.363 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[347/1000][15/16] KL_real/fake: 4.366/4.363 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[348/1000][15/16] KL_real/fake: 4.365/4.363 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[349/1000][15/16] KL_real/fake: 4.364/4.362 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[350/1000][15/16] KL_real/fake: 4.365/4.361 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[351/1000][15/16] KL_real/fake: 4.365/4.362 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[352/1000][15/16] KL_real/fake: 4.367/4.361 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[353/1000][15/16] KL_real/fake: 4.366/4.362 mean_real/fake: 0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[354/1000][15/16] KL_real/fake: 4.365/4.362 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[355/1000][15/16] KL_real/fake: 4.368/4.362 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[356/1000][15/16] KL_real/fake: 4.367/4.363 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[357/1000][15/16] KL_real/fake: 4.368/4.363 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[358/1000][15/16] KL_real/fake: 4.367/4.363 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[359/1000][15/16] KL_real/fake: 4.366/4.363 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[360/1000][15/16] KL_real/fake: 4.369/4.362 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[361/1000][15/16] KL_real/fake: 4.369/4.362 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[362/1000][15/16] KL_real/fake: 4.366/4.362 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[363/1000][15/16] KL_real/fake: 4.368/4.363 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[364/1000][15/16] KL_real/fake: 4.365/4.362 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[365/1000][15/16] KL_real/fake: 4.364/4.360 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[366/1000][15/16] KL_real/fake: 4.366/4.361 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[367/1000][15/16] KL_real/fake: 4.366/4.360 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[368/1000][15/16] KL_real/fake: 4.365/4.360 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[369/1000][15/16] KL_real/fake: 4.368/4.361 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[370/1000][15/16] KL_real/fake: 4.366/4.360 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[371/1000][15/16] KL_real/fake: 4.364/4.362 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[372/1000][15/16] KL_real/fake: 4.364/4.362 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[373/1000][15/16] KL_real/fake: 4.368/4.362 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[374/1000][15/16] KL_real/fake: 4.366/4.361 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[375/1000][15/16] KL_real/fake: 4.368/4.361 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[376/1000][15/16] KL_real/fake: 4.363/4.362 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[377/1000][15/16] KL_real/fake: 4.366/4.360 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[378/1000][15/16] KL_real/fake: 4.364/4.361 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[379/1000][15/16] KL_real/fake: 4.365/4.363 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[380/1000][15/16] KL_real/fake: 4.373/4.361 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[381/1000][15/16] KL_real/fake: 4.363/4.362 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[382/1000][15/16] KL_real/fake: 4.364/4.361 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[383/1000][15/16] KL_real/fake: 4.367/4.361 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[384/1000][15/16] KL_real/fake: 4.365/4.363 mean_real/fake: -0.001/0.004 var_real/fake: 0.008/0.008 \n",
      "[385/1000][15/16] KL_real/fake: 4.365/4.361 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[386/1000][15/16] KL_real/fake: 4.367/4.361 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[387/1000][15/16] KL_real/fake: 4.368/4.361 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[388/1000][15/16] KL_real/fake: 4.369/4.361 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[389/1000][15/16] KL_real/fake: 4.366/4.362 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[390/1000][15/16] KL_real/fake: 4.367/4.361 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[391/1000][15/16] KL_real/fake: 4.366/4.362 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[392/1000][15/16] KL_real/fake: 4.368/4.361 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[393/1000][15/16] KL_real/fake: 4.364/4.360 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[394/1000][15/16] KL_real/fake: 4.365/4.362 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[395/1000][15/16] KL_real/fake: 4.364/4.360 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[396/1000][15/16] KL_real/fake: 4.365/4.361 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[397/1000][15/16] KL_real/fake: 4.366/4.362 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[398/1000][15/16] KL_real/fake: 4.366/4.361 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[399/1000][15/16] KL_real/fake: 4.363/4.360 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[400/1000][15/16] KL_real/fake: 4.366/4.362 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[401/1000][15/16] KL_real/fake: 4.365/4.362 mean_real/fake: 0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[402/1000][15/16] KL_real/fake: 4.365/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "Adjusting learning rate from 0.000025 to 0.000013 on E and G\n",
      "[403/1000][15/16] KL_real/fake: 4.367/4.359 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[404/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[405/1000][15/16] KL_real/fake: 4.363/4.360 mean_real/fake: 0.000/0.003 var_real/fake: 0.008/0.008 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[406/1000][15/16] KL_real/fake: 4.365/4.360 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[407/1000][15/16] KL_real/fake: 4.364/4.360 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[408/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[409/1000][15/16] KL_real/fake: 4.368/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[410/1000][15/16] KL_real/fake: 4.362/4.361 mean_real/fake: 0.001/-0.002 var_real/fake: 0.008/0.008 \n",
      "[411/1000][15/16] KL_real/fake: 4.363/4.362 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[412/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[413/1000][15/16] KL_real/fake: 4.364/4.360 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[414/1000][15/16] KL_real/fake: 4.364/4.360 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[415/1000][15/16] KL_real/fake: 4.366/4.358 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[416/1000][15/16] KL_real/fake: 4.364/4.360 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[417/1000][15/16] KL_real/fake: 4.363/4.360 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[418/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[419/1000][15/16] KL_real/fake: 4.368/4.360 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[420/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[421/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[422/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[423/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[424/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[425/1000][15/16] KL_real/fake: 4.365/4.359 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[426/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[427/1000][15/16] KL_real/fake: 4.365/4.359 mean_real/fake: -0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[428/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[429/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[430/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[431/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.001/-0.002 var_real/fake: 0.008/0.008 \n",
      "[432/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[433/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[434/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[435/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[436/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[437/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[438/1000][15/16] KL_real/fake: 4.368/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[439/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[440/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[441/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: 0.002/0.001 var_real/fake: 0.008/0.008 \n",
      "[442/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[443/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[444/1000][15/16] KL_real/fake: 4.367/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[445/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[446/1000][15/16] KL_real/fake: 4.364/4.360 mean_real/fake: -0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[447/1000][15/16] KL_real/fake: 4.362/4.360 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[448/1000][15/16] KL_real/fake: 4.362/4.360 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[449/1000][15/16] KL_real/fake: 4.361/4.360 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[450/1000][15/16] KL_real/fake: 4.361/4.360 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[451/1000][15/16] KL_real/fake: 4.368/4.358 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[452/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[453/1000][15/16] KL_real/fake: 4.360/4.357 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[454/1000][15/16] KL_real/fake: 4.361/4.360 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[455/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.001/-0.002 var_real/fake: 0.008/0.008 \n",
      "[456/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[457/1000][15/16] KL_real/fake: 4.360/4.358 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[458/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[459/1000][15/16] KL_real/fake: 4.365/4.359 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[460/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[461/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[462/1000][15/16] KL_real/fake: 4.364/4.360 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[463/1000][15/16] KL_real/fake: 4.362/4.360 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[464/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[465/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[466/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[467/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[468/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[469/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[470/1000][15/16] KL_real/fake: 4.360/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[471/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[472/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[473/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[474/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[475/1000][15/16] KL_real/fake: 4.365/4.360 mean_real/fake: 0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[476/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[477/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[478/1000][15/16] KL_real/fake: 4.364/4.360 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[479/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[480/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[481/1000][15/16] KL_real/fake: 4.365/4.360 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[482/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[483/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[484/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[485/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[486/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[487/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[488/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[489/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[490/1000][15/16] KL_real/fake: 4.363/4.357 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[491/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[492/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[493/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[494/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.002/0.002 var_real/fake: 0.008/0.008 \n",
      "[495/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[496/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[497/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.001/-0.002 var_real/fake: 0.008/0.008 \n",
      "[498/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[499/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[500/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[501/1000][15/16] KL_real/fake: 4.364/4.360 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[502/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[503/1000][15/16] KL_real/fake: 4.364/4.360 mean_real/fake: -0.001/-0.003 var_real/fake: 0.008/0.008 \n",
      "Adjusting learning rate from 0.000013 to 0.000006 on E and G\n",
      "[504/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[505/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[506/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[507/1000][15/16] KL_real/fake: 4.365/4.359 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[508/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[509/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[510/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[511/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[512/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[513/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[514/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[515/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[516/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[517/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[518/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[519/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[520/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.001/-0.002 var_real/fake: 0.008/0.008 \n",
      "[521/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[522/1000][15/16] KL_real/fake: 4.365/4.360 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[523/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[524/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[525/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[526/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[527/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[528/1000][15/16] KL_real/fake: 4.365/4.359 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[529/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[530/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[531/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[532/1000][15/16] KL_real/fake: 4.362/4.360 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[533/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[534/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[535/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[536/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[537/1000][15/16] KL_real/fake: 4.363/4.360 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[538/1000][15/16] KL_real/fake: 4.360/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[539/1000][15/16] KL_real/fake: 4.363/4.357 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[540/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[541/1000][15/16] KL_real/fake: 4.360/4.357 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[542/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[543/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[544/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[545/1000][15/16] KL_real/fake: 4.369/4.357 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[546/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[547/1000][15/16] KL_real/fake: 4.360/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[548/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/0.003 var_real/fake: 0.008/0.008 \n",
      "[549/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[550/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[551/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[552/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[553/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[554/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[555/1000][15/16] KL_real/fake: 4.366/4.359 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[556/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[557/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[558/1000][15/16] KL_real/fake: 4.360/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[559/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[560/1000][15/16] KL_real/fake: 4.366/4.357 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[561/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[562/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[563/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[564/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[565/1000][15/16] KL_real/fake: 4.364/4.357 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[566/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[567/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[568/1000][15/16] KL_real/fake: 4.366/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[569/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[570/1000][15/16] KL_real/fake: 4.360/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[571/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[572/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[573/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[574/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[575/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[576/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[577/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[578/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[579/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[580/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[581/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[582/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[583/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[584/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[585/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[586/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[587/1000][15/16] KL_real/fake: 4.366/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[588/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[589/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[590/1000][15/16] KL_real/fake: 4.365/4.357 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[591/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[592/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/0.003 var_real/fake: 0.008/0.008 \n",
      "[593/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[594/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[595/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[596/1000][15/16] KL_real/fake: 4.365/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[597/1000][15/16] KL_real/fake: 4.365/4.359 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[598/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[599/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[600/1000][15/16] KL_real/fake: 4.362/4.360 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[601/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[602/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[603/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[604/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "Adjusting learning rate from 0.000006 to 0.000003 on E and G\n",
      "[605/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[606/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[607/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[608/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[609/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[610/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[611/1000][15/16] KL_real/fake: 4.361/4.360 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[612/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[613/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[614/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[615/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[616/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[617/1000][15/16] KL_real/fake: 4.362/4.360 mean_real/fake: 0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[618/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[619/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[620/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[621/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[622/1000][15/16] KL_real/fake: 4.362/4.360 mean_real/fake: 0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[623/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[624/1000][15/16] KL_real/fake: 4.365/4.359 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[625/1000][15/16] KL_real/fake: 4.363/4.360 mean_real/fake: 0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[626/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[627/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[628/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[629/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[630/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[631/1000][15/16] KL_real/fake: 4.361/4.360 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[632/1000][15/16] KL_real/fake: 4.366/4.359 mean_real/fake: 0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[633/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[634/1000][15/16] KL_real/fake: 4.360/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[635/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[636/1000][15/16] KL_real/fake: 4.367/4.359 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[637/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[638/1000][15/16] KL_real/fake: 4.361/4.360 mean_real/fake: 0.001/-0.002 var_real/fake: 0.008/0.008 \n",
      "[639/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[640/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[641/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[642/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[643/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[644/1000][15/16] KL_real/fake: 4.365/4.359 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[645/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[646/1000][15/16] KL_real/fake: 4.367/4.360 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[647/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[648/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[649/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[650/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[651/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[652/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[653/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[654/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[655/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[656/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[657/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[658/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[659/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[660/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[661/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[662/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[663/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[664/1000][15/16] KL_real/fake: 4.365/4.357 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[665/1000][15/16] KL_real/fake: 4.361/4.360 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[666/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[667/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[668/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[669/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[670/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[671/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[672/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[673/1000][15/16] KL_real/fake: 4.366/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[674/1000][15/16] KL_real/fake: 4.363/4.357 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[675/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[676/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[677/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[678/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[679/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[680/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[681/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[682/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[683/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[684/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[685/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[686/1000][15/16] KL_real/fake: 4.360/4.359 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[687/1000][15/16] KL_real/fake: 4.363/4.360 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[688/1000][15/16] KL_real/fake: 4.361/4.360 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[689/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[690/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[691/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[692/1000][15/16] KL_real/fake: 4.366/4.358 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[693/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[694/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[695/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[696/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[697/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[698/1000][15/16] KL_real/fake: 4.360/4.357 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[699/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[700/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[701/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[702/1000][15/16] KL_real/fake: 4.363/4.357 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[703/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[704/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[705/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "Adjusting learning rate from 0.000003 to 0.000002 on E and G\n",
      "[706/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[707/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[708/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[709/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[710/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[711/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[712/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[713/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[714/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[715/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[716/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[717/1000][15/16] KL_real/fake: 4.363/4.357 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[718/1000][15/16] KL_real/fake: 4.363/4.357 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[719/1000][15/16] KL_real/fake: 4.363/4.360 mean_real/fake: -0.000/0.003 var_real/fake: 0.008/0.008 \n",
      "[720/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[721/1000][15/16] KL_real/fake: 4.363/4.362 mean_real/fake: 0.000/0.003 var_real/fake: 0.008/0.008 \n",
      "[722/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[723/1000][15/16] KL_real/fake: 4.365/4.357 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[724/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[725/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[726/1000][15/16] KL_real/fake: 4.360/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[727/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[728/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[729/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[730/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[731/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[732/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: 0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[733/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[734/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[735/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[736/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[737/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[738/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[739/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[740/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[741/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[742/1000][15/16] KL_real/fake: 4.363/4.357 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[743/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[744/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[745/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[746/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[747/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[748/1000][15/16] KL_real/fake: 4.363/4.357 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[749/1000][15/16] KL_real/fake: 4.360/4.358 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[750/1000][15/16] KL_real/fake: 4.359/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[751/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[752/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[753/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[754/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[755/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[756/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[757/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[758/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[759/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[760/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[761/1000][15/16] KL_real/fake: 4.360/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[762/1000][15/16] KL_real/fake: 4.360/4.358 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[763/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[764/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[765/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[766/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[767/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[768/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[769/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[770/1000][15/16] KL_real/fake: 4.366/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[771/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[772/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[773/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[774/1000][15/16] KL_real/fake: 4.362/4.360 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[775/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[776/1000][15/16] KL_real/fake: 4.361/4.360 mean_real/fake: -0.001/-0.003 var_real/fake: 0.008/0.008 \n",
      "[777/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[778/1000][15/16] KL_real/fake: 4.361/4.360 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[779/1000][15/16] KL_real/fake: 4.363/4.357 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[780/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[781/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[782/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[783/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[784/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[785/1000][15/16] KL_real/fake: 4.367/4.359 mean_real/fake: -0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[786/1000][15/16] KL_real/fake: 4.360/4.358 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[787/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[788/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[789/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[790/1000][15/16] KL_real/fake: 4.360/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[791/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[792/1000][15/16] KL_real/fake: 4.365/4.359 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[793/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[794/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[795/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[796/1000][15/16] KL_real/fake: 4.363/4.357 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[797/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.002/-0.001 var_real/fake: 0.008/0.008 \n",
      "[798/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[799/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[800/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[801/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[802/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[803/1000][15/16] KL_real/fake: 4.365/4.357 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[804/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[805/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[806/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "Adjusting learning rate from 0.000002 to 0.000001 on E and G\n",
      "[807/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[808/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[809/1000][15/16] KL_real/fake: 4.363/4.360 mean_real/fake: 0.001/-0.002 var_real/fake: 0.008/0.008 \n",
      "[810/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[811/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[812/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[813/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[814/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[815/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[816/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[817/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[818/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[819/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[820/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[821/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[822/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[823/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[824/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[825/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[826/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[827/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[828/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[829/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[830/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[831/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[832/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[833/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[834/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[835/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[836/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[837/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[838/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/-0.003 var_real/fake: 0.008/0.008 \n",
      "[839/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[840/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[841/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[842/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[843/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[844/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.001/-0.002 var_real/fake: 0.008/0.008 \n",
      "[845/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[846/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[847/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[848/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[849/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[850/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[851/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[852/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[853/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[854/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[855/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[856/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[857/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[858/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[859/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[860/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[861/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[862/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[863/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[864/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[865/1000][15/16] KL_real/fake: 4.361/4.360 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[866/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[867/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[868/1000][15/16] KL_real/fake: 4.364/4.360 mean_real/fake: -0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[869/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[870/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[871/1000][15/16] KL_real/fake: 4.363/4.357 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[872/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[873/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[874/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[875/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[876/1000][15/16] KL_real/fake: 4.366/4.359 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[877/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[878/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[879/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[880/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[881/1000][15/16] KL_real/fake: 4.361/4.360 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[882/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: -0.000/0.003 var_real/fake: 0.008/0.008 \n",
      "[883/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[884/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[885/1000][15/16] KL_real/fake: 4.366/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[886/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[887/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[888/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[889/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[890/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[891/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[892/1000][15/16] KL_real/fake: 4.360/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[893/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[894/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[895/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[896/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[897/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[898/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[899/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: -0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[900/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[901/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: 0.001/0.002 var_real/fake: 0.008/0.008 \n",
      "[902/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[903/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[904/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[905/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[906/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[907/1000][15/16] KL_real/fake: 4.363/4.357 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "Adjusting learning rate from 0.000001 to 0.000000 on E and G\n",
      "[908/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[909/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[910/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[911/1000][15/16] KL_real/fake: 4.364/4.359 mean_real/fake: 0.001/-0.002 var_real/fake: 0.008/0.008 \n",
      "[912/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[913/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[914/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[915/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[916/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[917/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.001/-0.000 var_real/fake: 0.008/0.008 \n",
      "[918/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[919/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[920/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[921/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[922/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[923/1000][15/16] KL_real/fake: 4.362/4.360 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[924/1000][15/16] KL_real/fake: 4.365/4.358 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[925/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[926/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[927/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[928/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[929/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[930/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[931/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.001/-0.001 var_real/fake: 0.008/0.008 \n",
      "[932/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: 0.000/0.002 var_real/fake: 0.008/0.008 \n",
      "[933/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[934/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[935/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[936/1000][15/16] KL_real/fake: 4.362/4.357 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[937/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: 0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[938/1000][15/16] KL_real/fake: 4.360/4.358 mean_real/fake: -0.001/0.000 var_real/fake: 0.008/0.008 \n",
      "[939/1000][15/16] KL_real/fake: 4.361/4.357 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[940/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[941/1000][15/16] KL_real/fake: 4.362/4.359 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[942/1000][15/16] KL_real/fake: 4.363/4.357 mean_real/fake: -0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[943/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[944/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.001/0.001 var_real/fake: 0.008/0.008 \n",
      "[945/1000][15/16] KL_real/fake: 4.363/4.358 mean_real/fake: -0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[946/1000][15/16] KL_real/fake: 4.360/4.358 mean_real/fake: -0.000/-0.002 var_real/fake: 0.008/0.008 \n",
      "[947/1000][15/16] KL_real/fake: 4.363/4.359 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[948/1000][15/16] KL_real/fake: 4.361/4.358 mean_real/fake: 0.000/0.001 var_real/fake: 0.008/0.008 \n",
      "[949/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[950/1000][15/16] KL_real/fake: 4.361/4.359 mean_real/fake: -0.000/-0.001 var_real/fake: 0.008/0.008 \n",
      "[951/1000][15/16] KL_real/fake: 4.362/4.358 mean_real/fake: 0.000/-0.000 var_real/fake: 0.008/0.008 \n",
      "[952/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.000/0.000 var_real/fake: 0.008/0.008 \n",
      "[953/1000][15/16] KL_real/fake: 4.364/4.358 mean_real/fake: 0.001/-0.000 var_real/fake: 0.008/0.008 \n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "losses = {'KL_real': [],\n",
    "         'Ex_e': [],\n",
    "         'KL_fake': [], \n",
    "         'Ez_e': [],\n",
    "         'KL_fake_g': [],\n",
    "         'Ex_g: ': [],\n",
    "         'Ez_g_10e3': [],\n",
    "        }\n",
    "\n",
    "for epoch in range(start_epoch, nepoch):\n",
    "\n",
    "    # Adjust learning rate\n",
    "    adjust_lr(epoch)\n",
    "\n",
    "    for i in range(len(dataloader['train'])):\n",
    "\n",
    "        # ---------------------------\n",
    "        #        Optimize over e\n",
    "        # ---------------------------\n",
    "        \n",
    "        # e_updates = '1; KL_fake:1, KL_real:1, match_z:0, match_x:10' \n",
    "        \n",
    "        KL_real_ = []\n",
    "        KL_fake_ = []\n",
    "        Ex_e_ = []\n",
    "        Ez_e_ = []\n",
    "\n",
    "        for e_iter in range(updates['e']['num_updates']):\n",
    "            e_losses = []\n",
    "            netE.zero_grad()\n",
    "\n",
    "            # X - e(X)\n",
    "            populate_x(x, dataloader['train'])\n",
    "            ex = netE(x)\n",
    "\n",
    "            # KL_real: - \\Delta( e(X) , Z ) -> max_e\n",
    "            KL_real = KL_minimizer(ex)\n",
    "            e_losses.append(KL_real * updates['e']['KL_real'])\n",
    "            # === stats ===    \n",
    "            KL_real_.append(KL_real.data[0])\n",
    "            \n",
    "            if updates['e']['match_x'] != 0:\n",
    "                # g(e(X))\n",
    "                gex = netG(ex)\n",
    "\n",
    "                # match_x: E_x||g(e(x)) - x|| -> min_e\n",
    "                err = match(gex, x, match_x)\n",
    "                e_losses.append(err * 10) #updates['e']['match_x'])\n",
    "                # === stats === \n",
    "                Ex_e_.append(err.data[0])\n",
    "\n",
    "            # Save some stats\n",
    "            stats['real_mean'] = KL_minimizer.samples_mean.data.mean()\n",
    "            stats['real_var'] = KL_minimizer.samples_var.data.mean()\n",
    "            stats['KL_real'] = KL_real.data[0]\n",
    "\n",
    "            # ================================================\n",
    "\n",
    "            # Z - g(Z) - e(g(Z))\n",
    "            populate_z(z)\n",
    "            fake = netG(z).detach()\n",
    "            egz = netE(fake)\n",
    "\n",
    "            # KL_fake: \\Delta( e(g(Z)) , Z ) -> max_e\n",
    "            KL_fake = KL_maximizer(egz)\n",
    "            e_losses.append(KL_fake * updates['e']['KL_fake'])\n",
    "            # === stats === \n",
    "            KL_fake_.append(-KL_fake.data[0])\n",
    "            \n",
    "            if updates['e']['match_z'] != 0:\n",
    "                # match_z: E_z||e(g(z)) - z|| -> min_e\n",
    "                err = match(egz, z, match_z)\n",
    "                e_losses.append(err * updates['e']['match_z'])\n",
    "                # === stats === \n",
    "                Ez_e_.append(err.data[0])\n",
    "            \n",
    "            # Update e\n",
    "            sum(e_losses).backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "            # === stats === \n",
    "            stats['fake_mean'] = KL_maximizer.samples_mean.data.mean()\n",
    "            stats['fake_var'] = KL_maximizer.samples_var.data.mean()\n",
    "            stats['KL_fake'] = -KL_fake.data[0]\n",
    "            \n",
    "            if KL_real_: losses['KL_real'].append(np.mean(KL_real_))\n",
    "            if KL_fake_: losses['KL_fake'].append(np.mean(KL_fake_))\n",
    "            if Ex_e_: losses['Ex_e'].append(np.mean(Ex_e_))\n",
    "            if Ez_e_: losses['Ez_e'].append(np.mean(Ez_e_))\n",
    "            \n",
    "        # ---------------------------\n",
    "        #        Minimize over g\n",
    "        # ---------------------------\n",
    "        \n",
    "        # g_updates = '3; KL_fake:1, match_z:1000, match_x:0'\n",
    "\n",
    "        KL_fake_g_ = []\n",
    "        Ex_g_ = []\n",
    "        Ez_g_ = []\n",
    "        \n",
    "        for g_iter in range(updates['g']['num_updates']):\n",
    "            g_losses = []\n",
    "            netG.zero_grad()\n",
    "\n",
    "            # Z - g(Z) - e(g(Z))\n",
    "            populate_z(z)\n",
    "            fake = netG(z)\n",
    "            egz = netE(fake)\n",
    "\n",
    "            # KL_fake: \\Delta( e(g(Z)) , Z ) -> min_g\n",
    "            KL_fake_g = KL_minimizer(egz)\n",
    "            g_losses.append(KL_fake_g * updates['g']['KL_fake'])\n",
    "            # === stats === \n",
    "            KL_fake_g_.append(KL_fake_g.data[0])\n",
    "            \n",
    "            if updates['g']['match_z'] != 0:\n",
    "                # match_z: E_z||e(g(z)) - z|| -> min_g\n",
    "                err = match(egz, z, match_z)\n",
    "                err = err * updates['g']['match_z']\n",
    "                g_losses.append(err)\n",
    "                # === stats === \n",
    "#                 Ez_g_.append(err.data[0])\n",
    "                Ez_g_.append(err.data[0] / 1000)\n",
    "                \n",
    "            # ==================================\n",
    "\n",
    "            if updates['g']['match_x'] != 0:\n",
    "                # X - e(X) - g(e(X))\n",
    "                populate_x(x, dataloader['train'])\n",
    "                ex = netE(x)\n",
    "                gex = netG(ex)\n",
    "\n",
    "                # match_x: E_x||g(e(x)) - x|| -> min_g\n",
    "                err = match(gex, x, match_x)\n",
    "                err = err * updates['g']['match_x']\n",
    "                g_losses.append(err)\n",
    "                # === stats === \n",
    "                Ex_g_.append(err.data[0])\n",
    "\n",
    "            # Step g\n",
    "            sum(g_losses).backward()\n",
    "            optimizerG.step()\n",
    "        \n",
    "        # === stats === \n",
    "        if KL_fake_g_: losses['KL_fake_g'].append(np.mean(KL_fake_g_))\n",
    "        if Ez_g_: losses['Ez_g_10e3'].append(np.mean(Ez_g_))\n",
    "        if Ex_g_: losses['Ex_g'].append(np.mean(Ex_g_))\n",
    "        \n",
    "        # === saving === \n",
    "        \n",
    "#         if save_every_b:\n",
    "#             if i % save_every_b == 0: save_images(epoch)\n",
    "            \n",
    "        # If an epoch takes long time, dump intermediate\n",
    "        if dataset in ['lsun', 'imagenet'] and (i % 5000 == 0):\n",
    "            torch.save(netG, '%s/netG_epoch_%d_it_%d.pth' %\n",
    "                       (save_dir, epoch, i))\n",
    "            torch.save(netE, '%s/netE_epoch_%d_it_%d.pth' %\n",
    "                       (save_dir, epoch, i))\n",
    "    \n",
    "    if epoch % save_every_e == 0: \n",
    "        # echo:\n",
    "        print('[{epoch}/{nepoch}][{iter}/{niter}] '\n",
    "              'KL_real/fake: {KL_real:.3f}/{KL_fake:.3f} '\n",
    "              'mean_real/fake: {real_mean:.3f}/{fake_mean:.3f} '\n",
    "              'var_real/fake: {real_var:.3f}/{fake_var:.3f} '\n",
    "              ''.format(epoch=epoch,\n",
    "                        nepoch=nepoch,\n",
    "                        iter=i,\n",
    "                        niter=len(dataloader['train']),\n",
    "                        **stats))\n",
    "        save_images(epoch)\n",
    "        # checkpoint:\n",
    "#         torch.save(netG, '%s/netG_epoch_%d.pth' % (save_dir, epoch))\n",
    "#         torch.save(netE, '%s/netE_epoch_%d.pth' % (save_dir, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_updates = '1;KL_fake:1,KL_real:1,match_z:0,match_x:10' \n",
    "# g_updates = '3;KL_fake:1,match_z:1000,match_x:0'\n",
    "\n",
    "# 10 * 50 * 64 - 32000 \n",
    "# 10 * 25 * 128\n",
    "# 10 * 13 * 256\n",
    "\n",
    "# https://github.com/pytorch/pytorch/issues/1355\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hist(hist, run, show=True, save=True, o_p='output/' + dataset):\n",
    "    \n",
    "    plt.figure(figsize = (12, 6))\n",
    "    x = range(len(hist['KL_real']))\n",
    "    for i, j in hist.items():\n",
    "        if j: \n",
    "            plt.plot(x, j, label=i)\n",
    "            \n",
    "#     [plt.plot(x, j, label=i) for i, j in hist.items() if j]\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save: plt.savefig(os.path.join(o_p, 'train_hist_%s.png' %run))\n",
    "    if show: plt.show()\n",
    "    else: plt.close()\n",
    "    \n",
    "def log_last_epoch_ims(run, o_p='output/' + dataset):\n",
    "    ne = nepoch - 1\n",
    "    if ne < 10: f_name = os.path.join(o_p, 'reconstructions_epoch_00' + str(ne) + '.png')\n",
    "    elif ne < 100: f_name = os.path.join(o_p, 'reconstructions_epoch_0' + str(ne) + '.png')\n",
    "    else: f_name = os.path.join(o_p, 'reconstructions_epoch_' + str(ne) + '.png')\n",
    "    \n",
    "    im = cv2.imread(f_name)\n",
    "    cv2.imwrite(os.path.join(o_p, 'ims_last_epoch_%s.png' %run), im)\n",
    "    plot_im(im, alpha=False)\n",
    "    \n",
    "train_hist(losses, run, save=True)\n",
    "log_last_epoch_ims(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hist_ims_gif(otype, run, step=save_every_e, save=True, show=True, o_p = 'output/' + dataset):\n",
    "    ims_rec = []\n",
    "    i = 0\n",
    "    while i < nepoch:\n",
    "        if i < 10:\n",
    "            f_name = os.path.join(o_p, otype + '_epoch_00' + str(i) + '.png')\n",
    "            ims_rec.append(imageio.imread(f_name))\n",
    "        elif i < 100:\n",
    "            f_name = os.path.join(o_p, otype + '_epoch_0' + str(i) + '.png')\n",
    "            ims_rec.append(imageio.imread(f_name))                     \n",
    "        else:\n",
    "            f_name = os.path.join(o_p, otype + '_epoch_' + str(i) + '.png')\n",
    "            ims_rec.append(imageio.imread(f_name))  \n",
    "            \n",
    "        i += save_every_e\n",
    "#         print(len(ims_rec))\n",
    "        \n",
    "    o_f = os.path.join(o_p, otype + '_train_hist_ims_%s.gif' %run)\n",
    "\n",
    "    if save: \n",
    "        imageio.mimsave(o_f, ims_rec, fps=5)\n",
    "    \n",
    "    if show:\n",
    "        from IPython.display import HTML\n",
    "        HTML('<img src=\"%s\">' %o_f)\n",
    "\n",
    "for j in ['reconstructions', 'fake_samples']:\n",
    "    train_hist_ims_gif(j, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
