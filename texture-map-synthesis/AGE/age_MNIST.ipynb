{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--g_updates'], dest='g_updates', nargs=None, const=None, default='2;KL_fake:1,match_z:1,match_x:0', type=None, choices=None, help='Update plan for generator <number of updates>;[<term:weight>]', metavar=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', required=True,\n",
    "                    help='cifar10 | lsun | imagenet | folder | lfw ')\n",
    "parser.add_argument('--dataroot', type=str, help='path to dataset')\n",
    "parser.add_argument('--workers', type=int,\n",
    "                    help='number of data loading workers', default=8)\n",
    "parser.add_argument('--batch_size', type=int,\n",
    "                    default=64, help='batch size')\n",
    "parser.add_argument('--image_size', type=int, default=32,\n",
    "                    help='the resolution of the input image to network')\n",
    "parser.add_argument('--nz', type=int, default=100,\n",
    "                    help='size of the latent z vector')\n",
    "parser.add_argument('--ngf', type=int, default=64)\n",
    "parser.add_argument('--ndf', type=int, default=64)\n",
    "parser.add_argument('--nc', type=int)\n",
    "\n",
    "parser.add_argument('--nepoch', type=int, default=25,\n",
    "                    help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.0002,\n",
    "                    help='learning rate, default=0.0002')\n",
    "parser.add_argument('--beta1', type=float, default=0.5,\n",
    "                    help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--cpu', action='store_true',\n",
    "                    help='use CPU instead of GPU')\n",
    "parser.add_argument('--ngpu', type=int, default=1,\n",
    "                    help='number of GPUs to use')\n",
    "\n",
    "parser.add_argument('--netG', default='',\n",
    "                    help=\"path to netG config\")\n",
    "parser.add_argument('--netE', default='',\n",
    "                    help=\"path to netE config\")\n",
    "parser.add_argument('--netG_chp', default='',\n",
    "                    help=\"path to netG (to continue training)\")\n",
    "parser.add_argument('--netE_chp', default='',\n",
    "                    help=\"path to netE (to continue training)\")\n",
    "\n",
    "parser.add_argument('--save_dir', default='.',\n",
    "                    help='folder to output images and model checkpoints')\n",
    "parser.add_argument('--criterion', default='param',\n",
    "                    help='param|nonparam, How to estimate KL')\n",
    "parser.add_argument('--KL', default='qp', help='pq|qp')\n",
    "parser.add_argument('--noise', default='sphere', help='normal|sphere')\n",
    "parser.add_argument('--match_z', default='cos', help='none|L1|L2|cos')\n",
    "parser.add_argument('--match_x', default='L1', help='none|L1|L2|cos')\n",
    "\n",
    "parser.add_argument('--drop_lr', default=5, type=int, help='')\n",
    "parser.add_argument('--save_every', default=50, type=int, help='')\n",
    "\n",
    "parser.add_argument('--manual_seed', type=int, default=123, help='manual seed')\n",
    "parser.add_argument('--start_epoch', type=int, default=0, help='epoch number to start with')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--e_updates', default=\"1;KL_fake:1,KL_real:1,match_z:0,match_x:0\",\n",
    "    help='Update plan for encoder <number of updates>;[<term:weight>]'\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--g_updates', default=\"2;KL_fake:1,match_z:1,match_x:0\",\n",
    "    help='Update plan for generator <number of updates>;[<term:weight>]'\n",
    ")\n",
    "\n",
    "# opt = parser.parse_args()\n",
    "\n",
    "# python age.py \n",
    "# --dataset celeba \n",
    "# --dataroot <data_root> \n",
    "# --image_size 64 \n",
    "# --save_dir <save_dir> \n",
    "# --lr 0.0002 \n",
    "# --nz 64 \n",
    "# --batch_size 64 \n",
    "# --netG dcgan64px \n",
    "# --netE dcgan64px \n",
    "# --nepoch 5 \n",
    "# --drop_lr 5 \n",
    "# --e_updates '1;KL_fake:1,KL_real:1,match_z:0,match_x:10' \n",
    "# --g_updates '3;KL_fake:1,match_z:1000,match_x:0'\n",
    "\n",
    "# python age.py \n",
    "# --dataset celeba \n",
    "# --dataroot <data_root> \n",
    "# --image_size 64 \n",
    "# --save_dir <save_dir> \n",
    "# --start_epoch 5 \n",
    "# --lr 0.0002 \n",
    "# --nz 64 \n",
    "# --batch_size 256 \n",
    "# --netG dcgan64px \n",
    "# --netE dcgan64px \n",
    "# --nepoch 6 \n",
    "# --drop_lr 5   \n",
    "# --e_updates '1;KL_fake:1,KL_real:1,match_z:0,match_x:15' \n",
    "# --g_updates '3;KL_fake:1,match_z:1000,match_x:0' \n",
    "# --netE_chp  <save_dir>/netE_epoch_5.pth \n",
    "# --netG_chp <save_dir>/netG_epoch_5.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "import importlib\n",
    "# from .dataset import FolderWithImages\n",
    "import random\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory was not created.\n",
      "{'e': {'num_updates': 1, 'KL_fake': 1.0, 'KL_real': 1.0, 'match_z': 0.0, 'match_x': 0.0}, 'g': {'num_updates': 2, 'KL_fake': 1.0, 'match_z': 1.0, 'match_x': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# setup function @utils.py\n",
    "\n",
    "cuda = False # not opt.cpu\n",
    "torch.set_num_threads(4)\n",
    "dataset = 'mnist'\n",
    "save_dir = 'output/mnist'\n",
    "try:\n",
    "    os.makedirs(save_dir)\n",
    "except OSError:\n",
    "    print('Directory was not created.')\n",
    "\n",
    "manual_seed = random.randint(1, 10000)\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "# torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if torch.cuda.is_available() and not cuda:\n",
    "    print(\"WARNING: You have a CUDA device,\"\n",
    "            \"so you should probably run with --cuda\")\n",
    "\n",
    "e_updates = \"1;KL_fake:1,KL_real:1,match_z:0,match_x:0\"\n",
    "updates = {'e': {}, 'g': {}}\n",
    "updates['e']['num_updates'] = int(e_updates.split(';')[0])\n",
    "updates['e'].update({x.split(':')[0]: float(x.split(':')[1]) \n",
    "                     for x in e_updates.split(';')[1].split(',')})\n",
    "\n",
    "g_updates = \"2;KL_fake:1,match_z:1,match_x:0\"\n",
    "updates['g']['num_updates'] = int(g_updates.split(';')[0])\n",
    "updates['g'].update({x.split(':')[0]: float(x.split(':')[1]) \n",
    "                     for x in g_updates.split(';')[1].split(',')})\n",
    "\n",
    "print(updates)\n",
    "\n",
    "image_size = 32 # 512\n",
    "batch_size = 64\n",
    "workers = 8\n",
    "shuffle = True\n",
    "drop_last = True\n",
    "train = True\n",
    "\n",
    "nc = 1 \n",
    "nz = 64 #100 #'size of the latent z vector') size == batchsize??\n",
    "ngf = 32\n",
    "ndf = 32\n",
    "ngpu = 0 # 1 # 'number of GPUs to use')\n",
    "pin_memory= False # True | If ``True``, the data loader will copy tensors into CUDA pinned memory before returning them.\n",
    "\n",
    "noise = 'sphere' #help='normal|sphere')\n",
    "\n",
    "netG = 'dcgan32px' \n",
    "netE = 'dcgan32px' \n",
    "\n",
    "netG_chp = '' # \"path to netG (to continue training)\"\n",
    "netE_chp = '' # \"path to netE (to continue training)\"\n",
    "\n",
    "lr = 0.0002 \n",
    "drop_lr = 1 #5\n",
    "beta1 = 0.5 # help='beta1 for adam. default=0.5'\n",
    "criterion = 'param'# help='param|nonparam, How to estimate KL'\n",
    "KL ='qp' # help='pq|qp'\n",
    "match_z = 'cos' # help='none|L1|L2|cos'\n",
    "match_x = 'L1' # help='none|L1|L2|cos'\n",
    "\n",
    "start_epoch = 0\n",
    "nepoch = 1 # 5\n",
    "\n",
    "save_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blanca.alonso/anaconda3/envs/p3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    }
   ],
   "source": [
    "# setup_dataset @utils.py\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "import importlib\n",
    "# from .dataset import FolderWithImages\n",
    "import random\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "\n",
    "##\n",
    "import torch.utils.data as data\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath).convert('RGB')\n",
    "    return img\n",
    "\n",
    "class FolderWithImages(data.Dataset):\n",
    "    def __init__(self, root, input_transform=None, target_transform=None):\n",
    "        super(FolderWithImages, self).__init__()\n",
    "        self.image_filenames = [join(root, x)\n",
    "                                for x in listdir(root) if is_image_file(x.lower())]\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = load_img(self.image_filenames[index])\n",
    "        target = input.copy()\n",
    "        if self.input_transform:\n",
    "            input = self.input_transform(input)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return input, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "##\n",
    "\n",
    "def setup_dataset(dataset, train=True, shuffle=True, drop_last=True):\n",
    "    '''\n",
    "    Setups dataset.\n",
    "    '''\n",
    "    # Usual transform\n",
    "    t = transforms.Compose([\n",
    "        transforms.Scale([image_size, image_size]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    if dataset in ['imagenet', 'folder', 'lfw']:\n",
    "        imdir = 'train' if train else 'val'\n",
    "        dataroot = os.path.join(dataroot, imdir)\n",
    "\n",
    "        dataset = dset.ImageFolder(root=dataroot, transform=t)\n",
    "    elif dataset == 'lsun':\n",
    "        dataset = dset.LSUN(db_path=dataroot,\n",
    "                            classes=['bedroom_train'],\n",
    "                            train=train,\n",
    "                            transform=t)\n",
    "    elif dataset == 'cifar10':\n",
    "        dataset = dset.CIFAR10(root='data/raw/cifar10',\n",
    "                               download=True,\n",
    "                               train=train,\n",
    "                               transform=t\n",
    "                               )\n",
    "    elif dataset == 'mnist':\n",
    "        dataset = dset.MNIST(root='data/raw/mnist',\n",
    "                             download=True,\n",
    "                             train=train,\n",
    "                             transform=t\n",
    "                             )\n",
    "    elif dataset == 'svhn':\n",
    "        dataset = dset.SVHN(root='data/raw/svhn',\n",
    "                            download=True,\n",
    "                            train=train,\n",
    "                            transform=t)\n",
    "    elif dataset == 'celeba':\n",
    "        imdir = 'train' if train else 'val'\n",
    "        dataroot = os.path.join(dataroot, imdir)\n",
    "\n",
    "        dataset = FolderWithImages(root=dataroot,\n",
    "                                   input_transform=transforms.Compose([\n",
    "                                       ALICropAndScale(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                   ]),\n",
    "                                   target_transform=transforms.ToTensor()\n",
    "                                   )\n",
    "\n",
    "    else:\n",
    "        assert False, 'Wrong dataset name.'\n",
    "\n",
    "    assert len(dataset) > 0, 'No images found, check your paths.'\n",
    "\n",
    "    # Shuffle and drop last when training\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=shuffle,\n",
    "                                             num_workers=int(workers),\n",
    "                                             pin_memory=pin_memory,\n",
    "                                             drop_last=drop_last)\n",
    "\n",
    "    return InfiniteDataLoader(dataloader)\n",
    "\n",
    "\n",
    "class InfiniteDataLoader(object):\n",
    "    \"\"\"docstring for InfiniteDataLoader\"\"\"\n",
    "\n",
    "    def __init__(self, dataloader):\n",
    "        super(InfiniteDataLoader, self).__init__()\n",
    "        self.dataloader = dataloader\n",
    "        self.data_iter = None\n",
    "\n",
    "    def next(self):\n",
    "        try:\n",
    "            data = self.data_iter.next()\n",
    "        except Exception:\n",
    "            # Reached end of the dataset\n",
    "            self.data_iter = iter(self.dataloader)\n",
    "            data = self.data_iter.next()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "# Setup dataset\n",
    "dataloader = dict(train=setup_dataset(dataset, train=True),\n",
    "                  val=setup_dataset(dataset, train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator\n",
      " _netG_Base(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d (64, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d (256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d (128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d (64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d (64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Encoder\n",
      " _netE_Base(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d (1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(0.2, inplace)\n",
      "    (2): Conv2d (32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU(0.2, inplace)\n",
      "    (5): Conv2d (64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (7): LeakyReLU(0.2, inplace)\n",
      "    (8): Conv2d (128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    '''\n",
    "    Custom weights initialization called on netG and netE\n",
    "    '''\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class _netE_Base(nn.Module):\n",
    "    def __init__(self, main):\n",
    "        super(_netE_Base, self).__init__()\n",
    "        self.noise = noise\n",
    "        self.ngpu = ngpu\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        gpu_ids = None\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 0:\n",
    "            gpu_ids = range(self.ngpu)\n",
    "            output = nn.parallel.data_parallel(self.main, input, gpu_ids)\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        output = output.view(output.size(0), -1)\n",
    "        if self.noise == 'sphere':\n",
    "            output = normalize(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class _netG_Base(nn.Module):\n",
    "    def __init__(self, main):\n",
    "        super(_netG_Base, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # Check input is either (B,C,1,1) or (B,C)\n",
    "        assert input.nelement() == input.size(0) * input.size(1), 'wtf'\n",
    "        input = input.view(input.size(0), input.size(1), 1, 1)\n",
    "\n",
    "        gpu_ids = None\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 0:\n",
    "            gpu_ids = range(self.ngpu)\n",
    "            return nn.parallel.data_parallel(self.main, input, gpu_ids)\n",
    "        else:\n",
    "            return self.main(input)\n",
    "\n",
    "def _netE():\n",
    "\n",
    "    main = nn.Sequential(\n",
    "        # input is (nc) x 32 x 32\n",
    "        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "        nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ndf * 2),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ndf * 4),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "        nn.Conv2d(ndf * 4, nz, 4, 2, 1, bias=True),\n",
    "        nn.AvgPool2d(2),\n",
    "    )\n",
    "\n",
    "    return _netE_Base(main)\n",
    "\n",
    "def _netG():\n",
    "\n",
    "    main = nn.Sequential(\n",
    "        # input is Z, going into a convolution\n",
    "        nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(ngf * 8),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf*8) x 4 x 4\n",
    "        nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ngf * 4),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf*4) x 8 x 8\n",
    "        nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ngf * 2),\n",
    "        nn.ReLU(True),\n",
    "        # state size. (ngf*2) x 16 x 16\n",
    "        nn.ConvTranspose2d(ngf * 2, ngf * 2, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ngf * 2),\n",
    "        nn.ReLU(True),\n",
    "\n",
    "        nn.Conv2d(ngf * 2, nc, 1, bias=True),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "\n",
    "    return _netG_Base(main)\n",
    "\n",
    "def load_G():\n",
    "    '''\n",
    "    Loads generator model.\n",
    "    '''\n",
    "    netG = _netG()\n",
    "    netG.apply(weights_init)\n",
    "    netG.train()\n",
    "    if netG_chp != '':\n",
    "        netG.load_state_dict(torch.load(netG_chp).state_dict())\n",
    "\n",
    "    print('Generator\\n', netG)\n",
    "    return netG\n",
    "\n",
    "def load_E():\n",
    "    '''\n",
    "    Loads encoder model.\n",
    "    '''\n",
    "    netE = _netE()\n",
    "    netE.apply(weights_init)\n",
    "    netE.train()\n",
    "    if netE_chp != '':\n",
    "        netE.load_state_dict(torch.load(netE_chp).state_dict())\n",
    "\n",
    "    print('Encoder\\n', netE)\n",
    "\n",
    "    return netE\n",
    "\n",
    "# Load generator\n",
    "netG = load_G()\n",
    "\n",
    "# Load encoder\n",
    "netE = load_E()\n",
    "\n",
    "# RuntimeError: \n",
    "# Given input size: (512 x 2 x 2). \n",
    "# Calculated output size: (64 x -1 x -1). \n",
    "# Output size is too small at /Users/soumith/minicondabuild3/conda-bld/pytorch_1512381214802/work/torch/lib/THNN/generic/SpatialConvolutionMM.c:45\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def var(x, dim=0):\n",
    "    '''\n",
    "    Calculates variance.\n",
    "    '''\n",
    "    x_zero_meaned = x - x.mean(dim).expand_as(x)\n",
    "    return x_zero_meaned.pow(2).mean(dim)\n",
    "\n",
    "\n",
    "class KLN01Loss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, direction, minimize):\n",
    "        super(KLN01Loss, self).__init__()\n",
    "        self.minimize = minimize\n",
    "        assert direction in ['pq', 'qp'], 'direction?'\n",
    "\n",
    "        self.direction = direction\n",
    "\n",
    "    def forward(self, samples):\n",
    "\n",
    "        assert samples.nelement() == samples.size(1) * samples.size(0), 'wtf?'\n",
    "\n",
    "        samples = samples.view(samples.size(0), -1)\n",
    "\n",
    "        self.samples_var = var(samples)\n",
    "        self.samples_mean = samples.mean(0)\n",
    "\n",
    "        samples_mean = self.samples_mean\n",
    "        samples_var = self.samples_var\n",
    "\n",
    "        if self.direction == 'pq':\n",
    "            # mu_1 = 0; sigma_1 = 1\n",
    "\n",
    "            t1 = (1 + samples_mean.pow(2)) / (2 * samples_var.pow(2))\n",
    "            t2 = samples_var.log()\n",
    "\n",
    "            KL = (t1 + t2 - 0.5).mean()\n",
    "        else:\n",
    "            # mu_2 = 0; sigma_2 = 1\n",
    "\n",
    "            t1 = (samples_var.pow(2) + samples_mean.pow(2)) / 2\n",
    "            t2 = -samples_var.log()\n",
    "\n",
    "            KL = (t1 + t2 - 0.5).mean()\n",
    "\n",
    "        if not self.minimize:\n",
    "            KL *= -1\n",
    "\n",
    "        return KL\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "def pairwise_euclidean(samples):\n",
    "\n",
    "    B = samples.size(0)\n",
    "\n",
    "    samples_norm = samples.mul(samples).sum(1)\n",
    "    samples_norm = samples_norm.expand(B, B)\n",
    "\n",
    "    dist_mat = samples.mm(samples.t()).mul(-2) + \\\n",
    "        samples_norm.add(samples_norm.t())\n",
    "    return dist_mat\n",
    "\n",
    "def sample_entropy(samples):\n",
    "\n",
    "        # Assume B x C input\n",
    "\n",
    "    dist_mat = pairwise_euclidean(samples)\n",
    "\n",
    "    # Get max and add it to diag\n",
    "    m = dist_mat.max().detach()\n",
    "    dist_mat_d = dist_mat + \\\n",
    "        Variable(torch.eye(dist_mat.size(0)) * (m.data[0] + 1)).cuda()\n",
    "\n",
    "    entropy = (dist_mat_d.min(1)[0] + 1e-4).log().sum()\n",
    "\n",
    "    entropy *= (samples.size(1) + 0.) / samples.size(0)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "class SampleKLN01Loss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, direction, minimize):\n",
    "        super(SampleKLN01Loss, self).__init__()\n",
    "        self.minimize = minimize\n",
    "        assert direction in ['pq', 'qp'], 'direction?'\n",
    "\n",
    "        self.direction = direction\n",
    "\n",
    "    def forward(self, samples):\n",
    "\n",
    "        assert samples.ndimension == 2, 'wft'\n",
    "        samples = samples.view(samples.size(0), -1)\n",
    "\n",
    "        self.samples_var = var(samples)\n",
    "        self.samples_mean = samples.mean(0)\n",
    "\n",
    "        if self.direction == 'pq':\n",
    "            assert False, 'not possible'\n",
    "        else:\n",
    "            entropy = sample_entropy(samples)\n",
    "\n",
    "            cross_entropy = - samples.pow(2).mean() / 2.\n",
    "\n",
    "            KL = - cross_entropy - entropy\n",
    "\n",
    "        if not self.minimize:\n",
    "            KL *= -1\n",
    "\n",
    "        return KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 1, 1])\n",
      "torch.Size([64, 64, 1, 1])\n",
      "Using parametric criterion KL_qp\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(batch_size, nc, image_size, image_size)\n",
    "z = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "fixed_z = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "\n",
    "print(fixed_z.shape)\n",
    "\n",
    "def normalize_(x, dim=1):\n",
    "    '''\n",
    "    Projects points to a sphere inplace.\n",
    "    '''\n",
    "    x.div_(x.norm(2, dim=dim).expand_as(x))\n",
    "\n",
    "if noise == 'sphere':\n",
    "    normalize_(fixed_z)\n",
    "\n",
    "print(fixed_z.shape)\n",
    "    \n",
    "# if cuda:\n",
    "#     netE.cuda()\n",
    "#     netG.cuda()\n",
    "#     x = x.cuda()\n",
    "#     z, fixed_z = z.cuda(), fixed_z.cuda()\n",
    "\n",
    "x = Variable(x)\n",
    "z = Variable(z)\n",
    "fixed_z = Variable(fixed_z)\n",
    "\n",
    "# Setup optimizers\n",
    "optimizerD = optim.Adam(netE.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Setup criterions\n",
    "if criterion == 'param':\n",
    "    print('Using parametric criterion KL_%s' % KL)\n",
    "    KL_minimizer = KLN01Loss(direction=KL, minimize=True)\n",
    "    KL_maximizer = KLN01Loss(direction=KL, minimize=False)\n",
    "elif criterion == 'nonparam':\n",
    "    print('Using NON-parametric criterion KL_%s' % KL)\n",
    "    KL_minimizer = SampleKLN01Loss(direction=KL, minimize=True)\n",
    "    KL_maximizer = SampleKLN01Loss(direction=KL, minimize=False)\n",
    "else:\n",
    "    assert False, 'criterion?'\n",
    "\n",
    "real_cpu = torch.FloatTensor()\n",
    "\n",
    "def save_images(epoch):\n",
    "\n",
    "    real_cpu.resize_(x.data.size()).copy_(x.data)\n",
    "\n",
    "    # Real samples\n",
    "    save_path = '%s/real_samples.png' % save_dir\n",
    "    vutils.save_image(real_cpu[:64] / 2 + 0.5, save_path)\n",
    "\n",
    "    netG.eval()\n",
    "    fake = netG(fixed_z)\n",
    "\n",
    "    # Fake samples\n",
    "    save_path = '%s/fake_samples_epoch_%03d.png' % (save_dir, epoch)\n",
    "    vutils.save_image(fake.data[:64] / 2 + 0.5, save_path)\n",
    "\n",
    "    # Save reconstructions\n",
    "    populate_x(x, dataloader['val'])\n",
    "    gex = netG(netE(x))\n",
    "\n",
    "    t = torch.FloatTensor(x.size(0) * 2, x.size(1),\n",
    "                          x.size(2), x.size(3))\n",
    "\n",
    "    t[0::2] = x.data[:]\n",
    "    t[1::2] = gex.data[:]\n",
    "\n",
    "    save_path = '%s/reconstructions_epoch_%03d.png' % (save_dir, epoch)\n",
    "    grid = vutils.save_image(t[:64] / 2 + 0.5, save_path)\n",
    "\n",
    "    netG.train()\n",
    "\n",
    "## ORIGINAL\n",
    "    \n",
    "# def adjust_lr(epoch):\n",
    "#     if epoch % drop_lr == (drop_lr - 1):\n",
    "#         lr /= 2\n",
    "#         for param_group in optimizerD.param_groups:\n",
    "#             param_group['lr'] = lr\n",
    "\n",
    "#         for param_group in optimizerG.param_groups:\n",
    "#             param_group['lr'] = lr\n",
    "            \n",
    "            \n",
    "def adjust_lr(epoch):\n",
    "    print(epoch, drop_lr)\n",
    "    if epoch % drop_lr == (drop_lr - 1):\n",
    "        ###\n",
    "        assert optimizerD.param_groups[0]['lr'] == optimizerG.param_groups[0]['lr']\n",
    "        print('Adjusting learning rate from %f to %f on E and G' % (lr, lr / 2))\n",
    "        optimizerD.param_groups[0]['lr'] /= 2\n",
    "        optimizerG.param_groups[0]['lr'] /= 2\n",
    "        ###\n",
    "#         lr /= 2\n",
    "#         for param_group in optimizerD.param_groups:\n",
    "#             param_group['lr'] = lr\n",
    "\n",
    "#         for param_group in optimizerG.param_groups:\n",
    "#             param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populate_x(x, dataloader):\n",
    "    '''\n",
    "    Fills input variable `x` with data generated with dataloader\n",
    "    '''\n",
    "    real_cpu, _ = dataloader.next()\n",
    "    x.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "\n",
    "\n",
    "def populate_z(z):\n",
    "    '''\n",
    "    Fills noise variable `z` with noise U(S^M)\n",
    "    '''\n",
    "    z.data.resize_(batch_size, nz, 1, 1)\n",
    "    z.data.normal_(0, 1)\n",
    "    if noise == 'sphere':\n",
    "        normalize_(z.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(x, y, dist):\n",
    "    '''\n",
    "    Computes distance between corresponding points points in `x` and `y`\n",
    "    using distance `dist`.\n",
    "    '''\n",
    "    if dist == 'L2':\n",
    "        return (x - y).pow(2).mean()\n",
    "    elif dist == 'L1':\n",
    "        return (x - y).abs().mean()\n",
    "    elif dist == 'cos':\n",
    "        x_n = normalize(x)\n",
    "        y_n = normalize(y)\n",
    "\n",
    "        return 2 - (x_n).mul(y_n).mean()\n",
    "    else:\n",
    "        assert dist == 'none', 'wtf ?'\n",
    "\n",
    "\n",
    "def normalize(x, dim=1):\n",
    "    '''\n",
    "    Projects points to a sphere.\n",
    "    '''\n",
    "    return x.div(x.norm(2, dim=dim).expand_as(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "Adjusting learning rate from 0.000200 to 0.000100 on E and G\n",
      "[0/1][0/937] KL_real/fake: 4.343/4.313 mean_real/fake: 0.005/0.017 var_real/fake: 0.009/0.009 \n",
      "[0/1][1/937] KL_real/fake: 4.188/4.181 mean_real/fake: 0.007/0.021 var_real/fake: 0.010/0.010 \n",
      "[0/1][2/937] KL_real/fake: 4.045/4.131 mean_real/fake: 0.013/0.019 var_real/fake: 0.012/0.010 \n",
      "[0/1][3/937] KL_real/fake: 3.908/4.093 mean_real/fake: 0.012/0.018 var_real/fake: 0.014/0.011 \n",
      "[0/1][4/937] KL_real/fake: 3.886/4.120 mean_real/fake: 0.021/0.023 var_real/fake: 0.015/0.010 \n",
      "[0/1][5/937] KL_real/fake: 3.804/4.076 mean_real/fake: 0.017/0.024 var_real/fake: 0.016/0.011 \n",
      "[0/1][6/937] KL_real/fake: 3.734/3.998 mean_real/fake: 0.019/0.023 var_real/fake: 0.017/0.012 \n",
      "[0/1][7/937] KL_real/fake: 3.754/4.020 mean_real/fake: 0.022/0.022 var_real/fake: 0.018/0.012 \n",
      "[0/1][8/937] KL_real/fake: 3.668/4.050 mean_real/fake: 0.021/0.025 var_real/fake: 0.019/0.011 \n",
      "[0/1][9/937] KL_real/fake: 3.672/4.088 mean_real/fake: 0.016/0.028 var_real/fake: 0.019/0.011 \n",
      "[0/1][10/937] KL_real/fake: 3.678/4.093 mean_real/fake: 0.018/0.031 var_real/fake: 0.020/0.011 \n",
      "[0/1][11/937] KL_real/fake: 3.759/4.182 mean_real/fake: 0.026/0.032 var_real/fake: 0.018/0.010 \n",
      "[0/1][12/937] KL_real/fake: 3.663/4.185 mean_real/fake: 0.029/0.030 var_real/fake: 0.022/0.010 \n",
      "[0/1][13/937] KL_real/fake: 3.675/4.200 mean_real/fake: 0.025/0.027 var_real/fake: 0.023/0.011 \n",
      "[0/1][14/937] KL_real/fake: 3.686/4.205 mean_real/fake: 0.027/0.032 var_real/fake: 0.022/0.011 \n",
      "[0/1][15/937] KL_real/fake: 3.625/4.289 mean_real/fake: 0.033/0.031 var_real/fake: 0.026/0.009 \n",
      "[0/1][16/937] KL_real/fake: 3.719/4.240 mean_real/fake: 0.038/0.038 var_real/fake: 0.022/0.010 \n",
      "[0/1][17/937] KL_real/fake: 3.739/4.283 mean_real/fake: 0.035/0.037 var_real/fake: 0.022/0.010 \n",
      "[0/1][18/937] KL_real/fake: 3.730/4.268 mean_real/fake: 0.024/0.034 var_real/fake: 0.020/0.010 \n",
      "[0/1][19/937] KL_real/fake: 3.685/4.342 mean_real/fake: 0.033/0.028 var_real/fake: 0.023/0.009 \n",
      "[0/1][20/937] KL_real/fake: 3.730/4.301 mean_real/fake: 0.031/0.029 var_real/fake: 0.022/0.010 \n",
      "[0/1][21/937] KL_real/fake: 3.589/4.350 mean_real/fake: 0.037/0.033 var_real/fake: 0.032/0.010 \n",
      "[0/1][22/937] KL_real/fake: 3.712/4.223 mean_real/fake: 0.035/0.036 var_real/fake: 0.024/0.011 \n",
      "[0/1][23/937] KL_real/fake: 3.472/4.118 mean_real/fake: 0.032/0.031 var_real/fake: 0.038/0.012 \n",
      "[0/1][24/937] KL_real/fake: 3.678/4.247 mean_real/fake: 0.039/0.043 var_real/fake: 0.028/0.011 \n",
      "[0/1][25/937] KL_real/fake: 3.799/4.277 mean_real/fake: 0.034/0.039 var_real/fake: 0.025/0.012 \n",
      "[0/1][26/937] KL_real/fake: 3.709/4.439 mean_real/fake: 0.014/0.031 var_real/fake: 0.028/0.009 \n",
      "[0/1][27/937] KL_real/fake: 3.794/4.349 mean_real/fake: 0.042/0.032 var_real/fake: 0.025/0.010 \n",
      "[0/1][28/937] KL_real/fake: 3.939/4.414 mean_real/fake: 0.044/0.034 var_real/fake: 0.019/0.009 \n",
      "[0/1][29/937] KL_real/fake: 3.923/4.508 mean_real/fake: 0.027/0.037 var_real/fake: 0.018/0.008 \n",
      "[0/1][30/937] KL_real/fake: 3.842/4.395 mean_real/fake: 0.033/0.030 var_real/fake: 0.023/0.009 \n",
      "[0/1][31/937] KL_real/fake: 3.713/4.481 mean_real/fake: 0.039/0.031 var_real/fake: 0.028/0.008 \n",
      "[0/1][32/937] KL_real/fake: 3.680/4.425 mean_real/fake: 0.048/0.032 var_real/fake: 0.028/0.009 \n",
      "[0/1][33/937] KL_real/fake: 3.938/4.446 mean_real/fake: 0.032/0.033 var_real/fake: 0.019/0.009 \n",
      "[0/1][34/937] KL_real/fake: 3.846/4.539 mean_real/fake: 0.045/0.030 var_real/fake: 0.022/0.007 \n",
      "[0/1][35/937] KL_real/fake: 3.586/4.626 mean_real/fake: 0.035/0.039 var_real/fake: 0.030/0.007 \n",
      "[0/1][36/937] KL_real/fake: 3.545/4.377 mean_real/fake: 0.024/0.037 var_real/fake: 0.037/0.009 \n",
      "[0/1][37/937] KL_real/fake: 3.644/4.391 mean_real/fake: 0.041/0.033 var_real/fake: 0.033/0.010 \n",
      "[0/1][38/937] KL_real/fake: 3.526/4.618 mean_real/fake: 0.046/0.039 var_real/fake: 0.042/0.007 \n",
      "[0/1][39/937] KL_real/fake: 3.719/4.694 mean_real/fake: 0.042/0.029 var_real/fake: 0.031/0.007 \n",
      "[0/1][40/937] KL_real/fake: 3.646/4.484 mean_real/fake: 0.043/0.040 var_real/fake: 0.035/0.008 \n",
      "[0/1][41/937] KL_real/fake: 3.724/4.609 mean_real/fake: 0.039/0.040 var_real/fake: 0.026/0.008 \n",
      "[0/1][42/937] KL_real/fake: 3.833/4.376 mean_real/fake: 0.046/0.041 var_real/fake: 0.024/0.010 \n",
      "[0/1][43/937] KL_real/fake: 3.675/4.385 mean_real/fake: 0.040/0.038 var_real/fake: 0.029/0.010 \n",
      "[0/1][44/937] KL_real/fake: 3.790/4.554 mean_real/fake: 0.050/0.040 var_real/fake: 0.028/0.008 \n",
      "[0/1][45/937] KL_real/fake: 3.644/4.443 mean_real/fake: 0.050/0.036 var_real/fake: 0.029/0.009 \n",
      "[0/1][46/937] KL_real/fake: 3.738/4.454 mean_real/fake: 0.041/0.042 var_real/fake: 0.029/0.009 \n",
      "[0/1][47/937] KL_real/fake: 3.566/4.540 mean_real/fake: 0.041/0.027 var_real/fake: 0.032/0.008 \n",
      "[0/1][48/937] KL_real/fake: 3.513/4.401 mean_real/fake: 0.040/0.040 var_real/fake: 0.032/0.011 \n",
      "[0/1][49/937] KL_real/fake: 3.724/4.746 mean_real/fake: 0.057/0.039 var_real/fake: 0.027/0.007 \n",
      "[0/1][50/937] KL_real/fake: 3.524/4.753 mean_real/fake: 0.045/0.036 var_real/fake: 0.040/0.008 \n",
      "[0/1][51/937] KL_real/fake: 3.577/4.932 mean_real/fake: 0.044/0.037 var_real/fake: 0.040/0.006 \n",
      "[0/1][52/937] KL_real/fake: 3.420/4.818 mean_real/fake: 0.034/0.037 var_real/fake: 0.042/0.007 \n",
      "[0/1][53/937] KL_real/fake: 3.896/4.966 mean_real/fake: 0.057/0.052 var_real/fake: 0.029/0.005 \n",
      "[0/1][54/937] KL_real/fake: 3.669/4.948 mean_real/fake: 0.042/0.042 var_real/fake: 0.035/0.005 \n",
      "[0/1][55/937] KL_real/fake: 3.747/4.804 mean_real/fake: 0.022/0.040 var_real/fake: 0.030/0.007 \n",
      "[0/1][56/937] KL_real/fake: 3.809/4.797 mean_real/fake: 0.050/0.042 var_real/fake: 0.025/0.007 \n",
      "[0/1][57/937] KL_real/fake: 3.832/4.804 mean_real/fake: 0.039/0.048 var_real/fake: 0.027/0.007 \n",
      "[0/1][58/937] KL_real/fake: 3.599/5.062 mean_real/fake: 0.052/0.045 var_real/fake: 0.033/0.005 \n",
      "[0/1][59/937] KL_real/fake: 3.356/4.433 mean_real/fake: 0.043/0.041 var_real/fake: 0.042/0.010 \n",
      "[0/1][60/937] KL_real/fake: 3.423/5.085 mean_real/fake: 0.052/0.041 var_real/fake: 0.054/0.005 \n",
      "[0/1][61/937] KL_real/fake: 3.433/5.161 mean_real/fake: 0.057/0.041 var_real/fake: 0.043/0.004 \n",
      "[0/1][62/937] KL_real/fake: 3.670/5.205 mean_real/fake: 0.043/0.041 var_real/fake: 0.035/0.004 \n",
      "[0/1][63/937] KL_real/fake: 3.639/4.945 mean_real/fake: 0.061/0.044 var_real/fake: 0.043/0.006 \n",
      "[0/1][64/937] KL_real/fake: 3.608/4.799 mean_real/fake: 0.070/0.047 var_real/fake: 0.042/0.007 \n",
      "[0/1][65/937] KL_real/fake: 3.279/4.690 mean_real/fake: 0.054/0.042 var_real/fake: 0.059/0.008 \n",
      "[0/1][66/937] KL_real/fake: 3.455/4.620 mean_real/fake: 0.052/0.045 var_real/fake: 0.044/0.008 \n",
      "[0/1][67/937] KL_real/fake: 3.645/4.593 mean_real/fake: 0.058/0.056 var_real/fake: 0.037/0.009 \n",
      "[0/1][68/937] KL_real/fake: 3.383/4.659 mean_real/fake: 0.069/0.049 var_real/fake: 0.057/0.009 \n",
      "[0/1][69/937] KL_real/fake: 3.552/4.959 mean_real/fake: 0.050/0.043 var_real/fake: 0.033/0.006 \n",
      "[0/1][70/937] KL_real/fake: 3.459/5.369 mean_real/fake: 0.055/0.037 var_real/fake: 0.044/0.004 \n",
      "[0/1][71/937] KL_real/fake: 3.462/5.488 mean_real/fake: 0.066/0.039 var_real/fake: 0.058/0.003 \n",
      "[0/1][72/937] KL_real/fake: 3.106/5.521 mean_real/fake: 0.057/0.051 var_real/fake: 0.075/0.003 \n",
      "[0/1][73/937] KL_real/fake: 3.651/5.901 mean_real/fake: 0.007/0.042 var_real/fake: 0.051/0.002 \n",
      "[0/1][74/937] KL_real/fake: 3.227/5.482 mean_real/fake: 0.102/0.046 var_real/fake: 0.076/0.003 \n",
      "[0/1][75/937] KL_real/fake: 3.408/4.838 mean_real/fake: 0.085/0.032 var_real/fake: 0.066/0.008 \n",
      "[0/1][76/937] KL_real/fake: 4.153/4.559 mean_real/fake: 0.064/0.056 var_real/fake: 0.026/0.013 \n",
      "[0/1][77/937] KL_real/fake: 3.943/4.410 mean_real/fake: 0.072/0.048 var_real/fake: 0.041/0.014 \n",
      "[0/1][78/937] KL_real/fake: 3.966/4.764 mean_real/fake: 0.037/0.051 var_real/fake: 0.026/0.008 \n",
      "[0/1][79/937] KL_real/fake: 3.739/4.930 mean_real/fake: 0.085/0.037 var_real/fake: 0.047/0.006 \n",
      "[0/1][80/937] KL_real/fake: 3.709/5.077 mean_real/fake: 0.070/0.043 var_real/fake: 0.035/0.005 \n",
      "[0/1][81/937] KL_real/fake: 3.560/5.170 mean_real/fake: 0.047/0.037 var_real/fake: 0.051/0.004 \n",
      "[0/1][82/937] KL_real/fake: 3.767/5.446 mean_real/fake: 0.056/0.038 var_real/fake: 0.037/0.003 \n",
      "[0/1][83/937] KL_real/fake: 3.719/5.297 mean_real/fake: 0.049/0.036 var_real/fake: 0.037/0.004 \n",
      "[0/1][84/937] KL_real/fake: 3.696/5.625 mean_real/fake: 0.050/0.041 var_real/fake: 0.034/0.003 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][85/937] KL_real/fake: 3.387/5.402 mean_real/fake: 0.040/0.041 var_real/fake: 0.045/0.003 \n",
      "[0/1][86/937] KL_real/fake: 3.248/4.811 mean_real/fake: 0.052/0.040 var_real/fake: 0.047/0.007 \n",
      "[0/1][87/937] KL_real/fake: 3.967/5.032 mean_real/fake: 0.070/0.040 var_real/fake: 0.026/0.006 \n",
      "[0/1][88/937] KL_real/fake: 3.809/5.038 mean_real/fake: 0.027/0.047 var_real/fake: 0.034/0.005 \n",
      "[0/1][89/937] KL_real/fake: 3.439/4.724 mean_real/fake: 0.044/0.048 var_real/fake: 0.048/0.009 \n",
      "[0/1][90/937] KL_real/fake: 3.655/4.812 mean_real/fake: 0.046/0.047 var_real/fake: 0.035/0.008 \n",
      "[0/1][91/937] KL_real/fake: 3.385/4.312 mean_real/fake: 0.059/0.046 var_real/fake: 0.054/0.016 \n",
      "[0/1][92/937] KL_real/fake: 4.172/5.050 mean_real/fake: 0.074/0.051 var_real/fake: 0.022/0.006 \n",
      "[0/1][93/937] KL_real/fake: 3.920/5.499 mean_real/fake: 0.068/0.029 var_real/fake: 0.029/0.003 \n",
      "[0/1][94/937] KL_real/fake: 3.753/5.312 mean_real/fake: 0.059/0.045 var_real/fake: 0.033/0.004 \n",
      "[0/1][95/937] KL_real/fake: 3.112/5.251 mean_real/fake: 0.057/0.045 var_real/fake: 0.075/0.004 \n",
      "[0/1][96/937] KL_real/fake: 4.197/5.582 mean_real/fake: 0.047/0.031 var_real/fake: 0.026/0.003 \n",
      "[0/1][97/937] KL_real/fake: 3.703/5.479 mean_real/fake: 0.077/0.055 var_real/fake: 0.040/0.004 \n",
      "[0/1][98/937] KL_real/fake: 3.610/4.680 mean_real/fake: 0.038/0.059 var_real/fake: 0.033/0.010 \n",
      "[0/1][99/937] KL_real/fake: 3.962/4.816 mean_real/fake: 0.030/0.048 var_real/fake: 0.034/0.009 \n",
      "[0/1][100/937] KL_real/fake: 4.204/5.024 mean_real/fake: 0.040/0.034 var_real/fake: 0.016/0.006 \n",
      "[0/1][101/937] KL_real/fake: 3.589/4.951 mean_real/fake: 0.067/0.041 var_real/fake: 0.045/0.007 \n",
      "[0/1][102/937] KL_real/fake: 3.481/5.007 mean_real/fake: 0.051/0.049 var_real/fake: 0.040/0.006 \n",
      "[0/1][103/937] KL_real/fake: 3.471/5.046 mean_real/fake: 0.039/0.054 var_real/fake: 0.044/0.006 \n",
      "[0/1][104/937] KL_real/fake: 3.765/5.881 mean_real/fake: 0.063/0.038 var_real/fake: 0.039/0.002 \n",
      "[0/1][105/937] KL_real/fake: 3.118/4.857 mean_real/fake: 0.048/0.048 var_real/fake: 0.056/0.008 \n",
      "[0/1][106/937] KL_real/fake: 3.430/4.950 mean_real/fake: 0.074/0.059 var_real/fake: 0.051/0.008 \n",
      "[0/1][107/937] KL_real/fake: 3.899/5.362 mean_real/fake: 0.066/0.052 var_real/fake: 0.039/0.004 \n",
      "[0/1][108/937] KL_real/fake: 3.973/5.021 mean_real/fake: 0.054/0.046 var_real/fake: 0.024/0.007 \n",
      "[0/1][109/937] KL_real/fake: 3.532/4.658 mean_real/fake: 0.036/0.038 var_real/fake: 0.047/0.012 \n",
      "[0/1][110/937] KL_real/fake: 3.991/4.970 mean_real/fake: 0.054/0.057 var_real/fake: 0.024/0.008 \n",
      "[0/1][111/937] KL_real/fake: 3.657/4.746 mean_real/fake: 0.052/0.037 var_real/fake: 0.046/0.010 \n",
      "[0/1][112/937] KL_real/fake: 3.866/5.285 mean_real/fake: 0.025/0.040 var_real/fake: 0.031/0.004 \n",
      "[0/1][113/937] KL_real/fake: 3.402/4.778 mean_real/fake: 0.063/0.040 var_real/fake: 0.066/0.010 \n",
      "[0/1][114/937] KL_real/fake: 3.451/4.687 mean_real/fake: 0.038/0.040 var_real/fake: 0.048/0.010 \n",
      "[0/1][115/937] KL_real/fake: 3.884/5.185 mean_real/fake: 0.064/0.037 var_real/fake: 0.045/0.005 \n",
      "[0/1][116/937] KL_real/fake: 3.508/5.158 mean_real/fake: 0.054/0.042 var_real/fake: 0.054/0.007 \n",
      "[0/1][117/937] KL_real/fake: 3.269/4.478 mean_real/fake: 0.065/0.031 var_real/fake: 0.062/0.014 \n",
      "[0/1][118/937] KL_real/fake: 4.325/5.525 mean_real/fake: 0.041/0.048 var_real/fake: 0.021/0.003 \n",
      "[0/1][119/937] KL_real/fake: 3.834/5.494 mean_real/fake: 0.021/0.048 var_real/fake: 0.039/0.004 \n",
      "[0/1][120/937] KL_real/fake: 3.625/5.351 mean_real/fake: 0.045/0.039 var_real/fake: 0.042/0.004 \n",
      "[0/1][121/937] KL_real/fake: 3.485/4.944 mean_real/fake: 0.019/0.052 var_real/fake: 0.045/0.007 \n",
      "[0/1][122/937] KL_real/fake: 3.536/5.125 mean_real/fake: 0.058/0.041 var_real/fake: 0.057/0.006 \n",
      "[0/1][123/937] KL_real/fake: 4.574/5.671 mean_real/fake: 0.055/0.035 var_real/fake: 0.012/0.003 \n",
      "[0/1][124/937] KL_real/fake: 4.358/5.654 mean_real/fake: 0.043/0.051 var_real/fake: 0.015/0.003 \n",
      "[0/1][125/937] KL_real/fake: 3.566/4.808 mean_real/fake: 0.047/0.056 var_real/fake: 0.046/0.009 \n",
      "[0/1][126/937] KL_real/fake: 3.706/4.965 mean_real/fake: 0.058/0.037 var_real/fake: 0.044/0.008 \n",
      "[0/1][127/937] KL_real/fake: 4.195/5.358 mean_real/fake: 0.046/0.047 var_real/fake: 0.020/0.005 \n",
      "[0/1][128/937] KL_real/fake: 3.605/5.067 mean_real/fake: 0.078/0.043 var_real/fake: 0.046/0.006 \n",
      "[0/1][129/937] KL_real/fake: 3.585/5.233 mean_real/fake: 0.056/0.039 var_real/fake: 0.055/0.006 \n",
      "[0/1][130/937] KL_real/fake: 3.972/5.750 mean_real/fake: 0.069/0.043 var_real/fake: 0.030/0.003 \n",
      "[0/1][131/937] KL_real/fake: 3.582/5.048 mean_real/fake: 0.042/0.045 var_real/fake: 0.046/0.006 \n",
      "[0/1][132/937] KL_real/fake: 4.227/5.449 mean_real/fake: 0.069/0.051 var_real/fake: 0.030/0.004 \n",
      "[0/1][133/937] KL_real/fake: 3.601/5.379 mean_real/fake: 0.047/0.046 var_real/fake: 0.047/0.004 \n",
      "[0/1][134/937] KL_real/fake: 3.844/5.429 mean_real/fake: 0.057/0.048 var_real/fake: 0.031/0.004 \n",
      "[0/1][135/937] KL_real/fake: 3.485/4.696 mean_real/fake: 0.053/0.035 var_real/fake: 0.050/0.011 \n",
      "[0/1][136/937] KL_real/fake: 4.289/5.686 mean_real/fake: 0.066/0.037 var_real/fake: 0.021/0.003 \n",
      "[0/1][137/937] KL_real/fake: 3.721/5.263 mean_real/fake: 0.078/0.044 var_real/fake: 0.054/0.005 \n",
      "[0/1][138/937] KL_real/fake: 4.046/5.489 mean_real/fake: 0.051/0.042 var_real/fake: 0.032/0.004 \n",
      "[0/1][139/937] KL_real/fake: 3.491/5.033 mean_real/fake: 0.057/0.028 var_real/fake: 0.057/0.006 \n",
      "[0/1][140/937] KL_real/fake: 4.073/5.564 mean_real/fake: 0.031/0.042 var_real/fake: 0.024/0.003 \n",
      "[0/1][141/937] KL_real/fake: 3.498/5.035 mean_real/fake: 0.044/0.025 var_real/fake: 0.056/0.007 \n",
      "[0/1][142/937] KL_real/fake: 4.687/5.948 mean_real/fake: 0.042/0.043 var_real/fake: 0.010/0.002 \n",
      "[0/1][143/937] KL_real/fake: 4.322/5.707 mean_real/fake: 0.057/0.036 var_real/fake: 0.022/0.003 \n",
      "[0/1][144/937] KL_real/fake: 3.472/5.113 mean_real/fake: 0.053/0.039 var_real/fake: 0.076/0.006 \n",
      "[0/1][145/937] KL_real/fake: 3.534/4.665 mean_real/fake: 0.072/0.052 var_real/fake: 0.054/0.011 \n",
      "[0/1][146/937] KL_real/fake: 4.423/5.500 mean_real/fake: 0.046/0.032 var_real/fake: 0.016/0.004 \n",
      "[0/1][147/937] KL_real/fake: 4.370/5.547 mean_real/fake: 0.054/0.041 var_real/fake: 0.017/0.003 \n",
      "[0/1][148/937] KL_real/fake: 3.751/4.988 mean_real/fake: 0.033/0.051 var_real/fake: 0.036/0.008 \n",
      "[0/1][149/937] KL_real/fake: 3.794/5.269 mean_real/fake: 0.080/0.042 var_real/fake: 0.048/0.005 \n",
      "[0/1][150/937] KL_real/fake: 4.272/5.636 mean_real/fake: 0.032/0.046 var_real/fake: 0.019/0.003 \n",
      "[0/1][151/937] KL_real/fake: 3.729/5.358 mean_real/fake: 0.055/0.042 var_real/fake: 0.036/0.004 \n",
      "[0/1][152/937] KL_real/fake: 4.284/5.839 mean_real/fake: 0.035/0.041 var_real/fake: 0.025/0.002 \n",
      "[0/1][153/937] KL_real/fake: 3.382/5.161 mean_real/fake: 0.042/0.048 var_real/fake: 0.048/0.006 \n",
      "[0/1][154/937] KL_real/fake: 4.185/5.893 mean_real/fake: 0.047/0.038 var_real/fake: 0.026/0.002 \n",
      "[0/1][155/937] KL_real/fake: 3.419/5.173 mean_real/fake: 0.045/0.046 var_real/fake: 0.058/0.006 \n",
      "[0/1][156/937] KL_real/fake: 4.568/5.959 mean_real/fake: 0.027/0.039 var_real/fake: 0.012/0.002 \n",
      "[0/1][157/937] KL_real/fake: 3.800/5.638 mean_real/fake: 0.036/0.047 var_real/fake: 0.032/0.003 \n",
      "[0/1][158/937] KL_real/fake: 3.207/4.112 mean_real/fake: 0.027/0.055 var_real/fake: 0.046/0.025 \n",
      "[0/1][159/937] KL_real/fake: 5.137/6.079 mean_real/fake: 0.041/0.034 var_real/fake: 0.005/0.002 \n",
      "[0/1][160/937] KL_real/fake: 5.502/6.411 mean_real/fake: 0.040/0.044 var_real/fake: 0.003/0.001 \n",
      "[0/1][161/937] KL_real/fake: 4.786/6.091 mean_real/fake: 0.037/0.046 var_real/fake: 0.008/0.002 \n",
      "[0/1][162/937] KL_real/fake: 3.463/5.359 mean_real/fake: 0.036/0.053 var_real/fake: 0.047/0.004 \n",
      "[0/1][163/937] KL_real/fake: 3.444/4.984 mean_real/fake: 0.074/0.035 var_real/fake: 0.064/0.006 \n",
      "[0/1][164/937] KL_real/fake: 4.389/5.979 mean_real/fake: 0.045/0.045 var_real/fake: 0.015/0.002 \n",
      "[0/1][165/937] KL_real/fake: 4.240/6.042 mean_real/fake: 0.060/0.041 var_real/fake: 0.017/0.002 \n",
      "[0/1][166/937] KL_real/fake: 3.447/4.371 mean_real/fake: 0.054/0.039 var_real/fake: 0.036/0.016 \n",
      "[0/1][167/937] KL_real/fake: 3.547/5.191 mean_real/fake: 0.058/0.040 var_real/fake: 0.053/0.005 \n",
      "[0/1][168/937] KL_real/fake: 4.753/5.881 mean_real/fake: 0.049/0.033 var_real/fake: 0.008/0.002 \n",
      "[0/1][169/937] KL_real/fake: 4.781/6.135 mean_real/fake: 0.037/0.039 var_real/fake: 0.008/0.002 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][170/937] KL_real/fake: 3.487/5.360 mean_real/fake: 0.083/0.044 var_real/fake: 0.057/0.005 \n",
      "[0/1][171/937] KL_real/fake: 3.708/5.304 mean_real/fake: 0.059/0.035 var_real/fake: 0.047/0.004 \n",
      "[0/1][172/937] KL_real/fake: 3.882/5.559 mean_real/fake: 0.065/0.041 var_real/fake: 0.043/0.004 \n",
      "[0/1][173/937] KL_real/fake: 3.998/5.547 mean_real/fake: 0.064/0.044 var_real/fake: 0.027/0.003 \n",
      "[0/1][174/937] KL_real/fake: 3.444/5.118 mean_real/fake: 0.030/0.036 var_real/fake: 0.050/0.006 \n",
      "[0/1][175/937] KL_real/fake: 3.747/5.286 mean_real/fake: 0.049/0.045 var_real/fake: 0.040/0.005 \n",
      "[0/1][176/937] KL_real/fake: 4.069/5.450 mean_real/fake: 0.059/0.040 var_real/fake: 0.025/0.003 \n",
      "[0/1][177/937] KL_real/fake: 3.275/5.154 mean_real/fake: 0.067/0.043 var_real/fake: 0.059/0.006 \n",
      "[0/1][178/937] KL_real/fake: 4.128/5.581 mean_real/fake: 0.034/0.045 var_real/fake: 0.029/0.003 \n",
      "[0/1][179/937] KL_real/fake: 3.851/5.563 mean_real/fake: 0.044/0.038 var_real/fake: 0.032/0.003 \n",
      "[0/1][180/937] KL_real/fake: 3.900/5.732 mean_real/fake: 0.056/0.044 var_real/fake: 0.034/0.003 \n",
      "[0/1][181/937] KL_real/fake: 3.501/5.165 mean_real/fake: 0.040/0.062 var_real/fake: 0.050/0.006 \n",
      "[0/1][182/937] KL_real/fake: 3.851/5.504 mean_real/fake: 0.074/0.051 var_real/fake: 0.047/0.004 \n",
      "[0/1][183/937] KL_real/fake: 3.643/5.268 mean_real/fake: 0.034/0.044 var_real/fake: 0.045/0.004 \n",
      "[0/1][184/937] KL_real/fake: 3.831/5.548 mean_real/fake: 0.049/0.050 var_real/fake: 0.036/0.004 \n",
      "[0/1][185/937] KL_real/fake: 3.300/5.184 mean_real/fake: 0.066/0.039 var_real/fake: 0.053/0.006 \n",
      "[0/1][186/937] KL_real/fake: 4.316/5.972 mean_real/fake: 0.064/0.036 var_real/fake: 0.016/0.002 \n",
      "[0/1][187/937] KL_real/fake: 3.846/5.281 mean_real/fake: 0.043/0.046 var_real/fake: 0.032/0.006 \n",
      "[0/1][188/937] KL_real/fake: 3.429/4.998 mean_real/fake: 0.040/0.031 var_real/fake: 0.046/0.006 \n",
      "[0/1][189/937] KL_real/fake: 4.298/5.905 mean_real/fake: 0.059/0.043 var_real/fake: 0.019/0.002 \n",
      "[0/1][190/937] KL_real/fake: 3.546/5.291 mean_real/fake: 0.021/0.050 var_real/fake: 0.056/0.005 \n",
      "[0/1][191/937] KL_real/fake: 3.484/4.850 mean_real/fake: 0.067/0.055 var_real/fake: 0.059/0.008 \n",
      "[0/1][192/937] KL_real/fake: 4.516/5.847 mean_real/fake: 0.068/0.038 var_real/fake: 0.017/0.002 \n",
      "[0/1][193/937] KL_real/fake: 4.157/5.712 mean_real/fake: 0.065/0.043 var_real/fake: 0.021/0.003 \n",
      "[0/1][194/937] KL_real/fake: 3.448/4.662 mean_real/fake: 0.034/0.047 var_real/fake: 0.041/0.009 \n",
      "[0/1][195/937] KL_real/fake: 3.773/5.449 mean_real/fake: 0.056/0.039 var_real/fake: 0.036/0.004 \n",
      "[0/1][196/937] KL_real/fake: 3.981/5.392 mean_real/fake: 0.045/0.042 var_real/fake: 0.021/0.004 \n",
      "[0/1][197/937] KL_real/fake: 3.045/4.881 mean_real/fake: 0.036/0.051 var_real/fake: 0.078/0.009 \n",
      "[0/1][198/937] KL_real/fake: 5.548/6.535 mean_real/fake: 0.041/0.044 var_real/fake: 0.003/0.001 \n",
      "[0/1][199/937] KL_real/fake: 5.504/6.733 mean_real/fake: 0.044/0.041 var_real/fake: 0.003/0.001 \n",
      "[0/1][200/937] KL_real/fake: 4.724/6.339 mean_real/fake: 0.049/0.038 var_real/fake: 0.008/0.001 \n",
      "[0/1][201/937] KL_real/fake: 3.320/4.775 mean_real/fake: 0.093/0.050 var_real/fake: 0.066/0.010 \n",
      "[0/1][202/937] KL_real/fake: 3.894/5.515 mean_real/fake: 0.075/0.043 var_real/fake: 0.048/0.003 \n",
      "[0/1][203/937] KL_real/fake: 4.414/5.949 mean_real/fake: 0.056/0.043 var_real/fake: 0.021/0.002 \n",
      "[0/1][204/937] KL_real/fake: 4.144/5.661 mean_real/fake: 0.069/0.050 var_real/fake: 0.024/0.003 \n",
      "[0/1][205/937] KL_real/fake: 3.325/5.196 mean_real/fake: 0.047/0.041 var_real/fake: 0.070/0.005 \n",
      "[0/1][206/937] KL_real/fake: 4.270/5.973 mean_real/fake: 0.066/0.041 var_real/fake: 0.018/0.002 \n",
      "[0/1][207/937] KL_real/fake: 3.481/4.915 mean_real/fake: 0.032/0.040 var_real/fake: 0.045/0.007 \n",
      "[0/1][208/937] KL_real/fake: 4.447/6.104 mean_real/fake: 0.037/0.042 var_real/fake: 0.014/0.002 \n",
      "[0/1][209/937] KL_real/fake: 4.091/5.637 mean_real/fake: 0.040/0.035 var_real/fake: 0.031/0.003 \n",
      "[0/1][210/937] KL_real/fake: 3.509/5.136 mean_real/fake: 0.082/0.048 var_real/fake: 0.049/0.007 \n",
      "[0/1][211/937] KL_real/fake: 3.749/5.714 mean_real/fake: 0.056/0.041 var_real/fake: 0.051/0.003 \n",
      "[0/1][212/937] KL_real/fake: 4.636/6.274 mean_real/fake: 0.025/0.039 var_real/fake: 0.015/0.001 \n",
      "[0/1][213/937] KL_real/fake: 3.857/5.521 mean_real/fake: 0.035/0.042 var_real/fake: 0.037/0.003 \n",
      "[0/1][214/937] KL_real/fake: 3.337/4.595 mean_real/fake: 0.057/0.039 var_real/fake: 0.056/0.014 \n",
      "[0/1][215/937] KL_real/fake: 3.648/5.441 mean_real/fake: 0.055/0.048 var_real/fake: 0.049/0.004 \n",
      "[0/1][216/937] KL_real/fake: 5.001/6.374 mean_real/fake: 0.056/0.038 var_real/fake: 0.006/0.001 \n",
      "[0/1][217/937] KL_real/fake: 4.502/5.945 mean_real/fake: 0.057/0.050 var_real/fake: 0.018/0.002 \n",
      "[0/1][218/937] KL_real/fake: 3.631/4.762 mean_real/fake: 0.051/0.047 var_real/fake: 0.049/0.009 \n",
      "[0/1][219/937] KL_real/fake: 4.121/5.071 mean_real/fake: 0.049/0.053 var_real/fake: 0.028/0.008 \n",
      "[0/1][220/937] KL_real/fake: 4.680/5.762 mean_real/fake: 0.064/0.043 var_real/fake: 0.015/0.002 \n",
      "[0/1][221/937] KL_real/fake: 4.160/5.856 mean_real/fake: 0.042/0.051 var_real/fake: 0.028/0.003 \n",
      "[0/1][222/937] KL_real/fake: 3.383/5.071 mean_real/fake: 0.048/0.055 var_real/fake: 0.055/0.006 \n",
      "[0/1][223/937] KL_real/fake: 3.678/5.431 mean_real/fake: 0.082/0.052 var_real/fake: 0.060/0.004 \n",
      "[0/1][224/937] KL_real/fake: 3.897/5.792 mean_real/fake: 0.052/0.043 var_real/fake: 0.033/0.002 \n",
      "[0/1][225/937] KL_real/fake: 3.488/4.334 mean_real/fake: 0.047/0.044 var_real/fake: 0.045/0.015 \n",
      "[0/1][226/937] KL_real/fake: 3.936/5.193 mean_real/fake: 0.065/0.045 var_real/fake: 0.034/0.006 \n",
      "[0/1][227/937] KL_real/fake: 4.510/6.103 mean_real/fake: 0.056/0.037 var_real/fake: 0.014/0.002 \n",
      "[0/1][228/937] KL_real/fake: 3.813/5.755 mean_real/fake: 0.072/0.045 var_real/fake: 0.052/0.002 \n",
      "[0/1][229/937] KL_real/fake: 3.454/5.025 mean_real/fake: 0.038/0.054 var_real/fake: 0.044/0.007 \n",
      "[0/1][230/937] KL_real/fake: 4.005/5.515 mean_real/fake: 0.074/0.036 var_real/fake: 0.036/0.003 \n",
      "[0/1][231/937] KL_real/fake: 3.688/5.665 mean_real/fake: 0.056/0.044 var_real/fake: 0.050/0.003 \n",
      "[0/1][232/937] KL_real/fake: 3.834/5.660 mean_real/fake: 0.022/0.042 var_real/fake: 0.044/0.003 \n",
      "[0/1][233/937] KL_real/fake: 3.569/5.171 mean_real/fake: 0.061/0.053 var_real/fake: 0.050/0.006 \n",
      "[0/1][234/937] KL_real/fake: 3.409/5.109 mean_real/fake: 0.050/0.047 var_real/fake: 0.060/0.006 \n",
      "[0/1][235/937] KL_real/fake: 4.374/5.927 mean_real/fake: 0.047/0.039 var_real/fake: 0.019/0.002 \n",
      "[0/1][236/937] KL_real/fake: 3.840/5.439 mean_real/fake: 0.023/0.035 var_real/fake: 0.031/0.004 \n",
      "[0/1][237/937] KL_real/fake: 4.217/5.285 mean_real/fake: 0.026/0.042 var_real/fake: 0.018/0.004 \n",
      "[0/1][238/937] KL_real/fake: 3.851/5.827 mean_real/fake: 0.061/0.044 var_real/fake: 0.048/0.002 \n",
      "[0/1][239/937] KL_real/fake: 4.193/5.724 mean_real/fake: 0.041/0.042 var_real/fake: 0.025/0.003 \n",
      "[0/1][240/937] KL_real/fake: 4.067/5.361 mean_real/fake: 0.071/0.053 var_real/fake: 0.030/0.004 \n",
      "[0/1][241/937] KL_real/fake: 3.815/5.640 mean_real/fake: 0.074/0.043 var_real/fake: 0.041/0.003 \n",
      "[0/1][242/937] KL_real/fake: 3.454/5.424 mean_real/fake: 0.076/0.043 var_real/fake: 0.053/0.004 \n",
      "[0/1][243/937] KL_real/fake: 3.298/4.937 mean_real/fake: 0.017/0.041 var_real/fake: 0.041/0.006 \n",
      "[0/1][244/937] KL_real/fake: 4.890/6.421 mean_real/fake: 0.050/0.040 var_real/fake: 0.007/0.001 \n",
      "[0/1][245/937] KL_real/fake: 4.131/5.538 mean_real/fake: 0.045/0.037 var_real/fake: 0.021/0.003 \n",
      "[0/1][246/937] KL_real/fake: 3.095/4.177 mean_real/fake: 0.054/0.046 var_real/fake: 0.058/0.021 \n",
      "[0/1][247/937] KL_real/fake: 5.070/6.414 mean_real/fake: 0.066/0.041 var_real/fake: 0.006/0.001 \n",
      "[0/1][248/937] KL_real/fake: 5.068/6.589 mean_real/fake: 0.040/0.040 var_real/fake: 0.006/0.001 \n",
      "[0/1][249/937] KL_real/fake: 3.655/5.608 mean_real/fake: 0.040/0.049 var_real/fake: 0.048/0.003 \n",
      "[0/1][250/937] KL_real/fake: 3.493/5.087 mean_real/fake: 0.032/0.041 var_real/fake: 0.052/0.005 \n",
      "[0/1][251/937] KL_real/fake: 4.174/5.536 mean_real/fake: 0.095/0.044 var_real/fake: 0.036/0.003 \n",
      "[0/1][252/937] KL_real/fake: 4.446/5.707 mean_real/fake: 0.043/0.041 var_real/fake: 0.013/0.003 \n",
      "[0/1][253/937] KL_real/fake: 3.813/5.170 mean_real/fake: 0.064/0.046 var_real/fake: 0.049/0.006 \n",
      "[0/1][254/937] KL_real/fake: 3.780/5.441 mean_real/fake: 0.053/0.033 var_real/fake: 0.038/0.003 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][255/937] KL_real/fake: 3.191/4.883 mean_real/fake: 0.058/0.044 var_real/fake: 0.076/0.008 \n",
      "[0/1][256/937] KL_real/fake: 4.437/6.147 mean_real/fake: 0.045/0.050 var_real/fake: 0.013/0.002 \n",
      "[0/1][257/937] KL_real/fake: 4.081/5.992 mean_real/fake: 0.060/0.039 var_real/fake: 0.039/0.002 \n",
      "[0/1][258/937] KL_real/fake: 3.133/4.896 mean_real/fake: 0.050/0.068 var_real/fake: 0.067/0.009 \n",
      "[0/1][259/937] KL_real/fake: 4.822/6.331 mean_real/fake: 0.027/0.039 var_real/fake: 0.012/0.001 \n",
      "[0/1][260/937] KL_real/fake: 3.976/5.808 mean_real/fake: 0.053/0.035 var_real/fake: 0.037/0.002 \n",
      "[0/1][261/937] KL_real/fake: 4.028/5.836 mean_real/fake: 0.042/0.046 var_real/fake: 0.030/0.002 \n",
      "[0/1][262/937] KL_real/fake: 3.514/5.270 mean_real/fake: 0.048/0.043 var_real/fake: 0.056/0.005 \n",
      "[0/1][263/937] KL_real/fake: 4.056/5.789 mean_real/fake: 0.084/0.041 var_real/fake: 0.036/0.002 \n",
      "[0/1][264/937] KL_real/fake: 3.863/5.499 mean_real/fake: 0.090/0.047 var_real/fake: 0.053/0.004 \n",
      "[0/1][265/937] KL_real/fake: 3.389/5.241 mean_real/fake: 0.056/0.041 var_real/fake: 0.055/0.004 \n",
      "[0/1][266/937] KL_real/fake: 3.828/5.572 mean_real/fake: 0.027/0.039 var_real/fake: 0.040/0.003 \n",
      "[0/1][267/937] KL_real/fake: 3.762/5.444 mean_real/fake: 0.046/0.045 var_real/fake: 0.033/0.003 \n",
      "[0/1][268/937] KL_real/fake: 4.817/6.186 mean_real/fake: 0.054/0.041 var_real/fake: 0.009/0.001 \n",
      "[0/1][269/937] KL_real/fake: 3.614/5.177 mean_real/fake: 0.065/0.041 var_real/fake: 0.041/0.005 \n",
      "[0/1][270/937] KL_real/fake: 3.686/5.166 mean_real/fake: 0.045/0.056 var_real/fake: 0.036/0.005 \n",
      "[0/1][271/937] KL_real/fake: 4.651/6.228 mean_real/fake: 0.053/0.032 var_real/fake: 0.013/0.001 \n",
      "[0/1][272/937] KL_real/fake: 4.011/5.951 mean_real/fake: 0.029/0.037 var_real/fake: 0.026/0.002 \n",
      "[0/1][273/937] KL_real/fake: 3.395/5.229 mean_real/fake: 0.059/0.040 var_real/fake: 0.063/0.005 \n",
      "[0/1][274/937] KL_real/fake: 4.111/6.058 mean_real/fake: 0.056/0.035 var_real/fake: 0.027/0.002 \n",
      "[0/1][275/937] KL_real/fake: 3.673/5.876 mean_real/fake: 0.074/0.045 var_real/fake: 0.047/0.002 \n",
      "[0/1][276/937] KL_real/fake: 2.973/4.274 mean_real/fake: 0.032/0.031 var_real/fake: 0.063/0.015 \n",
      "[0/1][277/937] KL_real/fake: 4.792/6.352 mean_real/fake: 0.045/0.040 var_real/fake: 0.008/0.001 \n",
      "[0/1][278/937] KL_real/fake: 4.388/5.899 mean_real/fake: 0.057/0.037 var_real/fake: 0.015/0.002 \n",
      "[0/1][279/937] KL_real/fake: 3.459/5.211 mean_real/fake: 0.083/0.050 var_real/fake: 0.053/0.005 \n",
      "[0/1][280/937] KL_real/fake: 4.513/5.777 mean_real/fake: 0.080/0.039 var_real/fake: 0.016/0.003 \n",
      "[0/1][281/937] KL_real/fake: 3.668/5.184 mean_real/fake: 0.046/0.041 var_real/fake: 0.035/0.006 \n",
      "[0/1][282/937] KL_real/fake: 3.767/5.071 mean_real/fake: 0.042/0.031 var_real/fake: 0.032/0.006 \n",
      "[0/1][283/937] KL_real/fake: 3.639/5.688 mean_real/fake: 0.044/0.041 var_real/fake: 0.037/0.003 \n",
      "[0/1][284/937] KL_real/fake: 4.341/5.896 mean_real/fake: 0.045/0.040 var_real/fake: 0.013/0.002 \n",
      "[0/1][285/937] KL_real/fake: 2.945/4.414 mean_real/fake: 0.027/0.043 var_real/fake: 0.074/0.013 \n",
      "[0/1][286/937] KL_real/fake: 5.714/6.759 mean_real/fake: 0.043/0.039 var_real/fake: 0.002/0.001 \n",
      "[0/1][287/937] KL_real/fake: 5.842/6.917 mean_real/fake: 0.040/0.042 var_real/fake: 0.002/0.001 \n",
      "[0/1][288/937] KL_real/fake: 4.540/5.736 mean_real/fake: 0.067/0.041 var_real/fake: 0.014/0.002 \n",
      "[0/1][289/937] KL_real/fake: 3.254/4.863 mean_real/fake: 0.056/0.056 var_real/fake: 0.065/0.012 \n",
      "[0/1][290/937] KL_real/fake: 3.728/5.348 mean_real/fake: 0.090/0.044 var_real/fake: 0.054/0.005 \n",
      "[0/1][291/937] KL_real/fake: 4.230/5.876 mean_real/fake: 0.028/0.042 var_real/fake: 0.034/0.002 \n",
      "[0/1][292/937] KL_real/fake: 4.246/6.123 mean_real/fake: 0.027/0.034 var_real/fake: 0.020/0.002 \n",
      "[0/1][293/937] KL_real/fake: 3.263/4.701 mean_real/fake: 0.037/0.052 var_real/fake: 0.047/0.010 \n",
      "[0/1][294/937] KL_real/fake: 3.872/5.954 mean_real/fake: 0.061/0.045 var_real/fake: 0.032/0.002 \n",
      "[0/1][295/937] KL_real/fake: 3.485/5.187 mean_real/fake: 0.049/0.041 var_real/fake: 0.041/0.005 \n",
      "[0/1][296/937] KL_real/fake: 3.270/5.157 mean_real/fake: 0.073/0.045 var_real/fake: 0.071/0.005 \n",
      "[0/1][297/937] KL_real/fake: 3.969/5.705 mean_real/fake: 0.050/0.038 var_real/fake: 0.026/0.003 \n",
      "[0/1][298/937] KL_real/fake: 3.178/4.868 mean_real/fake: 0.036/0.039 var_real/fake: 0.065/0.010 \n",
      "[0/1][299/937] KL_real/fake: 3.925/5.883 mean_real/fake: 0.033/0.046 var_real/fake: 0.041/0.002 \n",
      "[0/1][300/937] KL_real/fake: 3.484/5.272 mean_real/fake: 0.061/0.045 var_real/fake: 0.069/0.005 \n",
      "[0/1][301/937] KL_real/fake: 3.614/5.704 mean_real/fake: 0.050/0.040 var_real/fake: 0.064/0.003 \n",
      "[0/1][302/937] KL_real/fake: 3.357/5.372 mean_real/fake: 0.059/0.052 var_real/fake: 0.048/0.004 \n",
      "[0/1][303/937] KL_real/fake: 4.631/6.035 mean_real/fake: 0.041/0.036 var_real/fake: 0.011/0.002 \n",
      "[0/1][304/937] KL_real/fake: 3.490/5.385 mean_real/fake: 0.079/0.045 var_real/fake: 0.066/0.004 \n",
      "[0/1][305/937] KL_real/fake: 3.640/5.262 mean_real/fake: 0.049/0.048 var_real/fake: 0.038/0.006 \n",
      "[0/1][306/937] KL_real/fake: 4.031/5.746 mean_real/fake: 0.045/0.048 var_real/fake: 0.030/0.003 \n",
      "[0/1][307/937] KL_real/fake: 3.639/5.644 mean_real/fake: 0.026/0.036 var_real/fake: 0.036/0.003 \n",
      "[0/1][308/937] KL_real/fake: 4.792/6.549 mean_real/fake: 0.049/0.040 var_real/fake: 0.009/0.001 \n",
      "[0/1][309/937] KL_real/fake: 3.564/5.559 mean_real/fake: 0.027/0.045 var_real/fake: 0.035/0.003 \n",
      "[0/1][310/937] KL_real/fake: 3.244/4.946 mean_real/fake: 0.036/0.055 var_real/fake: 0.050/0.007 \n",
      "[0/1][311/937] KL_real/fake: 4.679/6.510 mean_real/fake: 0.046/0.041 var_real/fake: 0.010/0.001 \n",
      "[0/1][312/937] KL_real/fake: 4.035/5.839 mean_real/fake: 0.024/0.042 var_real/fake: 0.036/0.002 \n",
      "[0/1][313/937] KL_real/fake: 3.186/4.484 mean_real/fake: 0.049/0.036 var_real/fake: 0.076/0.013 \n",
      "[0/1][314/937] KL_real/fake: 4.677/6.099 mean_real/fake: 0.033/0.039 var_real/fake: 0.012/0.002 \n",
      "[0/1][315/937] KL_real/fake: 4.450/6.302 mean_real/fake: 0.016/0.036 var_real/fake: 0.023/0.001 \n",
      "[0/1][316/937] KL_real/fake: 3.943/5.814 mean_real/fake: 0.037/0.033 var_real/fake: 0.025/0.002 \n",
      "[0/1][317/937] KL_real/fake: 3.741/5.750 mean_real/fake: 0.041/0.036 var_real/fake: 0.039/0.003 \n",
      "[0/1][318/937] KL_real/fake: 3.851/5.787 mean_real/fake: 0.069/0.041 var_real/fake: 0.042/0.002 \n",
      "[0/1][319/937] KL_real/fake: 3.628/5.790 mean_real/fake: 0.100/0.048 var_real/fake: 0.063/0.002 \n",
      "[0/1][320/937] KL_real/fake: 3.058/4.700 mean_real/fake: 0.038/0.047 var_real/fake: 0.059/0.010 \n",
      "[0/1][321/937] KL_real/fake: 5.458/6.496 mean_real/fake: 0.024/0.038 var_real/fake: 0.004/0.001 \n",
      "[0/1][322/937] KL_real/fake: 5.137/6.487 mean_real/fake: 0.052/0.040 var_real/fake: 0.006/0.001 \n",
      "[0/1][323/937] KL_real/fake: 3.128/4.719 mean_real/fake: 0.051/0.051 var_real/fake: 0.092/0.016 \n",
      "[0/1][324/937] KL_real/fake: 3.835/4.627 mean_real/fake: 0.075/0.075 var_real/fake: 0.051/0.018 \n",
      "[0/1][325/937] KL_real/fake: 4.106/5.752 mean_real/fake: 0.060/0.045 var_real/fake: 0.028/0.003 \n",
      "[0/1][326/937] KL_real/fake: 4.295/5.866 mean_real/fake: 0.041/0.039 var_real/fake: 0.020/0.002 \n",
      "[0/1][327/937] KL_real/fake: 3.132/5.146 mean_real/fake: 0.075/0.054 var_real/fake: 0.081/0.006 \n",
      "[0/1][328/937] KL_real/fake: 3.990/5.665 mean_real/fake: 0.038/0.049 var_real/fake: 0.027/0.003 \n",
      "[0/1][329/937] KL_real/fake: 4.113/5.682 mean_real/fake: 0.060/0.039 var_real/fake: 0.022/0.003 \n",
      "[0/1][330/937] KL_real/fake: 3.192/4.125 mean_real/fake: 0.017/0.055 var_real/fake: 0.052/0.024 \n",
      "[0/1][331/937] KL_real/fake: 5.360/6.527 mean_real/fake: 0.046/0.042 var_real/fake: 0.004/0.001 \n",
      "[0/1][332/937] KL_real/fake: 5.184/6.827 mean_real/fake: 0.048/0.043 var_real/fake: 0.007/0.001 \n",
      "[0/1][333/937] KL_real/fake: 3.666/5.676 mean_real/fake: 0.064/0.049 var_real/fake: 0.057/0.003 \n",
      "[0/1][334/937] KL_real/fake: 3.466/5.158 mean_real/fake: 0.043/0.050 var_real/fake: 0.048/0.006 \n",
      "[0/1][335/937] KL_real/fake: 4.025/5.991 mean_real/fake: 0.063/0.043 var_real/fake: 0.038/0.002 \n",
      "[0/1][336/937] KL_real/fake: 4.291/6.195 mean_real/fake: 0.048/0.042 var_real/fake: 0.023/0.002 \n",
      "[0/1][337/937] KL_real/fake: 3.488/5.143 mean_real/fake: 0.041/0.038 var_real/fake: 0.051/0.006 \n",
      "[0/1][338/937] KL_real/fake: 3.703/5.531 mean_real/fake: 0.067/0.046 var_real/fake: 0.036/0.003 \n",
      "[0/1][339/937] KL_real/fake: 3.396/5.134 mean_real/fake: 0.081/0.050 var_real/fake: 0.056/0.008 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][340/937] KL_real/fake: 4.880/6.452 mean_real/fake: 0.046/0.041 var_real/fake: 0.007/0.001 \n",
      "[0/1][341/937] KL_real/fake: 4.082/5.949 mean_real/fake: 0.095/0.047 var_real/fake: 0.031/0.002 \n",
      "[0/1][342/937] KL_real/fake: 3.262/4.734 mean_real/fake: 0.038/0.044 var_real/fake: 0.053/0.009 \n",
      "[0/1][343/937] KL_real/fake: 4.410/5.982 mean_real/fake: 0.053/0.045 var_real/fake: 0.020/0.002 \n",
      "[0/1][344/937] KL_real/fake: 3.851/5.819 mean_real/fake: 0.045/0.048 var_real/fake: 0.031/0.003 \n",
      "[0/1][345/937] KL_real/fake: 3.137/5.237 mean_real/fake: 0.091/0.046 var_real/fake: 0.080/0.005 \n",
      "[0/1][346/937] KL_real/fake: 4.405/6.008 mean_real/fake: 0.003/0.040 var_real/fake: 0.027/0.002 \n",
      "[0/1][347/937] KL_real/fake: 4.144/5.873 mean_real/fake: 0.048/0.048 var_real/fake: 0.024/0.003 \n",
      "[0/1][348/937] KL_real/fake: 3.086/5.166 mean_real/fake: 0.062/0.058 var_real/fake: 0.082/0.006 \n",
      "[0/1][349/937] KL_real/fake: 4.516/5.946 mean_real/fake: 0.056/0.044 var_real/fake: 0.015/0.002 \n",
      "[0/1][350/937] KL_real/fake: 3.294/5.331 mean_real/fake: 0.059/0.044 var_real/fake: 0.049/0.005 \n",
      "[0/1][351/937] KL_real/fake: 4.094/6.123 mean_real/fake: 0.048/0.034 var_real/fake: 0.023/0.002 \n",
      "[0/1][352/937] KL_real/fake: 3.258/4.911 mean_real/fake: 0.044/0.044 var_real/fake: 0.064/0.009 \n",
      "[0/1][353/937] KL_real/fake: 4.934/6.252 mean_real/fake: 0.032/0.043 var_real/fake: 0.007/0.001 \n",
      "[0/1][354/937] KL_real/fake: 5.082/6.452 mean_real/fake: 0.041/0.047 var_real/fake: 0.006/0.001 \n",
      "[0/1][355/937] KL_real/fake: 3.176/5.292 mean_real/fake: 0.069/0.041 var_real/fake: 0.083/0.005 \n",
      "[0/1][356/937] KL_real/fake: 3.282/5.509 mean_real/fake: 0.056/0.048 var_real/fake: 0.111/0.003 \n",
      "[0/1][357/937] KL_real/fake: 3.736/5.635 mean_real/fake: 0.045/0.048 var_real/fake: 0.058/0.003 \n",
      "[0/1][358/937] KL_real/fake: 4.005/5.765 mean_real/fake: 0.059/0.045 var_real/fake: 0.036/0.003 \n",
      "[0/1][359/937] KL_real/fake: 3.071/4.077 mean_real/fake: 0.010/0.024 var_real/fake: 0.058/0.028 \n",
      "[0/1][360/937] KL_real/fake: 5.273/6.399 mean_real/fake: 0.038/0.045 var_real/fake: 0.005/0.001 \n",
      "[0/1][361/937] KL_real/fake: 4.917/6.459 mean_real/fake: 0.039/0.046 var_real/fake: 0.009/0.001 \n",
      "[0/1][362/937] KL_real/fake: 3.712/4.926 mean_real/fake: 0.075/0.060 var_real/fake: 0.051/0.009 \n",
      "[0/1][363/937] KL_real/fake: 3.100/3.985 mean_real/fake: 0.036/0.073 var_real/fake: 0.073/0.036 \n",
      "[0/1][364/937] KL_real/fake: 3.679/5.518 mean_real/fake: 0.054/0.050 var_real/fake: 0.039/0.003 \n",
      "[0/1][365/937] KL_real/fake: 5.494/6.821 mean_real/fake: 0.049/0.041 var_real/fake: 0.004/0.001 \n",
      "[0/1][366/937] KL_real/fake: 5.879/7.005 mean_real/fake: 0.046/0.043 var_real/fake: 0.002/0.001 \n",
      "[0/1][367/937] KL_real/fake: 5.547/6.454 mean_real/fake: 0.044/0.031 var_real/fake: 0.003/0.001 \n",
      "[0/1][368/937] KL_real/fake: 4.330/6.074 mean_real/fake: 0.057/0.049 var_real/fake: 0.017/0.002 \n",
      "[0/1][369/937] KL_real/fake: 3.486/5.000 mean_real/fake: 0.063/0.044 var_real/fake: 0.059/0.006 \n",
      "[0/1][370/937] KL_real/fake: 3.568/5.413 mean_real/fake: 0.059/0.055 var_real/fake: 0.063/0.004 \n",
      "[0/1][371/937] KL_real/fake: 4.090/5.554 mean_real/fake: 0.047/0.054 var_real/fake: 0.029/0.004 \n",
      "[0/1][372/937] KL_real/fake: 3.163/4.786 mean_real/fake: 0.046/0.058 var_real/fake: 0.062/0.011 \n",
      "[0/1][373/937] KL_real/fake: 4.650/5.944 mean_real/fake: 0.052/0.051 var_real/fake: 0.010/0.002 \n",
      "[0/1][374/937] KL_real/fake: 4.168/5.800 mean_real/fake: 0.057/0.051 var_real/fake: 0.021/0.003 \n",
      "[0/1][375/937] KL_real/fake: 3.097/4.192 mean_real/fake: 0.047/0.053 var_real/fake: 0.073/0.019 \n",
      "[0/1][376/937] KL_real/fake: 4.432/5.944 mean_real/fake: 0.070/0.038 var_real/fake: 0.018/0.002 \n",
      "[0/1][377/937] KL_real/fake: 4.274/5.997 mean_real/fake: 0.037/0.038 var_real/fake: 0.018/0.002 \n",
      "[0/1][378/937] KL_real/fake: 3.458/4.900 mean_real/fake: 0.084/0.061 var_real/fake: 0.069/0.008 \n",
      "[0/1][379/937] KL_real/fake: 4.345/5.903 mean_real/fake: 0.053/0.048 var_real/fake: 0.018/0.002 \n",
      "[0/1][380/937] KL_real/fake: 4.307/5.884 mean_real/fake: 0.041/0.043 var_real/fake: 0.015/0.002 \n",
      "[0/1][381/937] KL_real/fake: 3.160/3.748 mean_real/fake: 0.035/0.056 var_real/fake: 0.055/0.042 \n",
      "[0/1][382/937] KL_real/fake: 4.730/6.584 mean_real/fake: 0.056/0.043 var_real/fake: 0.012/0.001 \n",
      "[0/1][383/937] KL_real/fake: 5.199/6.735 mean_real/fake: 0.040/0.042 var_real/fake: 0.006/0.001 \n",
      "[0/1][384/937] KL_real/fake: 4.083/5.842 mean_real/fake: 0.055/0.046 var_real/fake: 0.023/0.003 \n",
      "[0/1][385/937] KL_real/fake: 3.042/4.915 mean_real/fake: 0.069/0.043 var_real/fake: 0.079/0.008 \n",
      "[0/1][386/937] KL_real/fake: 3.891/5.942 mean_real/fake: 0.034/0.038 var_real/fake: 0.033/0.002 \n",
      "[0/1][387/937] KL_real/fake: 3.686/5.698 mean_real/fake: 0.072/0.050 var_real/fake: 0.042/0.003 \n",
      "[0/1][388/937] KL_real/fake: 3.393/5.089 mean_real/fake: 0.054/0.050 var_real/fake: 0.051/0.006 \n",
      "[0/1][389/937] KL_real/fake: 3.693/5.651 mean_real/fake: 0.049/0.052 var_real/fake: 0.037/0.003 \n",
      "[0/1][390/937] KL_real/fake: 3.826/5.138 mean_real/fake: 0.060/0.034 var_real/fake: 0.036/0.006 \n",
      "[0/1][391/937] KL_real/fake: 3.856/5.512 mean_real/fake: 0.038/0.043 var_real/fake: 0.037/0.004 \n",
      "[0/1][392/937] KL_real/fake: 3.269/5.278 mean_real/fake: 0.049/0.045 var_real/fake: 0.059/0.004 \n",
      "[0/1][393/937] KL_real/fake: 3.933/5.291 mean_real/fake: 0.056/0.050 var_real/fake: 0.030/0.005 \n",
      "[0/1][394/937] KL_real/fake: 3.067/4.718 mean_real/fake: 0.048/0.054 var_real/fake: 0.075/0.011 \n",
      "[0/1][395/937] KL_real/fake: 4.884/6.354 mean_real/fake: 0.056/0.037 var_real/fake: 0.007/0.001 \n",
      "[0/1][396/937] KL_real/fake: 3.924/5.645 mean_real/fake: 0.045/0.046 var_real/fake: 0.038/0.003 \n",
      "[0/1][397/937] KL_real/fake: 3.321/4.722 mean_real/fake: 0.043/0.059 var_real/fake: 0.079/0.014 \n",
      "[0/1][398/937] KL_real/fake: 4.131/5.859 mean_real/fake: 0.066/0.041 var_real/fake: 0.026/0.002 \n",
      "[0/1][399/937] KL_real/fake: 4.639/6.125 mean_real/fake: 0.038/0.046 var_real/fake: 0.011/0.002 \n",
      "[0/1][400/937] KL_real/fake: 3.468/5.643 mean_real/fake: 0.072/0.042 var_real/fake: 0.061/0.003 \n",
      "[0/1][401/937] KL_real/fake: 3.730/5.821 mean_real/fake: 0.088/0.048 var_real/fake: 0.041/0.002 \n",
      "[0/1][402/937] KL_real/fake: 3.124/5.175 mean_real/fake: 0.039/0.042 var_real/fake: 0.066/0.006 \n",
      "[0/1][403/937] KL_real/fake: 4.292/6.127 mean_real/fake: 0.064/0.042 var_real/fake: 0.025/0.002 \n",
      "[0/1][404/937] KL_real/fake: 3.869/5.738 mean_real/fake: 0.064/0.035 var_real/fake: 0.033/0.003 \n",
      "[0/1][405/937] KL_real/fake: 2.949/4.095 mean_real/fake: 0.019/0.081 var_real/fake: 0.065/0.024 \n",
      "[0/1][406/937] KL_real/fake: 4.648/6.284 mean_real/fake: 0.039/0.043 var_real/fake: 0.011/0.001 \n",
      "[0/1][407/937] KL_real/fake: 3.587/5.310 mean_real/fake: 0.078/0.042 var_real/fake: 0.060/0.005 \n",
      "[0/1][408/937] KL_real/fake: 4.269/5.950 mean_real/fake: 0.078/0.040 var_real/fake: 0.033/0.002 \n",
      "[0/1][409/937] KL_real/fake: 3.626/5.564 mean_real/fake: 0.073/0.037 var_real/fake: 0.068/0.003 \n",
      "[0/1][410/937] KL_real/fake: 3.256/5.088 mean_real/fake: 0.068/0.055 var_real/fake: 0.074/0.007 \n",
      "[0/1][411/937] KL_real/fake: 4.680/5.899 mean_real/fake: 0.039/0.049 var_real/fake: 0.011/0.003 \n",
      "[0/1][412/937] KL_real/fake: 4.215/6.100 mean_real/fake: 0.075/0.041 var_real/fake: 0.022/0.002 \n",
      "[0/1][413/937] KL_real/fake: 3.289/4.617 mean_real/fake: 0.050/0.026 var_real/fake: 0.051/0.011 \n",
      "[0/1][414/937] KL_real/fake: 5.051/6.432 mean_real/fake: 0.039/0.039 var_real/fake: 0.006/0.001 \n",
      "[0/1][415/937] KL_real/fake: 5.079/6.296 mean_real/fake: 0.037/0.039 var_real/fake: 0.006/0.001 \n",
      "[0/1][416/937] KL_real/fake: 3.068/4.878 mean_real/fake: 0.065/0.059 var_real/fake: 0.084/0.009 \n",
      "[0/1][417/937] KL_real/fake: 4.149/5.723 mean_real/fake: 0.041/0.033 var_real/fake: 0.021/0.003 \n",
      "[0/1][418/937] KL_real/fake: 3.977/6.084 mean_real/fake: 0.045/0.034 var_real/fake: 0.044/0.002 \n",
      "[0/1][419/937] KL_real/fake: 3.417/5.161 mean_real/fake: 0.068/0.043 var_real/fake: 0.069/0.011 \n",
      "[0/1][420/937] KL_real/fake: 3.142/4.886 mean_real/fake: 0.063/0.044 var_real/fake: 0.077/0.008 \n",
      "[0/1][421/937] KL_real/fake: 4.856/6.300 mean_real/fake: 0.032/0.042 var_real/fake: 0.008/0.001 \n",
      "[0/1][422/937] KL_real/fake: 4.076/5.852 mean_real/fake: 0.048/0.035 var_real/fake: 0.020/0.003 \n",
      "[0/1][423/937] KL_real/fake: 3.054/4.217 mean_real/fake: 0.030/0.044 var_real/fake: 0.062/0.029 \n",
      "[0/1][424/937] KL_real/fake: 5.766/6.825 mean_real/fake: 0.038/0.043 var_real/fake: 0.002/0.001 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][425/937] KL_real/fake: 5.874/6.979 mean_real/fake: 0.038/0.040 var_real/fake: 0.002/0.001 \n",
      "[0/1][426/937] KL_real/fake: 5.384/6.377 mean_real/fake: 0.041/0.041 var_real/fake: 0.004/0.001 \n",
      "[0/1][427/937] KL_real/fake: 4.242/5.308 mean_real/fake: 0.089/0.052 var_real/fake: 0.034/0.006 \n",
      "[0/1][428/937] KL_real/fake: 3.894/4.914 mean_real/fake: 0.065/0.049 var_real/fake: 0.035/0.017 \n",
      "[0/1][429/937] KL_real/fake: 3.637/4.517 mean_real/fake: 0.056/0.072 var_real/fake: 0.073/0.024 \n",
      "[0/1][430/937] KL_real/fake: 4.418/6.220 mean_real/fake: 0.054/0.043 var_real/fake: 0.016/0.002 \n",
      "[0/1][431/937] KL_real/fake: 4.507/5.873 mean_real/fake: 0.054/0.048 var_real/fake: 0.014/0.002 \n",
      "[0/1][432/937] KL_real/fake: 3.685/5.153 mean_real/fake: 0.057/0.051 var_real/fake: 0.037/0.006 \n",
      "[0/1][433/937] KL_real/fake: 3.044/3.976 mean_real/fake: 0.038/0.070 var_real/fake: 0.068/0.034 \n",
      "[0/1][434/937] KL_real/fake: 4.306/5.881 mean_real/fake: 0.044/0.045 var_real/fake: 0.018/0.003 \n",
      "[0/1][435/937] KL_real/fake: 4.796/5.835 mean_real/fake: 0.051/0.045 var_real/fake: 0.010/0.003 \n",
      "[0/1][436/937] KL_real/fake: 4.519/5.625 mean_real/fake: 0.028/0.041 var_real/fake: 0.011/0.004 \n",
      "[0/1][437/937] KL_real/fake: 3.170/4.490 mean_real/fake: 0.043/0.072 var_real/fake: 0.059/0.017 \n",
      "[0/1][438/937] KL_real/fake: 4.371/6.029 mean_real/fake: 0.035/0.049 var_real/fake: 0.021/0.002 \n",
      "[0/1][439/937] KL_real/fake: 3.643/5.055 mean_real/fake: 0.067/0.089 var_real/fake: 0.049/0.014 \n",
      "[0/1][440/937] KL_real/fake: 4.133/5.784 mean_real/fake: 0.058/0.046 var_real/fake: 0.032/0.003 \n",
      "[0/1][441/937] KL_real/fake: 3.449/4.945 mean_real/fake: 0.063/0.035 var_real/fake: 0.055/0.009 \n",
      "[0/1][442/937] KL_real/fake: 3.888/5.773 mean_real/fake: 0.052/0.038 var_real/fake: 0.033/0.003 \n",
      "[0/1][443/937] KL_real/fake: 2.984/4.536 mean_real/fake: 0.042/0.054 var_real/fake: 0.078/0.014 \n",
      "[0/1][444/937] KL_real/fake: 5.238/6.334 mean_real/fake: 0.042/0.042 var_real/fake: 0.004/0.001 \n",
      "[0/1][445/937] KL_real/fake: 4.387/6.095 mean_real/fake: 0.040/0.040 var_real/fake: 0.014/0.002 \n",
      "[0/1][446/937] KL_real/fake: 3.493/4.980 mean_real/fake: 0.061/0.047 var_real/fake: 0.043/0.007 \n",
      "[0/1][447/937] KL_real/fake: 4.466/5.819 mean_real/fake: 0.034/0.045 var_real/fake: 0.014/0.002 \n",
      "[0/1][448/937] KL_real/fake: 3.614/5.267 mean_real/fake: 0.074/0.036 var_real/fake: 0.047/0.005 \n",
      "[0/1][449/937] KL_real/fake: 3.050/4.980 mean_real/fake: 0.071/0.048 var_real/fake: 0.092/0.007 \n",
      "[0/1][450/937] KL_real/fake: 5.369/6.578 mean_real/fake: 0.043/0.048 var_real/fake: 0.004/0.001 \n",
      "[0/1][451/937] KL_real/fake: 5.216/6.377 mean_real/fake: 0.047/0.047 var_real/fake: 0.005/0.001 \n",
      "[0/1][452/937] KL_real/fake: 3.783/4.952 mean_real/fake: 0.061/0.060 var_real/fake: 0.044/0.008 \n",
      "[0/1][453/937] KL_real/fake: 3.247/4.454 mean_real/fake: 0.077/0.074 var_real/fake: 0.075/0.023 \n",
      "[0/1][454/937] KL_real/fake: 3.764/5.525 mean_real/fake: 0.104/0.041 var_real/fake: 0.061/0.004 \n",
      "[0/1][455/937] KL_real/fake: 4.438/5.709 mean_real/fake: 0.048/0.042 var_real/fake: 0.015/0.003 \n",
      "[0/1][456/937] KL_real/fake: 3.012/5.194 mean_real/fake: 0.104/0.048 var_real/fake: 0.082/0.006 \n",
      "[0/1][457/937] KL_real/fake: 3.048/3.828 mean_real/fake: 0.033/0.051 var_real/fake: 0.088/0.038 \n",
      "[0/1][458/937] KL_real/fake: 4.148/5.900 mean_real/fake: 0.073/0.047 var_real/fake: 0.024/0.002 \n",
      "[0/1][459/937] KL_real/fake: 3.737/5.074 mean_real/fake: 0.058/0.062 var_real/fake: 0.050/0.008 \n",
      "[0/1][460/937] KL_real/fake: 4.364/5.497 mean_real/fake: 0.044/0.058 var_real/fake: 0.022/0.004 \n",
      "[0/1][461/937] KL_real/fake: 3.827/5.349 mean_real/fake: 0.062/0.042 var_real/fake: 0.039/0.004 \n",
      "[0/1][462/937] KL_real/fake: 4.191/5.705 mean_real/fake: 0.060/0.055 var_real/fake: 0.023/0.003 \n",
      "[0/1][463/937] KL_real/fake: 3.670/5.701 mean_real/fake: 0.039/0.050 var_real/fake: 0.057/0.003 \n",
      "[0/1][464/937] KL_real/fake: 3.034/4.476 mean_real/fake: 0.032/0.063 var_real/fake: 0.067/0.017 \n",
      "[0/1][465/937] KL_real/fake: 5.583/6.186 mean_real/fake: 0.035/0.041 var_real/fake: 0.003/0.002 \n",
      "[0/1][466/937] KL_real/fake: 5.617/6.009 mean_real/fake: 0.044/0.039 var_real/fake: 0.003/0.002 \n",
      "[0/1][467/937] KL_real/fake: 3.626/5.522 mean_real/fake: 0.099/0.035 var_real/fake: 0.059/0.003 \n",
      "[0/1][468/937] KL_real/fake: 2.904/4.088 mean_real/fake: 0.042/0.028 var_real/fake: 0.090/0.027 \n",
      "[0/1][469/937] KL_real/fake: 4.266/5.500 mean_real/fake: 0.058/0.056 var_real/fake: 0.017/0.004 \n",
      "[0/1][470/937] KL_real/fake: 4.598/5.887 mean_real/fake: 0.063/0.047 var_real/fake: 0.014/0.002 \n",
      "[0/1][471/937] KL_real/fake: 3.967/5.630 mean_real/fake: 0.082/0.046 var_real/fake: 0.041/0.003 \n",
      "[0/1][472/937] KL_real/fake: 3.498/5.273 mean_real/fake: 0.066/0.060 var_real/fake: 0.054/0.007 \n",
      "[0/1][473/937] KL_real/fake: 3.193/5.085 mean_real/fake: 0.087/0.033 var_real/fake: 0.094/0.008 \n",
      "[0/1][474/937] KL_real/fake: 4.030/5.862 mean_real/fake: 0.057/0.039 var_real/fake: 0.035/0.002 \n",
      "[0/1][475/937] KL_real/fake: 3.600/5.530 mean_real/fake: 0.049/0.046 var_real/fake: 0.051/0.004 \n",
      "[0/1][476/937] KL_real/fake: 3.564/5.137 mean_real/fake: 0.018/0.065 var_real/fake: 0.054/0.007 \n",
      "[0/1][477/937] KL_real/fake: 3.641/5.794 mean_real/fake: 0.044/0.045 var_real/fake: 0.034/0.003 \n",
      "[0/1][478/937] KL_real/fake: 3.104/4.509 mean_real/fake: 0.041/0.045 var_real/fake: 0.060/0.014 \n",
      "[0/1][479/937] KL_real/fake: 5.642/6.494 mean_real/fake: 0.039/0.042 var_real/fake: 0.003/0.001 \n",
      "[0/1][480/937] KL_real/fake: 5.010/6.563 mean_real/fake: 0.040/0.048 var_real/fake: 0.007/0.001 \n",
      "[0/1][481/937] KL_real/fake: 3.634/4.904 mean_real/fake: 0.104/0.071 var_real/fake: 0.054/0.012 \n",
      "[0/1][482/937] KL_real/fake: 3.192/4.968 mean_real/fake: 0.085/0.044 var_real/fake: 0.110/0.010 \n",
      "[0/1][483/937] KL_real/fake: 4.506/5.881 mean_real/fake: 0.060/0.048 var_real/fake: 0.018/0.003 \n",
      "[0/1][484/937] KL_real/fake: 4.013/5.446 mean_real/fake: 0.048/0.037 var_real/fake: 0.040/0.004 \n",
      "[0/1][485/937] KL_real/fake: 4.120/5.422 mean_real/fake: 0.040/0.037 var_real/fake: 0.043/0.004 \n",
      "[0/1][486/937] KL_real/fake: 3.688/4.832 mean_real/fake: 0.066/0.053 var_real/fake: 0.051/0.010 \n",
      "[0/1][487/937] KL_real/fake: 3.659/5.609 mean_real/fake: 0.058/0.043 var_real/fake: 0.052/0.003 \n",
      "[0/1][488/937] KL_real/fake: 3.068/5.019 mean_real/fake: 0.051/0.062 var_real/fake: 0.084/0.008 \n",
      "[0/1][489/937] KL_real/fake: 5.007/6.278 mean_real/fake: 0.049/0.041 var_real/fake: 0.006/0.001 \n",
      "[0/1][490/937] KL_real/fake: 4.906/5.913 mean_real/fake: 0.038/0.044 var_real/fake: 0.007/0.003 \n",
      "[0/1][491/937] KL_real/fake: 3.260/5.153 mean_real/fake: 0.044/0.038 var_real/fake: 0.075/0.006 \n",
      "[0/1][492/937] KL_real/fake: 3.483/5.144 mean_real/fake: 0.080/0.045 var_real/fake: 0.051/0.006 \n",
      "[0/1][493/937] KL_real/fake: 4.136/5.224 mean_real/fake: 0.051/0.052 var_real/fake: 0.017/0.005 \n",
      "[0/1][494/937] KL_real/fake: 3.209/4.866 mean_real/fake: 0.059/0.065 var_real/fake: 0.072/0.011 \n",
      "[0/1][495/937] KL_real/fake: 3.755/5.358 mean_real/fake: 0.053/0.048 var_real/fake: 0.055/0.005 \n",
      "[0/1][496/937] KL_real/fake: 4.400/5.738 mean_real/fake: 0.081/0.046 var_real/fake: 0.022/0.003 \n",
      "[0/1][497/937] KL_real/fake: 3.570/5.200 mean_real/fake: 0.074/0.052 var_real/fake: 0.051/0.006 \n",
      "[0/1][498/937] KL_real/fake: 3.131/4.947 mean_real/fake: 0.043/0.069 var_real/fake: 0.076/0.013 \n",
      "[0/1][499/937] KL_real/fake: 5.089/6.359 mean_real/fake: 0.045/0.046 var_real/fake: 0.006/0.001 \n",
      "[0/1][500/937] KL_real/fake: 4.185/6.055 mean_real/fake: 0.034/0.034 var_real/fake: 0.018/0.002 \n",
      "[0/1][501/937] KL_real/fake: 3.107/4.191 mean_real/fake: 0.060/0.078 var_real/fake: 0.080/0.028 \n",
      "[0/1][502/937] KL_real/fake: 5.157/6.427 mean_real/fake: 0.047/0.037 var_real/fake: 0.006/0.001 \n",
      "[0/1][503/937] KL_real/fake: 5.595/6.580 mean_real/fake: 0.043/0.046 var_real/fake: 0.003/0.001 \n",
      "[0/1][504/937] KL_real/fake: 4.281/6.247 mean_real/fake: 0.052/0.047 var_real/fake: 0.022/0.002 \n",
      "[0/1][505/937] KL_real/fake: 4.077/5.629 mean_real/fake: 0.049/0.046 var_real/fake: 0.025/0.003 \n",
      "[0/1][506/937] KL_real/fake: 3.523/4.977 mean_real/fake: 0.051/0.045 var_real/fake: 0.060/0.008 \n",
      "[0/1][507/937] KL_real/fake: 4.286/6.160 mean_real/fake: 0.074/0.045 var_real/fake: 0.031/0.002 \n",
      "[0/1][508/937] KL_real/fake: 3.475/4.682 mean_real/fake: 0.051/0.048 var_real/fake: 0.062/0.011 \n",
      "[0/1][509/937] KL_real/fake: 3.623/5.062 mean_real/fake: 0.070/0.047 var_real/fake: 0.084/0.008 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][510/937] KL_real/fake: 3.714/5.519 mean_real/fake: 0.050/0.055 var_real/fake: 0.035/0.004 \n",
      "[0/1][511/937] KL_real/fake: 3.536/5.104 mean_real/fake: 0.057/0.039 var_real/fake: 0.046/0.006 \n",
      "[0/1][512/937] KL_real/fake: 3.560/5.134 mean_real/fake: 0.052/0.034 var_real/fake: 0.055/0.005 \n",
      "[0/1][513/937] KL_real/fake: 3.515/5.209 mean_real/fake: 0.080/0.040 var_real/fake: 0.064/0.005 \n",
      "[0/1][514/937] KL_real/fake: 4.212/5.894 mean_real/fake: 0.070/0.050 var_real/fake: 0.022/0.002 \n",
      "[0/1][515/937] KL_real/fake: 3.334/5.001 mean_real/fake: 0.031/0.034 var_real/fake: 0.044/0.007 \n",
      "[0/1][516/937] KL_real/fake: 4.308/5.715 mean_real/fake: 0.049/0.049 var_real/fake: 0.016/0.003 \n",
      "[0/1][517/937] KL_real/fake: 4.069/5.733 mean_real/fake: 0.084/0.043 var_real/fake: 0.033/0.003 \n",
      "[0/1][518/937] KL_real/fake: 4.000/5.221 mean_real/fake: 0.048/0.046 var_real/fake: 0.034/0.005 \n",
      "[0/1][519/937] KL_real/fake: 3.523/5.438 mean_real/fake: 0.072/0.032 var_real/fake: 0.053/0.004 \n",
      "[0/1][520/937] KL_real/fake: 5.211/6.311 mean_real/fake: 0.050/0.043 var_real/fake: 0.005/0.001 \n",
      "[0/1][521/937] KL_real/fake: 3.602/5.491 mean_real/fake: 0.067/0.045 var_real/fake: 0.050/0.004 \n",
      "[0/1][522/937] KL_real/fake: 2.970/4.279 mean_real/fake: 0.040/0.046 var_real/fake: 0.064/0.025 \n",
      "[0/1][523/937] KL_real/fake: 5.297/6.581 mean_real/fake: 0.043/0.042 var_real/fake: 0.004/0.001 \n",
      "[0/1][524/937] KL_real/fake: 5.687/6.606 mean_real/fake: 0.048/0.037 var_real/fake: 0.003/0.001 \n",
      "[0/1][525/937] KL_real/fake: 5.067/6.806 mean_real/fake: 0.065/0.041 var_real/fake: 0.007/0.001 \n",
      "[0/1][526/937] KL_real/fake: 3.865/5.366 mean_real/fake: 0.052/0.046 var_real/fake: 0.041/0.005 \n",
      "[0/1][527/937] KL_real/fake: 3.170/5.054 mean_real/fake: 0.103/0.044 var_real/fake: 0.091/0.006 \n",
      "[0/1][528/937] KL_real/fake: 3.880/5.607 mean_real/fake: 0.035/0.042 var_real/fake: 0.034/0.003 \n",
      "[0/1][529/937] KL_real/fake: 4.138/5.731 mean_real/fake: 0.086/0.042 var_real/fake: 0.031/0.003 \n",
      "[0/1][530/937] KL_real/fake: 4.116/5.454 mean_real/fake: 0.094/0.036 var_real/fake: 0.031/0.003 \n",
      "[0/1][531/937] KL_real/fake: 3.241/5.022 mean_real/fake: 0.051/0.054 var_real/fake: 0.068/0.008 \n",
      "[0/1][532/937] KL_real/fake: 4.088/5.974 mean_real/fake: 0.055/0.031 var_real/fake: 0.027/0.002 \n",
      "[0/1][533/937] KL_real/fake: 2.964/4.304 mean_real/fake: 0.032/0.048 var_real/fake: 0.064/0.020 \n",
      "[0/1][534/937] KL_real/fake: 5.637/6.225 mean_real/fake: 0.037/0.041 var_real/fake: 0.003/0.001 \n",
      "[0/1][535/937] KL_real/fake: 5.205/6.157 mean_real/fake: 0.055/0.045 var_real/fake: 0.006/0.002 \n",
      "[0/1][536/937] KL_real/fake: 3.510/4.834 mean_real/fake: 0.066/0.041 var_real/fake: 0.065/0.009 \n",
      "[0/1][537/937] KL_real/fake: 3.152/4.191 mean_real/fake: 0.037/0.064 var_real/fake: 0.073/0.026 \n",
      "[0/1][538/937] KL_real/fake: 5.349/6.451 mean_real/fake: 0.044/0.047 var_real/fake: 0.004/0.001 \n",
      "[0/1][539/937] KL_real/fake: 5.702/7.094 mean_real/fake: 0.048/0.039 var_real/fake: 0.003/0.001 \n",
      "[0/1][540/937] KL_real/fake: 5.604/6.551 mean_real/fake: 0.038/0.044 var_real/fake: 0.003/0.001 \n",
      "[0/1][541/937] KL_real/fake: 5.005/6.237 mean_real/fake: 0.054/0.044 var_real/fake: 0.009/0.002 \n",
      "[0/1][542/937] KL_real/fake: 4.254/5.454 mean_real/fake: 0.079/0.054 var_real/fake: 0.029/0.005 \n",
      "[0/1][543/937] KL_real/fake: 3.157/4.570 mean_real/fake: 0.071/0.050 var_real/fake: 0.137/0.014 \n",
      "[0/1][544/937] KL_real/fake: 4.252/5.563 mean_real/fake: 0.066/0.041 var_real/fake: 0.025/0.003 \n",
      "[0/1][545/937] KL_real/fake: 4.122/5.494 mean_real/fake: 0.061/0.048 var_real/fake: 0.023/0.004 \n",
      "[0/1][546/937] KL_real/fake: 3.993/5.485 mean_real/fake: 0.047/0.037 var_real/fake: 0.027/0.004 \n",
      "[0/1][547/937] KL_real/fake: 3.550/5.118 mean_real/fake: 0.073/0.043 var_real/fake: 0.044/0.007 \n",
      "[0/1][548/937] KL_real/fake: 2.933/4.258 mean_real/fake: 0.025/0.059 var_real/fake: 0.082/0.021 \n",
      "[0/1][549/937] KL_real/fake: 6.204/6.422 mean_real/fake: 0.044/0.038 var_real/fake: 0.002/0.001 \n",
      "[0/1][550/937] KL_real/fake: 6.617/6.631 mean_real/fake: 0.041/0.049 var_real/fake: 0.001/0.001 \n",
      "[0/1][551/937] KL_real/fake: 5.967/7.028 mean_real/fake: 0.037/0.041 var_real/fake: 0.002/0.001 \n",
      "[0/1][552/937] KL_real/fake: 4.811/6.063 mean_real/fake: 0.069/0.045 var_real/fake: 0.015/0.002 \n",
      "[0/1][553/937] KL_real/fake: 4.235/4.561 mean_real/fake: 0.075/0.049 var_real/fake: 0.035/0.011 \n",
      "[0/1][554/937] KL_real/fake: 3.551/4.649 mean_real/fake: 0.070/0.049 var_real/fake: 0.063/0.016 \n",
      "[0/1][555/937] KL_real/fake: 3.930/5.137 mean_real/fake: 0.061/0.056 var_real/fake: 0.040/0.006 \n",
      "[0/1][556/937] KL_real/fake: 3.650/4.770 mean_real/fake: 0.090/0.054 var_real/fake: 0.081/0.010 \n",
      "[0/1][557/937] KL_real/fake: 4.039/5.173 mean_real/fake: 0.059/0.042 var_real/fake: 0.037/0.006 \n",
      "[0/1][558/937] KL_real/fake: 3.536/5.238 mean_real/fake: 0.085/0.041 var_real/fake: 0.080/0.006 \n",
      "[0/1][559/937] KL_real/fake: 3.919/5.229 mean_real/fake: 0.040/0.052 var_real/fake: 0.038/0.007 \n",
      "[0/1][560/937] KL_real/fake: 3.744/5.091 mean_real/fake: 0.074/0.045 var_real/fake: 0.039/0.006 \n",
      "[0/1][561/937] KL_real/fake: 3.642/5.050 mean_real/fake: 0.058/0.047 var_real/fake: 0.055/0.008 \n",
      "[0/1][562/937] KL_real/fake: 4.613/6.010 mean_real/fake: 0.048/0.044 var_real/fake: 0.013/0.002 \n",
      "[0/1][563/937] KL_real/fake: 3.406/5.100 mean_real/fake: 0.041/0.045 var_real/fake: 0.061/0.006 \n",
      "[0/1][564/937] KL_real/fake: 2.950/3.576 mean_real/fake: 0.028/0.057 var_real/fake: 0.075/0.055 \n",
      "[0/1][565/937] KL_real/fake: 4.391/5.597 mean_real/fake: 0.048/0.040 var_real/fake: 0.020/0.004 \n",
      "[0/1][566/937] KL_real/fake: 4.392/5.486 mean_real/fake: 0.053/0.047 var_real/fake: 0.017/0.005 \n",
      "[0/1][567/937] KL_real/fake: 3.320/4.773 mean_real/fake: 0.055/0.052 var_real/fake: 0.058/0.009 \n",
      "[0/1][568/937] KL_real/fake: 3.708/5.354 mean_real/fake: 0.064/0.044 var_real/fake: 0.032/0.005 \n",
      "[0/1][569/937] KL_real/fake: 3.314/4.911 mean_real/fake: 0.049/0.052 var_real/fake: 0.076/0.008 \n",
      "[0/1][570/937] KL_real/fake: 5.299/6.170 mean_real/fake: 0.037/0.041 var_real/fake: 0.004/0.002 \n",
      "[0/1][571/937] KL_real/fake: 4.869/6.215 mean_real/fake: 0.052/0.040 var_real/fake: 0.011/0.002 \n",
      "[0/1][572/937] KL_real/fake: 3.118/4.146 mean_real/fake: 0.046/0.068 var_real/fake: 0.088/0.029 \n",
      "[0/1][573/937] KL_real/fake: 3.819/5.216 mean_real/fake: 0.063/0.049 var_real/fake: 0.036/0.005 \n",
      "[0/1][574/937] KL_real/fake: 4.672/6.157 mean_real/fake: 0.069/0.046 var_real/fake: 0.014/0.002 \n",
      "[0/1][575/937] KL_real/fake: 4.453/5.950 mean_real/fake: 0.063/0.046 var_real/fake: 0.027/0.002 \n",
      "[0/1][576/937] KL_real/fake: 3.767/5.003 mean_real/fake: 0.048/0.042 var_real/fake: 0.043/0.007 \n",
      "[0/1][577/937] KL_real/fake: 3.365/4.779 mean_real/fake: 0.077/0.057 var_real/fake: 0.080/0.010 \n",
      "[0/1][578/937] KL_real/fake: 5.183/6.270 mean_real/fake: 0.046/0.040 var_real/fake: 0.005/0.001 \n",
      "[0/1][579/937] KL_real/fake: 4.507/5.615 mean_real/fake: 0.037/0.044 var_real/fake: 0.011/0.004 \n",
      "[0/1][580/937] KL_real/fake: 3.026/4.322 mean_real/fake: 0.048/0.048 var_real/fake: 0.066/0.020 \n",
      "[0/1][581/937] KL_real/fake: 4.253/6.074 mean_real/fake: 0.057/0.037 var_real/fake: 0.021/0.002 \n",
      "[0/1][582/937] KL_real/fake: 4.075/5.658 mean_real/fake: 0.048/0.042 var_real/fake: 0.024/0.003 \n",
      "[0/1][583/937] KL_real/fake: 3.380/4.671 mean_real/fake: 0.055/0.061 var_real/fake: 0.075/0.014 \n",
      "[0/1][584/937] KL_real/fake: 4.744/5.900 mean_real/fake: 0.061/0.041 var_real/fake: 0.015/0.003 \n",
      "[0/1][585/937] KL_real/fake: 3.867/5.076 mean_real/fake: 0.046/0.056 var_real/fake: 0.031/0.010 \n",
      "[0/1][586/937] KL_real/fake: 3.030/4.069 mean_real/fake: 0.036/0.045 var_real/fake: 0.062/0.031 \n",
      "[0/1][587/937] KL_real/fake: 4.578/5.778 mean_real/fake: 0.062/0.039 var_real/fake: 0.011/0.003 \n",
      "[0/1][588/937] KL_real/fake: 5.111/6.209 mean_real/fake: 0.039/0.039 var_real/fake: 0.005/0.002 \n",
      "[0/1][589/937] KL_real/fake: 3.730/5.018 mean_real/fake: 0.066/0.041 var_real/fake: 0.050/0.008 \n",
      "[0/1][590/937] KL_real/fake: 3.537/5.386 mean_real/fake: 0.026/0.030 var_real/fake: 0.046/0.004 \n",
      "[0/1][591/937] KL_real/fake: 3.067/4.861 mean_real/fake: 0.056/0.050 var_real/fake: 0.075/0.012 \n",
      "[0/1][592/937] KL_real/fake: 5.851/6.464 mean_real/fake: 0.043/0.038 var_real/fake: 0.002/0.001 \n",
      "[0/1][593/937] KL_real/fake: 5.644/6.753 mean_real/fake: 0.031/0.038 var_real/fake: 0.003/0.001 \n",
      "[0/1][594/937] KL_real/fake: 4.708/5.960 mean_real/fake: 0.033/0.041 var_real/fake: 0.010/0.002 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][595/937] KL_real/fake: 3.781/5.096 mean_real/fake: 0.072/0.046 var_real/fake: 0.064/0.007 \n",
      "[0/1][596/937] KL_real/fake: 3.138/4.090 mean_real/fake: 0.052/0.086 var_real/fake: 0.081/0.037 \n",
      "[0/1][597/937] KL_real/fake: 4.902/6.181 mean_real/fake: 0.036/0.036 var_real/fake: 0.007/0.002 \n",
      "[0/1][598/937] KL_real/fake: 5.594/6.540 mean_real/fake: 0.043/0.046 var_real/fake: 0.003/0.001 \n",
      "[0/1][599/937] KL_real/fake: 5.354/6.496 mean_real/fake: 0.050/0.044 var_real/fake: 0.004/0.001 \n",
      "[0/1][600/937] KL_real/fake: 4.574/5.823 mean_real/fake: 0.044/0.044 var_real/fake: 0.011/0.003 \n",
      "[0/1][601/937] KL_real/fake: 3.620/4.866 mean_real/fake: 0.051/0.056 var_real/fake: 0.052/0.014 \n",
      "[0/1][602/937] KL_real/fake: 3.475/5.245 mean_real/fake: 0.055/0.053 var_real/fake: 0.041/0.005 \n",
      "[0/1][603/937] KL_real/fake: 3.324/4.925 mean_real/fake: 0.061/0.042 var_real/fake: 0.061/0.008 \n",
      "[0/1][604/937] KL_real/fake: 4.299/5.411 mean_real/fake: 0.049/0.048 var_real/fake: 0.022/0.005 \n",
      "[0/1][605/937] KL_real/fake: 3.571/5.107 mean_real/fake: 0.054/0.046 var_real/fake: 0.043/0.007 \n",
      "[0/1][606/937] KL_real/fake: 3.169/4.028 mean_real/fake: 0.038/0.063 var_real/fake: 0.067/0.026 \n",
      "[0/1][607/937] KL_real/fake: 5.023/6.453 mean_real/fake: 0.036/0.044 var_real/fake: 0.006/0.001 \n",
      "[0/1][608/937] KL_real/fake: 5.095/6.468 mean_real/fake: 0.071/0.043 var_real/fake: 0.008/0.001 \n",
      "[0/1][609/937] KL_real/fake: 3.886/5.622 mean_real/fake: 0.079/0.037 var_real/fake: 0.032/0.003 \n",
      "[0/1][610/937] KL_real/fake: 3.587/5.024 mean_real/fake: 0.057/0.038 var_real/fake: 0.052/0.009 \n",
      "[0/1][611/937] KL_real/fake: 2.967/4.465 mean_real/fake: 0.066/0.040 var_real/fake: 0.090/0.013 \n",
      "[0/1][612/937] KL_real/fake: 4.946/5.920 mean_real/fake: 0.048/0.043 var_real/fake: 0.007/0.002 \n",
      "[0/1][613/937] KL_real/fake: 4.516/6.241 mean_real/fake: 0.021/0.043 var_real/fake: 0.016/0.002 \n",
      "[0/1][614/937] KL_real/fake: 2.975/4.530 mean_real/fake: 0.061/0.049 var_real/fake: 0.076/0.017 \n",
      "[0/1][615/937] KL_real/fake: 5.226/6.286 mean_real/fake: 0.051/0.046 var_real/fake: 0.006/0.001 \n",
      "[0/1][616/937] KL_real/fake: 4.855/6.314 mean_real/fake: 0.065/0.044 var_real/fake: 0.009/0.001 \n",
      "[0/1][617/937] KL_real/fake: 4.064/5.167 mean_real/fake: 0.033/0.041 var_real/fake: 0.036/0.006 \n",
      "[0/1][618/937] KL_real/fake: 3.103/4.466 mean_real/fake: 0.071/0.068 var_real/fake: 0.101/0.023 \n",
      "[0/1][619/937] KL_real/fake: 5.147/6.227 mean_real/fake: 0.035/0.043 var_real/fake: 0.005/0.002 \n",
      "[0/1][620/937] KL_real/fake: 5.281/6.689 mean_real/fake: 0.051/0.037 var_real/fake: 0.005/0.001 \n",
      "[0/1][621/937] KL_real/fake: 4.720/5.786 mean_real/fake: 0.041/0.047 var_real/fake: 0.011/0.003 \n",
      "[0/1][622/937] KL_real/fake: 3.518/4.848 mean_real/fake: 0.058/0.059 var_real/fake: 0.056/0.009 \n",
      "[0/1][623/937] KL_real/fake: 3.224/5.074 mean_real/fake: 0.071/0.047 var_real/fake: 0.068/0.005 \n",
      "[0/1][624/937] KL_real/fake: 3.453/4.757 mean_real/fake: 0.066/0.045 var_real/fake: 0.053/0.009 \n",
      "[0/1][625/937] KL_real/fake: 4.555/5.992 mean_real/fake: 0.055/0.051 var_real/fake: 0.011/0.002 \n",
      "[0/1][626/937] KL_real/fake: 4.462/5.618 mean_real/fake: 0.043/0.038 var_real/fake: 0.010/0.003 \n",
      "[0/1][627/937] KL_real/fake: 3.125/4.203 mean_real/fake: 0.050/0.058 var_real/fake: 0.068/0.025 \n",
      "[0/1][628/937] KL_real/fake: 4.217/5.699 mean_real/fake: 0.052/0.039 var_real/fake: 0.018/0.003 \n",
      "[0/1][629/937] KL_real/fake: 4.366/5.313 mean_real/fake: 0.056/0.045 var_real/fake: 0.013/0.005 \n",
      "[0/1][630/937] KL_real/fake: 3.038/4.308 mean_real/fake: 0.027/0.059 var_real/fake: 0.072/0.021 \n",
      "[0/1][631/937] KL_real/fake: 4.484/6.211 mean_real/fake: 0.056/0.041 var_real/fake: 0.012/0.002 \n",
      "[0/1][632/937] KL_real/fake: 4.368/5.840 mean_real/fake: 0.066/0.045 var_real/fake: 0.024/0.002 \n",
      "[0/1][633/937] KL_real/fake: 3.473/4.555 mean_real/fake: 0.032/0.043 var_real/fake: 0.059/0.016 \n",
      "[0/1][634/937] KL_real/fake: 4.644/5.900 mean_real/fake: 0.034/0.032 var_real/fake: 0.011/0.002 \n",
      "[0/1][635/937] KL_real/fake: 4.992/5.983 mean_real/fake: 0.035/0.043 var_real/fake: 0.007/0.002 \n",
      "[0/1][636/937] KL_real/fake: 3.224/4.614 mean_real/fake: 0.078/0.043 var_real/fake: 0.097/0.011 \n",
      "[0/1][637/937] KL_real/fake: 4.683/5.662 mean_real/fake: 0.033/0.047 var_real/fake: 0.010/0.003 \n",
      "[0/1][638/937] KL_real/fake: 3.916/5.402 mean_real/fake: 0.054/0.055 var_real/fake: 0.030/0.005 \n",
      "[0/1][639/937] KL_real/fake: 2.718/3.952 mean_real/fake: 0.041/0.052 var_real/fake: 0.085/0.026 \n",
      "[0/1][640/937] KL_real/fake: 5.534/6.468 mean_real/fake: 0.043/0.044 var_real/fake: 0.003/0.001 \n",
      "[0/1][641/937] KL_real/fake: 6.163/6.814 mean_real/fake: 0.039/0.034 var_real/fake: 0.002/0.001 \n",
      "[0/1][642/937] KL_real/fake: 5.731/6.841 mean_real/fake: 0.046/0.045 var_real/fake: 0.003/0.001 \n",
      "[0/1][643/937] KL_real/fake: 4.464/5.621 mean_real/fake: 0.051/0.043 var_real/fake: 0.014/0.003 \n",
      "[0/1][644/937] KL_real/fake: 3.577/4.501 mean_real/fake: 0.051/0.070 var_real/fake: 0.077/0.017 \n",
      "[0/1][645/937] KL_real/fake: 3.260/3.767 mean_real/fake: 0.064/0.070 var_real/fake: 0.102/0.044 \n",
      "[0/1][646/937] KL_real/fake: 3.581/4.610 mean_real/fake: 0.060/0.064 var_real/fake: 0.060/0.012 \n",
      "[0/1][647/937] KL_real/fake: 3.823/4.917 mean_real/fake: 0.066/0.055 var_real/fake: 0.046/0.009 \n",
      "[0/1][648/937] KL_real/fake: 3.804/5.299 mean_real/fake: 0.055/0.037 var_real/fake: 0.046/0.005 \n",
      "[0/1][649/937] KL_real/fake: 3.742/5.031 mean_real/fake: 0.062/0.039 var_real/fake: 0.043/0.008 \n",
      "[0/1][650/937] KL_real/fake: 3.241/4.519 mean_real/fake: 0.054/0.063 var_real/fake: 0.089/0.017 \n",
      "[0/1][651/937] KL_real/fake: 4.653/5.750 mean_real/fake: 0.073/0.039 var_real/fake: 0.013/0.003 \n",
      "[0/1][652/937] KL_real/fake: 4.415/5.871 mean_real/fake: 0.064/0.043 var_real/fake: 0.013/0.002 \n",
      "[0/1][653/937] KL_real/fake: 3.330/4.540 mean_real/fake: 0.043/0.021 var_real/fake: 0.070/0.018 \n",
      "[0/1][654/937] KL_real/fake: 5.502/6.445 mean_real/fake: 0.037/0.040 var_real/fake: 0.003/0.001 \n",
      "[0/1][655/937] KL_real/fake: 5.159/6.374 mean_real/fake: 0.070/0.043 var_real/fake: 0.007/0.001 \n",
      "[0/1][656/937] KL_real/fake: 4.130/5.577 mean_real/fake: 0.078/0.044 var_real/fake: 0.028/0.004 \n",
      "[0/1][657/937] KL_real/fake: 3.014/3.912 mean_real/fake: 0.038/0.034 var_real/fake: 0.064/0.027 \n",
      "[0/1][658/937] KL_real/fake: 6.150/6.457 mean_real/fake: 0.040/0.045 var_real/fake: 0.002/0.001 \n",
      "[0/1][659/937] KL_real/fake: 6.576/7.087 mean_real/fake: 0.040/0.039 var_real/fake: 0.001/0.001 \n",
      "[0/1][660/937] KL_real/fake: 6.791/7.038 mean_real/fake: 0.041/0.034 var_real/fake: 0.001/0.001 \n",
      "[0/1][661/937] KL_real/fake: 6.291/6.820 mean_real/fake: 0.041/0.038 var_real/fake: 0.001/0.001 \n",
      "[0/1][662/937] KL_real/fake: 5.041/6.002 mean_real/fake: 0.046/0.045 var_real/fake: 0.006/0.002 \n",
      "[0/1][663/937] KL_real/fake: 4.431/5.656 mean_real/fake: 0.050/0.051 var_real/fake: 0.018/0.003 \n",
      "[0/1][664/937] KL_real/fake: 3.507/4.726 mean_real/fake: 0.097/0.048 var_real/fake: 0.075/0.012 \n",
      "[0/1][665/937] KL_real/fake: 3.392/4.854 mean_real/fake: 0.065/0.055 var_real/fake: 0.068/0.009 \n",
      "[0/1][666/937] KL_real/fake: 3.263/4.403 mean_real/fake: 0.051/0.056 var_real/fake: 0.118/0.016 \n",
      "[0/1][667/937] KL_real/fake: 4.160/5.639 mean_real/fake: 0.038/0.044 var_real/fake: 0.019/0.003 \n",
      "[0/1][668/937] KL_real/fake: 3.425/5.300 mean_real/fake: 0.044/0.036 var_real/fake: 0.069/0.005 \n",
      "[0/1][669/937] KL_real/fake: 3.145/3.953 mean_real/fake: 0.036/0.043 var_real/fake: 0.062/0.037 \n",
      "[0/1][670/937] KL_real/fake: 5.162/6.327 mean_real/fake: 0.031/0.040 var_real/fake: 0.005/0.001 \n",
      "[0/1][671/937] KL_real/fake: 5.341/6.726 mean_real/fake: 0.036/0.039 var_real/fake: 0.004/0.001 \n",
      "[0/1][672/937] KL_real/fake: 4.468/5.923 mean_real/fake: 0.059/0.048 var_real/fake: 0.015/0.002 \n",
      "[0/1][673/937] KL_real/fake: 3.355/4.115 mean_real/fake: 0.053/0.041 var_real/fake: 0.074/0.031 \n",
      "[0/1][674/937] KL_real/fake: 4.152/5.237 mean_real/fake: 0.060/0.041 var_real/fake: 0.034/0.005 \n",
      "[0/1][675/937] KL_real/fake: 4.314/5.526 mean_real/fake: 0.035/0.042 var_real/fake: 0.020/0.004 \n",
      "[0/1][676/937] KL_real/fake: 3.721/4.864 mean_real/fake: 0.083/0.075 var_real/fake: 0.039/0.013 \n",
      "[0/1][677/937] KL_real/fake: 3.654/4.907 mean_real/fake: 0.050/0.038 var_real/fake: 0.040/0.009 \n",
      "[0/1][678/937] KL_real/fake: 4.162/5.261 mean_real/fake: 0.033/0.034 var_real/fake: 0.030/0.004 \n",
      "[0/1][679/937] KL_real/fake: 4.265/5.158 mean_real/fake: 0.046/0.044 var_real/fake: 0.018/0.006 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][680/937] KL_real/fake: 3.526/4.675 mean_real/fake: 0.057/0.062 var_real/fake: 0.039/0.010 \n",
      "[0/1][681/937] KL_real/fake: 4.049/5.513 mean_real/fake: 0.037/0.040 var_real/fake: 0.025/0.003 \n",
      "[0/1][682/937] KL_real/fake: 3.971/5.095 mean_real/fake: 0.058/0.038 var_real/fake: 0.028/0.006 \n",
      "[0/1][683/937] KL_real/fake: 3.529/4.612 mean_real/fake: 0.097/0.063 var_real/fake: 0.058/0.015 \n",
      "[0/1][684/937] KL_real/fake: 3.988/5.636 mean_real/fake: 0.049/0.048 var_real/fake: 0.029/0.003 \n",
      "[0/1][685/937] KL_real/fake: 3.629/5.401 mean_real/fake: 0.083/0.046 var_real/fake: 0.058/0.005 \n",
      "[0/1][686/937] KL_real/fake: 3.089/4.297 mean_real/fake: 0.055/0.060 var_real/fake: 0.064/0.025 \n",
      "[0/1][687/937] KL_real/fake: 5.351/6.023 mean_real/fake: 0.029/0.038 var_real/fake: 0.004/0.002 \n",
      "[0/1][688/937] KL_real/fake: 4.912/6.030 mean_real/fake: 0.045/0.046 var_real/fake: 0.008/0.002 \n",
      "[0/1][689/937] KL_real/fake: 3.826/4.905 mean_real/fake: 0.071/0.065 var_real/fake: 0.036/0.010 \n",
      "[0/1][690/937] KL_real/fake: 3.511/4.960 mean_real/fake: 0.071/0.061 var_real/fake: 0.050/0.008 \n",
      "[0/1][691/937] KL_real/fake: 4.786/5.535 mean_real/fake: 0.050/0.049 var_real/fake: 0.015/0.004 \n",
      "[0/1][692/937] KL_real/fake: 5.181/5.961 mean_real/fake: 0.053/0.041 var_real/fake: 0.005/0.002 \n",
      "[0/1][693/937] KL_real/fake: 4.398/5.632 mean_real/fake: 0.058/0.056 var_real/fake: 0.018/0.006 \n",
      "[0/1][694/937] KL_real/fake: 3.749/5.184 mean_real/fake: 0.083/0.040 var_real/fake: 0.042/0.005 \n",
      "[0/1][695/937] KL_real/fake: 3.566/5.107 mean_real/fake: 0.071/0.030 var_real/fake: 0.057/0.007 \n",
      "[0/1][696/937] KL_real/fake: 4.355/5.518 mean_real/fake: 0.081/0.038 var_real/fake: 0.026/0.003 \n",
      "[0/1][697/937] KL_real/fake: 4.579/5.483 mean_real/fake: 0.043/0.049 var_real/fake: 0.013/0.005 \n",
      "[0/1][698/937] KL_real/fake: 4.312/5.253 mean_real/fake: 0.058/0.054 var_real/fake: 0.018/0.006 \n",
      "[0/1][699/937] KL_real/fake: 3.479/4.455 mean_real/fake: 0.035/0.061 var_real/fake: 0.039/0.021 \n",
      "[0/1][700/937] KL_real/fake: 3.431/4.472 mean_real/fake: 0.077/0.034 var_real/fake: 0.059/0.013 \n",
      "[0/1][701/937] KL_real/fake: 5.298/6.102 mean_real/fake: 0.043/0.033 var_real/fake: 0.004/0.002 \n",
      "[0/1][702/937] KL_real/fake: 4.509/5.735 mean_real/fake: 0.043/0.051 var_real/fake: 0.015/0.003 \n",
      "[0/1][703/937] KL_real/fake: 3.677/4.612 mean_real/fake: 0.052/0.039 var_real/fake: 0.048/0.015 \n",
      "[0/1][704/937] KL_real/fake: 3.156/4.335 mean_real/fake: 0.084/0.072 var_real/fake: 0.094/0.031 \n",
      "[0/1][705/937] KL_real/fake: 4.664/5.593 mean_real/fake: 0.060/0.045 var_real/fake: 0.010/0.004 \n",
      "[0/1][706/937] KL_real/fake: 4.320/5.073 mean_real/fake: 0.038/0.047 var_real/fake: 0.015/0.008 \n",
      "[0/1][707/937] KL_real/fake: 3.547/4.799 mean_real/fake: 0.044/0.046 var_real/fake: 0.055/0.014 \n",
      "[0/1][708/937] KL_real/fake: 3.482/5.048 mean_real/fake: 0.052/0.070 var_real/fake: 0.061/0.010 \n",
      "[0/1][709/937] KL_real/fake: 4.273/5.781 mean_real/fake: 0.044/0.044 var_real/fake: 0.020/0.003 \n",
      "[0/1][710/937] KL_real/fake: 3.079/4.632 mean_real/fake: 0.057/0.067 var_real/fake: 0.081/0.014 \n",
      "[0/1][711/937] KL_real/fake: 4.875/5.733 mean_real/fake: 0.041/0.052 var_real/fake: 0.008/0.003 \n",
      "[0/1][712/937] KL_real/fake: 3.709/5.028 mean_real/fake: 0.065/0.059 var_real/fake: 0.047/0.007 \n",
      "[0/1][713/937] KL_real/fake: 3.897/5.488 mean_real/fake: 0.074/0.044 var_real/fake: 0.044/0.004 \n",
      "[0/1][714/937] KL_real/fake: 3.446/4.487 mean_real/fake: 0.045/0.074 var_real/fake: 0.053/0.016 \n",
      "[0/1][715/937] KL_real/fake: 4.931/6.105 mean_real/fake: 0.066/0.044 var_real/fake: 0.008/0.002 \n",
      "[0/1][716/937] KL_real/fake: 5.138/6.227 mean_real/fake: 0.040/0.038 var_real/fake: 0.005/0.001 \n",
      "[0/1][717/937] KL_real/fake: 4.383/5.360 mean_real/fake: 0.064/0.055 var_real/fake: 0.022/0.005 \n",
      "[0/1][718/937] KL_real/fake: 3.546/4.984 mean_real/fake: 0.079/0.052 var_real/fake: 0.071/0.007 \n",
      "[0/1][719/937] KL_real/fake: 2.920/4.250 mean_real/fake: 0.051/0.058 var_real/fake: 0.089/0.028 \n",
      "[0/1][720/937] KL_real/fake: 5.249/6.500 mean_real/fake: 0.049/0.040 var_real/fake: 0.005/0.001 \n",
      "[0/1][721/937] KL_real/fake: 5.340/6.247 mean_real/fake: 0.050/0.044 var_real/fake: 0.005/0.002 \n",
      "[0/1][722/937] KL_real/fake: 5.133/6.098 mean_real/fake: 0.050/0.039 var_real/fake: 0.006/0.002 \n",
      "[0/1][723/937] KL_real/fake: 4.333/5.454 mean_real/fake: 0.071/0.056 var_real/fake: 0.018/0.005 \n",
      "[0/1][724/937] KL_real/fake: 3.177/3.706 mean_real/fake: 0.043/0.080 var_real/fake: 0.074/0.065 \n",
      "[0/1][725/937] KL_real/fake: 4.145/5.223 mean_real/fake: 0.069/0.056 var_real/fake: 0.025/0.006 \n",
      "[0/1][726/937] KL_real/fake: 4.171/5.633 mean_real/fake: 0.069/0.051 var_real/fake: 0.020/0.003 \n",
      "[0/1][727/937] KL_real/fake: 3.509/4.920 mean_real/fake: 0.062/0.042 var_real/fake: 0.079/0.009 \n",
      "[0/1][728/937] KL_real/fake: 3.582/4.724 mean_real/fake: 0.042/0.045 var_real/fake: 0.045/0.011 \n",
      "[0/1][729/937] KL_real/fake: 3.442/4.851 mean_real/fake: 0.016/0.036 var_real/fake: 0.051/0.009 \n",
      "[0/1][730/937] KL_real/fake: 4.025/5.499 mean_real/fake: 0.057/0.042 var_real/fake: 0.042/0.004 \n",
      "[0/1][731/937] KL_real/fake: 3.242/4.476 mean_real/fake: 0.032/0.061 var_real/fake: 0.053/0.015 \n",
      "[0/1][732/937] KL_real/fake: 5.455/6.527 mean_real/fake: 0.053/0.045 var_real/fake: 0.004/0.001 \n",
      "[0/1][733/937] KL_real/fake: 5.234/6.405 mean_real/fake: 0.054/0.044 var_real/fake: 0.006/0.001 \n",
      "[0/1][734/937] KL_real/fake: 4.138/5.762 mean_real/fake: 0.040/0.043 var_real/fake: 0.022/0.003 \n",
      "[0/1][735/937] KL_real/fake: 3.078/4.160 mean_real/fake: 0.034/0.087 var_real/fake: 0.067/0.036 \n",
      "[0/1][736/937] KL_real/fake: 4.739/5.673 mean_real/fake: 0.044/0.031 var_real/fake: 0.009/0.003 \n",
      "[0/1][737/937] KL_real/fake: 5.059/5.895 mean_real/fake: 0.055/0.048 var_real/fake: 0.007/0.003 \n",
      "[0/1][738/937] KL_real/fake: 4.470/5.661 mean_real/fake: 0.060/0.057 var_real/fake: 0.013/0.004 \n",
      "[0/1][739/937] KL_real/fake: 3.558/4.912 mean_real/fake: 0.062/0.060 var_real/fake: 0.049/0.009 \n",
      "[0/1][740/937] KL_real/fake: 3.908/5.144 mean_real/fake: 0.049/0.048 var_real/fake: 0.027/0.007 \n",
      "[0/1][741/937] KL_real/fake: 3.709/4.955 mean_real/fake: 0.045/0.049 var_real/fake: 0.045/0.009 \n",
      "[0/1][742/937] KL_real/fake: 4.169/5.169 mean_real/fake: 0.084/0.029 var_real/fake: 0.031/0.006 \n",
      "[0/1][743/937] KL_real/fake: 3.044/4.542 mean_real/fake: 0.070/0.049 var_real/fake: 0.071/0.014 \n",
      "[0/1][744/937] KL_real/fake: 3.822/5.366 mean_real/fake: 0.036/0.057 var_real/fake: 0.029/0.005 \n",
      "[0/1][745/937] KL_real/fake: 3.822/4.792 mean_real/fake: 0.007/0.049 var_real/fake: 0.035/0.010 \n",
      "[0/1][746/937] KL_real/fake: 4.130/5.626 mean_real/fake: 0.064/0.045 var_real/fake: 0.040/0.003 \n",
      "[0/1][747/937] KL_real/fake: 4.541/5.630 mean_real/fake: 0.049/0.042 var_real/fake: 0.013/0.003 \n",
      "[0/1][748/937] KL_real/fake: 3.914/5.406 mean_real/fake: 0.070/0.050 var_real/fake: 0.044/0.004 \n",
      "[0/1][749/937] KL_real/fake: 3.968/5.544 mean_real/fake: 0.060/0.042 var_real/fake: 0.043/0.005 \n",
      "[0/1][750/937] KL_real/fake: 4.736/5.753 mean_real/fake: 0.050/0.045 var_real/fake: 0.009/0.003 \n",
      "[0/1][751/937] KL_real/fake: 3.147/4.340 mean_real/fake: 0.028/0.067 var_real/fake: 0.058/0.022 \n",
      "[0/1][752/937] KL_real/fake: 5.301/6.354 mean_real/fake: 0.037/0.039 var_real/fake: 0.004/0.001 \n",
      "[0/1][753/937] KL_real/fake: 5.795/6.691 mean_real/fake: 0.044/0.040 var_real/fake: 0.002/0.001 \n",
      "[0/1][754/937] KL_real/fake: 4.797/5.890 mean_real/fake: 0.043/0.036 var_real/fake: 0.011/0.003 \n",
      "[0/1][755/937] KL_real/fake: 3.881/5.101 mean_real/fake: 0.039/0.050 var_real/fake: 0.035/0.007 \n",
      "[0/1][756/937] KL_real/fake: 3.528/4.628 mean_real/fake: 0.072/0.053 var_real/fake: 0.049/0.015 \n",
      "[0/1][757/937] KL_real/fake: 4.249/5.363 mean_real/fake: 0.024/0.053 var_real/fake: 0.020/0.005 \n",
      "[0/1][758/937] KL_real/fake: 3.766/5.093 mean_real/fake: 0.049/0.039 var_real/fake: 0.036/0.008 \n",
      "[0/1][759/937] KL_real/fake: 3.566/4.994 mean_real/fake: 0.080/0.051 var_real/fake: 0.075/0.009 \n",
      "[0/1][760/937] KL_real/fake: 4.149/5.296 mean_real/fake: 0.016/0.065 var_real/fake: 0.024/0.006 \n",
      "[0/1][761/937] KL_real/fake: 3.324/4.444 mean_real/fake: 0.040/0.070 var_real/fake: 0.042/0.022 \n",
      "[0/1][762/937] KL_real/fake: 5.316/6.374 mean_real/fake: 0.048/0.045 var_real/fake: 0.004/0.001 \n",
      "[0/1][763/937] KL_real/fake: 5.044/6.329 mean_real/fake: 0.046/0.041 var_real/fake: 0.007/0.001 \n",
      "[0/1][764/937] KL_real/fake: 3.848/5.243 mean_real/fake: 0.062/0.047 var_real/fake: 0.052/0.006 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][765/937] KL_real/fake: 3.483/4.211 mean_real/fake: 0.055/0.073 var_real/fake: 0.052/0.026 \n",
      "[0/1][766/937] KL_real/fake: 3.837/5.068 mean_real/fake: 0.129/0.053 var_real/fake: 0.075/0.008 \n",
      "[0/1][767/937] KL_real/fake: 4.165/5.178 mean_real/fake: 0.061/0.037 var_real/fake: 0.018/0.005 \n",
      "[0/1][768/937] KL_real/fake: 3.042/5.029 mean_real/fake: 0.045/0.042 var_real/fake: 0.117/0.007 \n",
      "[0/1][769/937] KL_real/fake: 5.022/6.197 mean_real/fake: 0.023/0.036 var_real/fake: 0.007/0.002 \n",
      "[0/1][770/937] KL_real/fake: 5.212/6.274 mean_real/fake: 0.047/0.046 var_real/fake: 0.005/0.002 \n",
      "[0/1][771/937] KL_real/fake: 4.151/5.414 mean_real/fake: 0.065/0.047 var_real/fake: 0.032/0.004 \n",
      "[0/1][772/937] KL_real/fake: 3.434/4.611 mean_real/fake: 0.106/0.053 var_real/fake: 0.080/0.017 \n",
      "[0/1][773/937] KL_real/fake: 4.214/5.567 mean_real/fake: 0.084/0.044 var_real/fake: 0.035/0.004 \n",
      "[0/1][774/937] KL_real/fake: 3.897/5.509 mean_real/fake: 0.081/0.059 var_real/fake: 0.039/0.005 \n",
      "[0/1][775/937] KL_real/fake: 3.744/4.751 mean_real/fake: 0.050/0.045 var_real/fake: 0.043/0.013 \n",
      "[0/1][776/937] KL_real/fake: 3.832/4.924 mean_real/fake: 0.023/0.053 var_real/fake: 0.038/0.009 \n",
      "[0/1][777/937] KL_real/fake: 3.601/4.743 mean_real/fake: 0.051/0.055 var_real/fake: 0.034/0.014 \n",
      "[0/1][778/937] KL_real/fake: 3.596/5.016 mean_real/fake: 0.104/0.051 var_real/fake: 0.053/0.007 \n",
      "[0/1][779/937] KL_real/fake: 3.524/5.430 mean_real/fake: 0.077/0.046 var_real/fake: 0.045/0.004 \n",
      "[0/1][780/937] KL_real/fake: 2.893/4.676 mean_real/fake: 0.050/0.053 var_real/fake: 0.108/0.013 \n",
      "[0/1][781/937] KL_real/fake: 6.018/6.674 mean_real/fake: 0.041/0.040 var_real/fake: 0.002/0.001 \n",
      "[0/1][782/937] KL_real/fake: 6.381/6.554 mean_real/fake: 0.036/0.043 var_real/fake: 0.001/0.001 \n",
      "[0/1][783/937] KL_real/fake: 6.374/6.451 mean_real/fake: 0.040/0.043 var_real/fake: 0.001/0.001 \n",
      "[0/1][784/937] KL_real/fake: 4.962/6.224 mean_real/fake: 0.056/0.046 var_real/fake: 0.007/0.002 \n",
      "[0/1][785/937] KL_real/fake: 3.367/4.670 mean_real/fake: 0.087/0.065 var_real/fake: 0.091/0.013 \n",
      "[0/1][786/937] KL_real/fake: 2.901/3.891 mean_real/fake: 0.074/0.076 var_real/fake: 0.119/0.044 \n",
      "[0/1][787/937] KL_real/fake: 3.663/4.525 mean_real/fake: 0.075/0.066 var_real/fake: 0.047/0.018 \n",
      "[0/1][788/937] KL_real/fake: 4.603/5.148 mean_real/fake: 0.059/0.060 var_real/fake: 0.016/0.007 \n",
      "[0/1][789/937] KL_real/fake: 5.098/6.099 mean_real/fake: 0.050/0.052 var_real/fake: 0.006/0.002 \n",
      "[0/1][790/937] KL_real/fake: 5.241/6.024 mean_real/fake: 0.045/0.034 var_real/fake: 0.005/0.002 \n",
      "[0/1][791/937] KL_real/fake: 4.510/5.884 mean_real/fake: 0.035/0.038 var_real/fake: 0.012/0.002 \n",
      "[0/1][792/937] KL_real/fake: 4.105/5.370 mean_real/fake: 0.086/0.037 var_real/fake: 0.042/0.005 \n",
      "[0/1][793/937] KL_real/fake: 3.331/4.394 mean_real/fake: 0.045/0.010 var_real/fake: 0.064/0.022 \n",
      "[0/1][794/937] KL_real/fake: 3.709/4.500 mean_real/fake: 0.063/0.056 var_real/fake: 0.056/0.017 \n",
      "[0/1][795/937] KL_real/fake: 4.126/5.241 mean_real/fake: 0.069/0.057 var_real/fake: 0.034/0.006 \n",
      "[0/1][796/937] KL_real/fake: 4.195/5.083 mean_real/fake: 0.058/0.045 var_real/fake: 0.019/0.007 \n",
      "[0/1][797/937] KL_real/fake: 4.184/5.133 mean_real/fake: 0.059/0.043 var_real/fake: 0.022/0.007 \n",
      "[0/1][798/937] KL_real/fake: 3.062/3.881 mean_real/fake: 0.033/0.045 var_real/fake: 0.064/0.030 \n",
      "[0/1][799/937] KL_real/fake: 4.144/5.060 mean_real/fake: 0.026/0.055 var_real/fake: 0.018/0.007 \n",
      "[0/1][800/937] KL_real/fake: 4.015/5.102 mean_real/fake: 0.055/0.039 var_real/fake: 0.022/0.006 \n",
      "[0/1][801/937] KL_real/fake: 3.504/4.749 mean_real/fake: 0.042/0.057 var_real/fake: 0.048/0.009 \n",
      "[0/1][802/937] KL_real/fake: 3.593/5.063 mean_real/fake: 0.073/0.047 var_real/fake: 0.071/0.007 \n",
      "[0/1][803/937] KL_real/fake: 4.566/5.402 mean_real/fake: 0.069/0.055 var_real/fake: 0.014/0.006 \n",
      "[0/1][804/937] KL_real/fake: 3.306/4.330 mean_real/fake: 0.046/0.028 var_real/fake: 0.053/0.020 \n",
      "[0/1][805/937] KL_real/fake: 5.056/5.766 mean_real/fake: 0.032/0.043 var_real/fake: 0.006/0.003 \n",
      "[0/1][806/937] KL_real/fake: 4.947/6.012 mean_real/fake: 0.052/0.041 var_real/fake: 0.009/0.002 \n",
      "[0/1][807/937] KL_real/fake: 4.001/5.082 mean_real/fake: 0.053/0.040 var_real/fake: 0.030/0.008 \n",
      "[0/1][808/937] KL_real/fake: 3.884/5.388 mean_real/fake: 0.046/0.031 var_real/fake: 0.062/0.005 \n",
      "[0/1][809/937] KL_real/fake: 3.417/4.651 mean_real/fake: 0.038/0.040 var_real/fake: 0.077/0.016 \n",
      "[0/1][810/937] KL_real/fake: 4.001/5.124 mean_real/fake: 0.067/0.048 var_real/fake: 0.030/0.007 \n",
      "[0/1][811/937] KL_real/fake: 3.294/4.907 mean_real/fake: 0.080/0.061 var_real/fake: 0.071/0.009 \n",
      "[0/1][812/937] KL_real/fake: 3.643/5.083 mean_real/fake: 0.041/0.060 var_real/fake: 0.061/0.008 \n",
      "[0/1][813/937] KL_real/fake: 3.487/4.906 mean_real/fake: 0.057/0.048 var_real/fake: 0.071/0.008 \n",
      "[0/1][814/937] KL_real/fake: 5.549/6.632 mean_real/fake: 0.038/0.042 var_real/fake: 0.004/0.001 \n",
      "[0/1][815/937] KL_real/fake: 5.466/6.352 mean_real/fake: 0.049/0.035 var_real/fake: 0.004/0.001 \n",
      "[0/1][816/937] KL_real/fake: 4.000/5.419 mean_real/fake: 0.084/0.043 var_real/fake: 0.031/0.004 \n",
      "[0/1][817/937] KL_real/fake: 3.412/4.662 mean_real/fake: 0.059/0.054 var_real/fake: 0.066/0.014 \n",
      "[0/1][818/937] KL_real/fake: 4.599/5.739 mean_real/fake: 0.057/0.050 var_real/fake: 0.016/0.003 \n",
      "[0/1][819/937] KL_real/fake: 4.384/5.715 mean_real/fake: 0.066/0.041 var_real/fake: 0.024/0.003 \n",
      "[0/1][820/937] KL_real/fake: 3.867/5.236 mean_real/fake: 0.065/0.020 var_real/fake: 0.039/0.006 \n",
      "[0/1][821/937] KL_real/fake: 3.862/5.324 mean_real/fake: 0.041/0.049 var_real/fake: 0.034/0.006 \n",
      "[0/1][822/937] KL_real/fake: 3.286/4.481 mean_real/fake: 0.072/0.076 var_real/fake: 0.088/0.021 \n",
      "[0/1][823/937] KL_real/fake: 5.436/6.123 mean_real/fake: 0.034/0.046 var_real/fake: 0.004/0.002 \n",
      "[0/1][824/937] KL_real/fake: 5.468/6.017 mean_real/fake: 0.057/0.051 var_real/fake: 0.004/0.002 \n",
      "[0/1][825/937] KL_real/fake: 4.872/6.140 mean_real/fake: 0.038/0.040 var_real/fake: 0.008/0.002 \n",
      "[0/1][826/937] KL_real/fake: 3.468/4.610 mean_real/fake: 0.068/0.091 var_real/fake: 0.074/0.015 \n",
      "[0/1][827/937] KL_real/fake: 3.567/4.853 mean_real/fake: 0.070/0.065 var_real/fake: 0.068/0.011 \n",
      "[0/1][828/937] KL_real/fake: 5.057/5.881 mean_real/fake: 0.048/0.044 var_real/fake: 0.007/0.003 \n",
      "[0/1][829/937] KL_real/fake: 4.897/6.137 mean_real/fake: 0.040/0.038 var_real/fake: 0.011/0.002 \n",
      "[0/1][830/937] KL_real/fake: 4.671/5.792 mean_real/fake: 0.050/0.045 var_real/fake: 0.012/0.003 \n",
      "[0/1][831/937] KL_real/fake: 3.788/4.955 mean_real/fake: 0.079/0.065 var_real/fake: 0.051/0.009 \n",
      "[0/1][832/937] KL_real/fake: 3.214/4.417 mean_real/fake: 0.071/0.057 var_real/fake: 0.074/0.020 \n",
      "[0/1][833/937] KL_real/fake: 4.453/5.703 mean_real/fake: 0.062/0.044 var_real/fake: 0.017/0.003 \n",
      "[0/1][834/937] KL_real/fake: 4.209/5.591 mean_real/fake: 0.083/0.062 var_real/fake: 0.055/0.005 \n",
      "[0/1][835/937] KL_real/fake: 5.549/6.276 mean_real/fake: 0.035/0.039 var_real/fake: 0.003/0.002 \n",
      "[0/1][836/937] KL_real/fake: 4.640/6.022 mean_real/fake: 0.075/0.040 var_real/fake: 0.014/0.002 \n",
      "[0/1][837/937] KL_real/fake: 3.817/4.965 mean_real/fake: 0.030/0.037 var_real/fake: 0.035/0.007 \n",
      "[0/1][838/937] KL_real/fake: 3.380/3.991 mean_real/fake: 0.036/0.056 var_real/fake: 0.046/0.031 \n",
      "[0/1][839/937] KL_real/fake: 4.962/5.848 mean_real/fake: 0.051/0.039 var_real/fake: 0.008/0.002 \n",
      "[0/1][840/937] KL_real/fake: 4.763/5.590 mean_real/fake: 0.064/0.042 var_real/fake: 0.011/0.003 \n",
      "[0/1][841/937] KL_real/fake: 3.889/5.260 mean_real/fake: 0.080/0.040 var_real/fake: 0.039/0.005 \n",
      "[0/1][842/937] KL_real/fake: 3.452/4.236 mean_real/fake: 0.069/0.056 var_real/fake: 0.060/0.020 \n",
      "[0/1][843/937] KL_real/fake: 3.869/5.251 mean_real/fake: 0.064/0.045 var_real/fake: 0.034/0.006 \n",
      "[0/1][844/937] KL_real/fake: 3.202/4.695 mean_real/fake: 0.049/0.038 var_real/fake: 0.087/0.010 \n",
      "[0/1][845/937] KL_real/fake: 4.389/5.453 mean_real/fake: 0.060/0.043 var_real/fake: 0.019/0.005 \n",
      "[0/1][846/937] KL_real/fake: 4.502/5.586 mean_real/fake: 0.067/0.052 var_real/fake: 0.013/0.004 \n",
      "[0/1][847/937] KL_real/fake: 3.852/5.157 mean_real/fake: 0.053/0.042 var_real/fake: 0.042/0.007 \n",
      "[0/1][848/937] KL_real/fake: 3.533/5.025 mean_real/fake: 0.049/0.041 var_real/fake: 0.050/0.005 \n",
      "[0/1][849/937] KL_real/fake: 3.092/3.812 mean_real/fake: 0.039/0.049 var_real/fake: 0.061/0.037 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][850/937] KL_real/fake: 4.328/5.441 mean_real/fake: 0.064/0.060 var_real/fake: 0.020/0.005 \n",
      "[0/1][851/937] KL_real/fake: 4.345/5.396 mean_real/fake: 0.053/0.063 var_real/fake: 0.017/0.006 \n",
      "[0/1][852/937] KL_real/fake: 3.397/4.450 mean_real/fake: 0.034/0.061 var_real/fake: 0.050/0.018 \n",
      "[0/1][853/937] KL_real/fake: 3.699/4.927 mean_real/fake: 0.064/0.044 var_real/fake: 0.044/0.008 \n",
      "[0/1][854/937] KL_real/fake: 3.804/5.386 mean_real/fake: 0.062/0.052 var_real/fake: 0.036/0.005 \n",
      "[0/1][855/937] KL_real/fake: 4.327/5.665 mean_real/fake: 0.033/0.048 var_real/fake: 0.016/0.003 \n",
      "[0/1][856/937] KL_real/fake: 3.595/5.008 mean_real/fake: 0.051/0.050 var_real/fake: 0.048/0.008 \n",
      "[0/1][857/937] KL_real/fake: 3.922/5.419 mean_real/fake: 0.056/0.055 var_real/fake: 0.031/0.004 \n",
      "[0/1][858/937] KL_real/fake: 4.304/5.559 mean_real/fake: 0.035/0.041 var_real/fake: 0.015/0.003 \n",
      "[0/1][859/937] KL_real/fake: 3.287/4.388 mean_real/fake: 0.047/0.035 var_real/fake: 0.056/0.017 \n",
      "[0/1][860/937] KL_real/fake: 4.369/5.548 mean_real/fake: 0.054/0.034 var_real/fake: 0.018/0.004 \n",
      "[0/1][861/937] KL_real/fake: 4.557/5.492 mean_real/fake: 0.049/0.048 var_real/fake: 0.013/0.004 \n",
      "[0/1][862/937] KL_real/fake: 3.566/4.552 mean_real/fake: 0.045/0.050 var_real/fake: 0.067/0.019 \n",
      "[0/1][863/937] KL_real/fake: 3.830/5.042 mean_real/fake: 0.052/0.042 var_real/fake: 0.056/0.007 \n",
      "[0/1][864/937] KL_real/fake: 4.363/5.790 mean_real/fake: 0.054/0.053 var_real/fake: 0.027/0.004 \n",
      "[0/1][865/937] KL_real/fake: 3.547/5.016 mean_real/fake: 0.077/0.035 var_real/fake: 0.075/0.008 \n",
      "[0/1][866/937] KL_real/fake: 4.095/5.200 mean_real/fake: 0.043/0.035 var_real/fake: 0.025/0.006 \n",
      "[0/1][867/937] KL_real/fake: 3.502/4.951 mean_real/fake: 0.061/0.044 var_real/fake: 0.073/0.007 \n",
      "[0/1][868/937] KL_real/fake: 4.077/5.411 mean_real/fake: 0.037/0.048 var_real/fake: 0.021/0.005 \n",
      "[0/1][869/937] KL_real/fake: 3.176/4.729 mean_real/fake: 0.029/0.035 var_real/fake: 0.074/0.009 \n",
      "[0/1][870/937] KL_real/fake: 5.115/6.123 mean_real/fake: 0.048/0.036 var_real/fake: 0.005/0.002 \n",
      "[0/1][871/937] KL_real/fake: 4.368/5.829 mean_real/fake: 0.052/0.039 var_real/fake: 0.015/0.002 \n",
      "[0/1][872/937] KL_real/fake: 3.161/4.193 mean_real/fake: 0.051/0.065 var_real/fake: 0.070/0.022 \n",
      "[0/1][873/937] KL_real/fake: 5.362/6.436 mean_real/fake: 0.048/0.037 var_real/fake: 0.005/0.001 \n",
      "[0/1][874/937] KL_real/fake: 5.411/6.172 mean_real/fake: 0.044/0.044 var_real/fake: 0.003/0.002 \n",
      "[0/1][875/937] KL_real/fake: 3.954/5.103 mean_real/fake: 0.057/0.034 var_real/fake: 0.028/0.008 \n",
      "[0/1][876/937] KL_real/fake: 3.198/3.843 mean_real/fake: 0.049/0.060 var_real/fake: 0.059/0.045 \n",
      "[0/1][877/937] KL_real/fake: 5.285/6.509 mean_real/fake: 0.058/0.044 var_real/fake: 0.005/0.001 \n",
      "[0/1][878/937] KL_real/fake: 5.700/6.370 mean_real/fake: 0.042/0.047 var_real/fake: 0.003/0.001 \n",
      "[0/1][879/937] KL_real/fake: 5.081/6.342 mean_real/fake: 0.048/0.033 var_real/fake: 0.013/0.001 \n",
      "[0/1][880/937] KL_real/fake: 4.468/5.669 mean_real/fake: 0.095/0.036 var_real/fake: 0.026/0.004 \n",
      "[0/1][881/937] KL_real/fake: 5.153/5.597 mean_real/fake: 0.051/0.047 var_real/fake: 0.006/0.005 \n",
      "[0/1][882/937] KL_real/fake: 4.833/5.741 mean_real/fake: 0.043/0.045 var_real/fake: 0.009/0.003 \n",
      "[0/1][883/937] KL_real/fake: 4.364/5.513 mean_real/fake: 0.054/0.046 var_real/fake: 0.020/0.004 \n",
      "[0/1][884/937] KL_real/fake: 4.034/5.134 mean_real/fake: 0.065/0.039 var_real/fake: 0.031/0.007 \n",
      "[0/1][885/937] KL_real/fake: 3.723/4.424 mean_real/fake: 0.035/0.049 var_real/fake: 0.043/0.019 \n",
      "[0/1][886/937] KL_real/fake: 5.199/6.119 mean_real/fake: 0.057/0.047 var_real/fake: 0.006/0.002 \n",
      "[0/1][887/937] KL_real/fake: 4.656/5.752 mean_real/fake: 0.046/0.041 var_real/fake: 0.010/0.002 \n",
      "[0/1][888/937] KL_real/fake: 3.084/4.157 mean_real/fake: 0.046/0.087 var_real/fake: 0.068/0.025 \n",
      "[0/1][889/937] KL_real/fake: 4.319/5.813 mean_real/fake: 0.078/0.041 var_real/fake: 0.020/0.002 \n",
      "[0/1][890/937] KL_real/fake: 4.905/5.898 mean_real/fake: 0.039/0.037 var_real/fake: 0.008/0.002 \n",
      "[0/1][891/937] KL_real/fake: 4.447/5.481 mean_real/fake: 0.081/0.051 var_real/fake: 0.020/0.004 \n",
      "[0/1][892/937] KL_real/fake: 4.001/4.966 mean_real/fake: 0.047/0.052 var_real/fake: 0.025/0.012 \n",
      "[0/1][893/937] KL_real/fake: 3.129/4.129 mean_real/fake: 0.043/0.052 var_real/fake: 0.064/0.028 \n",
      "[0/1][894/937] KL_real/fake: 5.319/6.207 mean_real/fake: 0.050/0.045 var_real/fake: 0.005/0.002 \n",
      "[0/1][895/937] KL_real/fake: 5.766/6.362 mean_real/fake: 0.043/0.044 var_real/fake: 0.002/0.001 \n",
      "[0/1][896/937] KL_real/fake: 4.833/5.899 mean_real/fake: 0.053/0.033 var_real/fake: 0.009/0.002 \n",
      "[0/1][897/937] KL_real/fake: 3.441/4.509 mean_real/fake: 0.050/0.073 var_real/fake: 0.050/0.017 \n",
      "[0/1][898/937] KL_real/fake: 3.830/5.130 mean_real/fake: 0.019/0.059 var_real/fake: 0.041/0.007 \n",
      "[0/1][899/937] KL_real/fake: 5.161/6.127 mean_real/fake: 0.052/0.038 var_real/fake: 0.006/0.002 \n",
      "[0/1][900/937] KL_real/fake: 5.229/6.344 mean_real/fake: 0.049/0.038 var_real/fake: 0.005/0.001 \n",
      "[0/1][901/937] KL_real/fake: 4.414/5.647 mean_real/fake: 0.046/0.047 var_real/fake: 0.016/0.004 \n",
      "[0/1][902/937] KL_real/fake: 3.745/4.758 mean_real/fake: 0.076/0.030 var_real/fake: 0.044/0.011 \n",
      "[0/1][903/937] KL_real/fake: 3.981/5.232 mean_real/fake: 0.046/0.043 var_real/fake: 0.025/0.005 \n",
      "[0/1][904/937] KL_real/fake: 3.708/4.997 mean_real/fake: 0.052/0.043 var_real/fake: 0.048/0.008 \n",
      "[0/1][905/937] KL_real/fake: 3.411/4.436 mean_real/fake: 0.060/0.034 var_real/fake: 0.077/0.012 \n",
      "[0/1][906/937] KL_real/fake: 3.889/5.110 mean_real/fake: 0.061/0.044 var_real/fake: 0.029/0.006 \n",
      "[0/1][907/937] KL_real/fake: 3.234/4.497 mean_real/fake: 0.053/0.041 var_real/fake: 0.069/0.013 \n",
      "[0/1][908/937] KL_real/fake: 5.154/6.092 mean_real/fake: 0.057/0.040 var_real/fake: 0.006/0.002 \n",
      "[0/1][909/937] KL_real/fake: 5.045/5.893 mean_real/fake: 0.055/0.043 var_real/fake: 0.006/0.002 \n",
      "[0/1][910/937] KL_real/fake: 4.370/5.728 mean_real/fake: 0.054/0.048 var_real/fake: 0.018/0.003 \n",
      "[0/1][911/937] KL_real/fake: 4.168/5.209 mean_real/fake: 0.030/0.057 var_real/fake: 0.037/0.006 \n",
      "[0/1][912/937] KL_real/fake: 3.803/5.095 mean_real/fake: 0.062/0.050 var_real/fake: 0.070/0.007 \n",
      "[0/1][913/937] KL_real/fake: 5.356/6.005 mean_real/fake: 0.048/0.039 var_real/fake: 0.004/0.002 \n",
      "[0/1][914/937] KL_real/fake: 5.470/6.090 mean_real/fake: 0.041/0.042 var_real/fake: 0.004/0.002 \n",
      "[0/1][915/937] KL_real/fake: 4.271/5.710 mean_real/fake: 0.045/0.047 var_real/fake: 0.020/0.003 \n",
      "[0/1][916/937] KL_real/fake: 3.193/3.687 mean_real/fake: 0.028/0.063 var_real/fake: 0.058/0.038 \n",
      "[0/1][917/937] KL_real/fake: 4.053/4.723 mean_real/fake: 0.052/0.060 var_real/fake: 0.028/0.011 \n",
      "[0/1][918/937] KL_real/fake: 4.592/5.591 mean_real/fake: 0.058/0.050 var_real/fake: 0.016/0.004 \n",
      "[0/1][919/937] KL_real/fake: 4.653/5.550 mean_real/fake: 0.037/0.056 var_real/fake: 0.014/0.004 \n",
      "[0/1][920/937] KL_real/fake: 4.544/5.708 mean_real/fake: 0.045/0.049 var_real/fake: 0.011/0.003 \n",
      "[0/1][921/937] KL_real/fake: 4.242/5.295 mean_real/fake: 0.034/0.042 var_real/fake: 0.026/0.005 \n",
      "[0/1][922/937] KL_real/fake: 3.610/4.934 mean_real/fake: 0.058/0.040 var_real/fake: 0.059/0.007 \n",
      "[0/1][923/937] KL_real/fake: 3.709/4.791 mean_real/fake: 0.067/0.061 var_real/fake: 0.073/0.016 \n",
      "[0/1][924/937] KL_real/fake: 3.823/5.501 mean_real/fake: 0.063/0.038 var_real/fake: 0.042/0.004 \n",
      "[0/1][925/937] KL_real/fake: 4.278/5.136 mean_real/fake: 0.066/0.062 var_real/fake: 0.021/0.009 \n",
      "[0/1][926/937] KL_real/fake: 3.395/4.599 mean_real/fake: 0.089/0.041 var_real/fake: 0.067/0.013 \n",
      "[0/1][927/937] KL_real/fake: 3.951/5.313 mean_real/fake: 0.068/0.048 var_real/fake: 0.038/0.005 \n",
      "[0/1][928/937] KL_real/fake: 4.339/5.322 mean_real/fake: 0.057/0.055 var_real/fake: 0.017/0.006 \n",
      "[0/1][929/937] KL_real/fake: 3.697/4.859 mean_real/fake: 0.059/0.064 var_real/fake: 0.051/0.010 \n",
      "[0/1][930/937] KL_real/fake: 2.923/4.038 mean_real/fake: 0.056/0.076 var_real/fake: 0.085/0.040 \n",
      "[0/1][931/937] KL_real/fake: 5.966/6.620 mean_real/fake: 0.041/0.040 var_real/fake: 0.002/0.001 \n",
      "[0/1][932/937] KL_real/fake: 5.910/6.804 mean_real/fake: 0.043/0.040 var_real/fake: 0.002/0.001 \n",
      "[0/1][933/937] KL_real/fake: 4.966/6.206 mean_real/fake: 0.057/0.044 var_real/fake: 0.010/0.002 \n",
      "[0/1][934/937] KL_real/fake: 4.133/4.800 mean_real/fake: 0.033/0.029 var_real/fake: 0.024/0.011 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][935/937] KL_real/fake: 3.973/4.806 mean_real/fake: 0.058/0.046 var_real/fake: 0.035/0.010 \n",
      "[0/1][936/937] KL_real/fake: 3.709/4.717 mean_real/fake: 0.109/0.049 var_real/fake: 0.067/0.015 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blanca.alonso/anaconda3/envs/p3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type _netG_Base. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/blanca.alonso/anaconda3/envs/p3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type _netE_Base. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "for epoch in range(start_epoch, nepoch):\n",
    "\n",
    "    # Adjust learning rate\n",
    "    adjust_lr(epoch)\n",
    "\n",
    "    for i in range(len(dataloader['train'])):\n",
    "\n",
    "        # ---------------------------\n",
    "        #        Optimize over e\n",
    "        # ---------------------------\n",
    "\n",
    "        for e_iter in range(updates['e']['num_updates']):\n",
    "            e_losses = []\n",
    "            netE.zero_grad()\n",
    "\n",
    "            # X\n",
    "            populate_x(x, dataloader['train'])\n",
    "            # e(X)\n",
    "#             print(x.shape) # torch.Size([64, 1, 32, 32])\n",
    "            ex = netE(x)\n",
    "\n",
    "            # KL_real: - \\Delta( e(X) , Z ) -> max_e\n",
    "            KL_real = KL_minimizer(ex)\n",
    "            e_losses.append(KL_real * updates['e']['KL_real'])\n",
    "\n",
    "            if updates['e']['match_x'] != 0:\n",
    "                # g(e(X))\n",
    "                gex = netG(ex)\n",
    "\n",
    "                # match_x: E_x||g(e(x)) - x|| -> min_e\n",
    "                err = match(gex, x, opt.match_x)\n",
    "                e_losses.append(err * updates['e']['match_x'])\n",
    "\n",
    "            # Save some stats\n",
    "            stats['real_mean'] = KL_minimizer.samples_mean.data.mean()\n",
    "            stats['real_var'] = KL_minimizer.samples_var.data.mean()\n",
    "            stats['KL_real'] = KL_real.data[0]\n",
    "\n",
    "            # ================================================\n",
    "\n",
    "            # Z\n",
    "            populate_z(z)\n",
    "            # g(Z)\n",
    "            fake = netG(z).detach()\n",
    "            # e(g(Z))\n",
    "            egz = netE(fake)\n",
    "\n",
    "            # KL_fake: \\Delta( e(g(Z)) , Z ) -> max_e\n",
    "            KL_fake = KL_maximizer(egz)\n",
    "            e_losses.append(KL_fake * updates['e']['KL_fake'])\n",
    "\n",
    "            if updates['e']['match_z'] != 0:\n",
    "                # match_z: E_z||e(g(z)) - z|| -> min_e\n",
    "                err = match(egz, z, match_z)\n",
    "                e_losses.append(err * updates['e']['match_z'])\n",
    "\n",
    "            # Save some stats\n",
    "            stats['fake_mean'] = KL_maximizer.samples_mean.data.mean()\n",
    "            stats['fake_var'] = KL_maximizer.samples_var.data.mean()\n",
    "            stats['KL_fake'] = -KL_fake.data[0]\n",
    "\n",
    "            # Update e\n",
    "            sum(e_losses).backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "        # ---------------------------\n",
    "        #        Minimize over g\n",
    "        # ---------------------------\n",
    "\n",
    "        for g_iter in range(updates['g']['num_updates']):\n",
    "            g_losses = []\n",
    "            netG.zero_grad()\n",
    "\n",
    "            # Z\n",
    "            populate_z(z)\n",
    "            # g(Z)\n",
    "            fake = netG(z)\n",
    "            # e(g(Z))\n",
    "            egz = netE(fake)\n",
    "\n",
    "            # KL_fake: \\Delta( e(g(Z)) , Z ) -> min_g\n",
    "            KL_fake_g = KL_minimizer(egz)\n",
    "            g_losses.append(KL_fake_g * updates['g']['KL_fake'])\n",
    "\n",
    "            if updates['g']['match_z'] != 0:\n",
    "                # match_z: E_z||e(g(z)) - z|| -> min_g\n",
    "                err = match(egz, z, match_z)\n",
    "                err = err * updates['g']['match_z']\n",
    "                g_losses.append(err)\n",
    "\n",
    "            # ==================================\n",
    "\n",
    "            if updates['g']['match_x'] != 0:\n",
    "                # X\n",
    "                populate_x(x, dataloader['train'])\n",
    "                # e(X)\n",
    "                ex = netE(x)\n",
    "\n",
    "                # g(e(X))\n",
    "                gex = netG(ex)\n",
    "\n",
    "                # match_x: E_x||g(e(x)) - x|| -> min_g\n",
    "                err = match(gex, x, match_x)\n",
    "                err = err * updates['g']['match_x']\n",
    "                g_losses.append(err)\n",
    "\n",
    "            # Step g\n",
    "            sum(g_losses).backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "        print('[{epoch}/{nepoch}][{iter}/{niter}] '\n",
    "              'KL_real/fake: {KL_real:.3f}/{KL_fake:.3f} '\n",
    "              'mean_real/fake: {real_mean:.3f}/{fake_mean:.3f} '\n",
    "              'var_real/fake: {real_var:.3f}/{fake_var:.3f} '\n",
    "              ''.format(epoch=epoch,\n",
    "                        nepoch=nepoch, #(this should be either nepoch-1 or start the logging display at 1 instead of 0)\n",
    "                        iter=i,\n",
    "                        niter=len(dataloader['train']),\n",
    "                        **stats))\n",
    "\n",
    "        if i % save_every == 0:\n",
    "            save_images(epoch)\n",
    "\n",
    "        # If an epoch takes long time, dump intermediate\n",
    "        if dataset in ['lsun', 'imagenet'] and (i % 5000 == 0):\n",
    "            torch.save(netG, '%s/netG_epoch_%d_it_%d.pth' %\n",
    "                       (save_dir, epoch, i))\n",
    "            torch.save(netE, '%s/netE_epoch_%d_it_%d.pth' %\n",
    "                       (save_dir, epoch, i))\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG, '%s/netG_epoch_%d.pth' % (save_dir, epoch))\n",
    "    torch.save(netE, '%s/netE_epoch_%d.pth' % (save_dir, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
